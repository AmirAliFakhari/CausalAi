{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **II. Discover and Infere the Causality Graph**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **A. Initialize the Project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This top-level section initiates the entire notebook project by establishing the computational environment and configurations required for subsequent analytical workflows. It comprises two primary subsections: (1) loading all necessary libraries and modules, and (2) configuring essential tools and settings for reproducibility, visualization, and environment setup. These foundational steps ensure that all dependencies are correctly initialized and the environment is consistently reproducible across executions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the Packages and Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This subsection is responsible for importing all required Python packages and modules that will be utilized throughout the notebook. It includes essential standard library modules, scientific computing tools, machine learning frameworks, deep learning utilities, and specialized packages for causal inference and structural equation modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Import Essential Python Standard Library Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sub-subsection imports foundational Python standard library modules that provide base functionalities such as abstract base classes (abc), combinatorics (itertools), JSON processing, OS-level operations, warning handling, and logging utilities. These libraries serve as foundational components supporting utility functions, configuration management, and logging mechanisms used throughout the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6JLXa5s9HHbS"
   },
   "outputs": [],
   "source": [
    "# Import necessary modules.\n",
    "from abc import ABC, abstractmethod\n",
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Import Scientific Computing, Machine Learning, and Utility Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sub-subsection imports a suite of scientific and machine learning libraries crucial for data manipulation, statistical analysis, visualization, and model building. These include:\n",
    "- numpy, scipy, and pandas for numerical and statistical computations.\n",
    "- matplotlib for data visualization.\n",
    "- networkx for graph-based structures.\n",
    "- scikit-learn for preprocessing and regression.\n",
    "- pytorch_lightning and torch for deep learning infrastructure and training utilities.\n",
    "\n",
    "These libraries establish the computational and analytical core of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZZnw05AaHHbT"
   },
   "outputs": [],
   "source": [
    "# Import additional data handling, visualization, machine learning, and deep learning modules.\n",
    "import fsspec\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "import pytorch_lightning as pl\n",
    "from scipy import stats\n",
    "import scipy.linalg as slin\n",
    "import scipy.optimize as sopt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Import Causal Inference and causalnex and Structural Equation Modeling Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sub-subsection loads specialized modules from the causica and lingam libraries, which are essential for performing causal discovery, structural learning, and related inference tasks. It includes dataset handling classes, probabilistic distribution tools, Lightning modules for model training, and algorithmic components for augmented Lagrangian training and linear non-Gaussian acyclic modeling. These imports directly support the notebookâ€™s focus on causal inference methodologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XTOQ0G7bHHbT"
   },
   "outputs": [],
   "source": [
    "# Import modules from the causica and lingam packages.\n",
    "from causica.datasets.causica_dataset_format import Variable\n",
    "from causica.distributions import ContinuousNoiseDist\n",
    "from causica.lightning.data_modules.basic_data_module import BasicDECIDataModule\n",
    "from causica.lightning.modules.deci_module import DECIModule\n",
    "from causica.training.auglag import AugLagLRConfig\n",
    "from causalnex.structure import notears\n",
    "from causalnex.structure.structuremodel import StructureModel\n",
    "from causalnex.plots import plot_structure\n",
    "import lingam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Configure the Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This subsection configures critical operational settings for numerical precision, plotting aesthetics, reproducibility, and environmental setup. It ensures standardized formatting and controlled randomness to support robust, interpretable, and reproducible results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Configure NumPy, Pandas, and Reproducibility Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sub-subsection sets options for numpy and pandas to ensure readable and consistent output formatting for arrays and dataframes. It also seeds random number generators across numpy, torch, and pytorch_lightning to ensure deterministic behavior, which is crucial for reproducibility in experimental workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set NumPy configuration options.\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set pandas display options.\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.precision\", 2)\n",
    "\n",
    "# Seed random number generators for PyTorch and PyLightning.\n",
    "torch.manual_seed(42)\n",
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Configure Matplotlib Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sub-subsection customizes matplotlib's font family to \"Times New Roman\" to standardize the visual appearance of all plots generated, enhancing readability and stylistic uniformity in reports or publications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Matplotlib font family.\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Environment and Path Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sub-subsection sets project-specific path variables for datasets and configuration files, loads environment variables using dotenv, configures fallback options for PyTorch hardware acceleration, and initializes logging settings. The configuration facilitates modular control of data access, secure variable management, and robust runtime diagnostics through logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set configuration variables.\n",
    "test_run = bool(os.environ.get(\"TEST_RUN\", False))\n",
    "DATA_PATH = \"data/dataset.csv\"\n",
    "VARIABLES_PATH = \"data/variables.json\"\n",
    "\n",
    "# Import environment variable loader.\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from a .env file.\n",
    "load_dotenv()\n",
    "\n",
    "# Set PyTorch environment variable for MPS fallback.\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "# Configure logging settings.\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **B. Get the Data Prepared**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section performs comprehensive preprocessing of the input dataset to ensure its readiness for causal analysis and modeling. It includes steps for loading, cleaning, transforming, encoding, and splitting the data. Emphasis is placed on systematically identifying relevant features, removing redundancies and data leaks, handling missing values and outliers, and encoding categorical variables appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Explore and Clean the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This subsection initiates the data preparation pipeline. It sequentially processes the dataset by loading it, inspecting the content, renaming and selecting pertinent features, handling anomalies, and preparing the data for modeling. It ensures data integrity, feature relevance, and analytical consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the dataset from a CSV file into a Pandas DataFrame. This provides the raw input data necessary for subsequent preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "H3dhIK6_HHbW"
   },
   "outputs": [],
   "source": [
    "# Load the dataset xslsx file into a Pandas DataFrame.\n",
    "data = pd.read_csv(\n",
    "    DATA_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Show the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displays the first 5 rows of the dataset to facilitate initial inspection and validation of the dataset structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "sKPokcEQHHbW"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_7f69d\">\n",
       "  <caption><b>IBM Telco Customer Churn Dataset (First 5 Rows)</b></caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_7f69d_level0_col0\" class=\"col_heading level0 col0\" >Customer ID</th>\n",
       "      <th id=\"T_7f69d_level0_col1\" class=\"col_heading level0 col1\" >Gender</th>\n",
       "      <th id=\"T_7f69d_level0_col2\" class=\"col_heading level0 col2\" >Age</th>\n",
       "      <th id=\"T_7f69d_level0_col3\" class=\"col_heading level0 col3\" >Under 30</th>\n",
       "      <th id=\"T_7f69d_level0_col4\" class=\"col_heading level0 col4\" >Senior Citizen</th>\n",
       "      <th id=\"T_7f69d_level0_col5\" class=\"col_heading level0 col5\" >Married</th>\n",
       "      <th id=\"T_7f69d_level0_col6\" class=\"col_heading level0 col6\" >Dependents</th>\n",
       "      <th id=\"T_7f69d_level0_col7\" class=\"col_heading level0 col7\" >Number of Dependents</th>\n",
       "      <th id=\"T_7f69d_level0_col8\" class=\"col_heading level0 col8\" >Country</th>\n",
       "      <th id=\"T_7f69d_level0_col9\" class=\"col_heading level0 col9\" >State</th>\n",
       "      <th id=\"T_7f69d_level0_col10\" class=\"col_heading level0 col10\" >City</th>\n",
       "      <th id=\"T_7f69d_level0_col11\" class=\"col_heading level0 col11\" >Zip Code</th>\n",
       "      <th id=\"T_7f69d_level0_col12\" class=\"col_heading level0 col12\" >Latitude</th>\n",
       "      <th id=\"T_7f69d_level0_col13\" class=\"col_heading level0 col13\" >Longitude</th>\n",
       "      <th id=\"T_7f69d_level0_col14\" class=\"col_heading level0 col14\" >Population</th>\n",
       "      <th id=\"T_7f69d_level0_col15\" class=\"col_heading level0 col15\" >Quarter</th>\n",
       "      <th id=\"T_7f69d_level0_col16\" class=\"col_heading level0 col16\" >Referred a Friend</th>\n",
       "      <th id=\"T_7f69d_level0_col17\" class=\"col_heading level0 col17\" >Number of Referrals</th>\n",
       "      <th id=\"T_7f69d_level0_col18\" class=\"col_heading level0 col18\" >Tenure in Months</th>\n",
       "      <th id=\"T_7f69d_level0_col19\" class=\"col_heading level0 col19\" >Offer</th>\n",
       "      <th id=\"T_7f69d_level0_col20\" class=\"col_heading level0 col20\" >Phone Service</th>\n",
       "      <th id=\"T_7f69d_level0_col21\" class=\"col_heading level0 col21\" >Avg Monthly Long Distance Charges</th>\n",
       "      <th id=\"T_7f69d_level0_col22\" class=\"col_heading level0 col22\" >Multiple Lines</th>\n",
       "      <th id=\"T_7f69d_level0_col23\" class=\"col_heading level0 col23\" >Internet Service</th>\n",
       "      <th id=\"T_7f69d_level0_col24\" class=\"col_heading level0 col24\" >Internet Type</th>\n",
       "      <th id=\"T_7f69d_level0_col25\" class=\"col_heading level0 col25\" >Avg Monthly GB Download</th>\n",
       "      <th id=\"T_7f69d_level0_col26\" class=\"col_heading level0 col26\" >Online Security</th>\n",
       "      <th id=\"T_7f69d_level0_col27\" class=\"col_heading level0 col27\" >Online Backup</th>\n",
       "      <th id=\"T_7f69d_level0_col28\" class=\"col_heading level0 col28\" >Device Protection Plan</th>\n",
       "      <th id=\"T_7f69d_level0_col29\" class=\"col_heading level0 col29\" >Premium Tech Support</th>\n",
       "      <th id=\"T_7f69d_level0_col30\" class=\"col_heading level0 col30\" >Streaming TV</th>\n",
       "      <th id=\"T_7f69d_level0_col31\" class=\"col_heading level0 col31\" >Streaming Movies</th>\n",
       "      <th id=\"T_7f69d_level0_col32\" class=\"col_heading level0 col32\" >Streaming Music</th>\n",
       "      <th id=\"T_7f69d_level0_col33\" class=\"col_heading level0 col33\" >Unlimited Data</th>\n",
       "      <th id=\"T_7f69d_level0_col34\" class=\"col_heading level0 col34\" >Contract</th>\n",
       "      <th id=\"T_7f69d_level0_col35\" class=\"col_heading level0 col35\" >Paperless Billing</th>\n",
       "      <th id=\"T_7f69d_level0_col36\" class=\"col_heading level0 col36\" >Payment Method</th>\n",
       "      <th id=\"T_7f69d_level0_col37\" class=\"col_heading level0 col37\" >Monthly Charge</th>\n",
       "      <th id=\"T_7f69d_level0_col38\" class=\"col_heading level0 col38\" >Total Charges</th>\n",
       "      <th id=\"T_7f69d_level0_col39\" class=\"col_heading level0 col39\" >Total Refunds</th>\n",
       "      <th id=\"T_7f69d_level0_col40\" class=\"col_heading level0 col40\" >Total Extra Data Charges</th>\n",
       "      <th id=\"T_7f69d_level0_col41\" class=\"col_heading level0 col41\" >Total Long Distance Charges</th>\n",
       "      <th id=\"T_7f69d_level0_col42\" class=\"col_heading level0 col42\" >Total Revenue</th>\n",
       "      <th id=\"T_7f69d_level0_col43\" class=\"col_heading level0 col43\" >Satisfaction Score</th>\n",
       "      <th id=\"T_7f69d_level0_col44\" class=\"col_heading level0 col44\" >Customer Status</th>\n",
       "      <th id=\"T_7f69d_level0_col45\" class=\"col_heading level0 col45\" >Churn Label</th>\n",
       "      <th id=\"T_7f69d_level0_col46\" class=\"col_heading level0 col46\" >Churn Score</th>\n",
       "      <th id=\"T_7f69d_level0_col47\" class=\"col_heading level0 col47\" >CLTV</th>\n",
       "      <th id=\"T_7f69d_level0_col48\" class=\"col_heading level0 col48\" >Churn Category</th>\n",
       "      <th id=\"T_7f69d_level0_col49\" class=\"col_heading level0 col49\" >Churn Reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_7f69d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_7f69d_row0_col0\" class=\"data row0 col0\" >8779-QRDMV</td>\n",
       "      <td id=\"T_7f69d_row0_col1\" class=\"data row0 col1\" >Male</td>\n",
       "      <td id=\"T_7f69d_row0_col2\" class=\"data row0 col2\" >78</td>\n",
       "      <td id=\"T_7f69d_row0_col3\" class=\"data row0 col3\" >No</td>\n",
       "      <td id=\"T_7f69d_row0_col4\" class=\"data row0 col4\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row0_col5\" class=\"data row0 col5\" >No</td>\n",
       "      <td id=\"T_7f69d_row0_col6\" class=\"data row0 col6\" >No</td>\n",
       "      <td id=\"T_7f69d_row0_col7\" class=\"data row0 col7\" >0</td>\n",
       "      <td id=\"T_7f69d_row0_col8\" class=\"data row0 col8\" >United States</td>\n",
       "      <td id=\"T_7f69d_row0_col9\" class=\"data row0 col9\" >California</td>\n",
       "      <td id=\"T_7f69d_row0_col10\" class=\"data row0 col10\" >Los Angeles</td>\n",
       "      <td id=\"T_7f69d_row0_col11\" class=\"data row0 col11\" >90022</td>\n",
       "      <td id=\"T_7f69d_row0_col12\" class=\"data row0 col12\" >34.023810</td>\n",
       "      <td id=\"T_7f69d_row0_col13\" class=\"data row0 col13\" >-118.156582</td>\n",
       "      <td id=\"T_7f69d_row0_col14\" class=\"data row0 col14\" >68701</td>\n",
       "      <td id=\"T_7f69d_row0_col15\" class=\"data row0 col15\" >Q3</td>\n",
       "      <td id=\"T_7f69d_row0_col16\" class=\"data row0 col16\" >No</td>\n",
       "      <td id=\"T_7f69d_row0_col17\" class=\"data row0 col17\" >0</td>\n",
       "      <td id=\"T_7f69d_row0_col18\" class=\"data row0 col18\" >1</td>\n",
       "      <td id=\"T_7f69d_row0_col19\" class=\"data row0 col19\" >None</td>\n",
       "      <td id=\"T_7f69d_row0_col20\" class=\"data row0 col20\" >No</td>\n",
       "      <td id=\"T_7f69d_row0_col21\" class=\"data row0 col21\" >0.000000</td>\n",
       "      <td id=\"T_7f69d_row0_col22\" class=\"data row0 col22\" >No</td>\n",
       "      <td id=\"T_7f69d_row0_col23\" class=\"data row0 col23\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row0_col24\" class=\"data row0 col24\" >DSL</td>\n",
       "      <td id=\"T_7f69d_row0_col25\" class=\"data row0 col25\" >8</td>\n",
       "      <td id=\"T_7f69d_row0_col26\" class=\"data row0 col26\" >No</td>\n",
       "      <td id=\"T_7f69d_row0_col27\" class=\"data row0 col27\" >No</td>\n",
       "      <td id=\"T_7f69d_row0_col28\" class=\"data row0 col28\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row0_col29\" class=\"data row0 col29\" >No</td>\n",
       "      <td id=\"T_7f69d_row0_col30\" class=\"data row0 col30\" >No</td>\n",
       "      <td id=\"T_7f69d_row0_col31\" class=\"data row0 col31\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row0_col32\" class=\"data row0 col32\" >No</td>\n",
       "      <td id=\"T_7f69d_row0_col33\" class=\"data row0 col33\" >No</td>\n",
       "      <td id=\"T_7f69d_row0_col34\" class=\"data row0 col34\" >Month-to-Month</td>\n",
       "      <td id=\"T_7f69d_row0_col35\" class=\"data row0 col35\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row0_col36\" class=\"data row0 col36\" >Bank Withdrawal</td>\n",
       "      <td id=\"T_7f69d_row0_col37\" class=\"data row0 col37\" >39.650000</td>\n",
       "      <td id=\"T_7f69d_row0_col38\" class=\"data row0 col38\" >39.650000</td>\n",
       "      <td id=\"T_7f69d_row0_col39\" class=\"data row0 col39\" >0.000000</td>\n",
       "      <td id=\"T_7f69d_row0_col40\" class=\"data row0 col40\" >20</td>\n",
       "      <td id=\"T_7f69d_row0_col41\" class=\"data row0 col41\" >0.000000</td>\n",
       "      <td id=\"T_7f69d_row0_col42\" class=\"data row0 col42\" >59.650000</td>\n",
       "      <td id=\"T_7f69d_row0_col43\" class=\"data row0 col43\" >3</td>\n",
       "      <td id=\"T_7f69d_row0_col44\" class=\"data row0 col44\" >Churned</td>\n",
       "      <td id=\"T_7f69d_row0_col45\" class=\"data row0 col45\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row0_col46\" class=\"data row0 col46\" >91</td>\n",
       "      <td id=\"T_7f69d_row0_col47\" class=\"data row0 col47\" >5433</td>\n",
       "      <td id=\"T_7f69d_row0_col48\" class=\"data row0 col48\" >Competitor</td>\n",
       "      <td id=\"T_7f69d_row0_col49\" class=\"data row0 col49\" >Competitor offered more data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f69d_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_7f69d_row1_col0\" class=\"data row1 col0\" >7495-OOKFY</td>\n",
       "      <td id=\"T_7f69d_row1_col1\" class=\"data row1 col1\" >Female</td>\n",
       "      <td id=\"T_7f69d_row1_col2\" class=\"data row1 col2\" >74</td>\n",
       "      <td id=\"T_7f69d_row1_col3\" class=\"data row1 col3\" >No</td>\n",
       "      <td id=\"T_7f69d_row1_col4\" class=\"data row1 col4\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row1_col5\" class=\"data row1 col5\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row1_col6\" class=\"data row1 col6\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row1_col7\" class=\"data row1 col7\" >1</td>\n",
       "      <td id=\"T_7f69d_row1_col8\" class=\"data row1 col8\" >United States</td>\n",
       "      <td id=\"T_7f69d_row1_col9\" class=\"data row1 col9\" >California</td>\n",
       "      <td id=\"T_7f69d_row1_col10\" class=\"data row1 col10\" >Los Angeles</td>\n",
       "      <td id=\"T_7f69d_row1_col11\" class=\"data row1 col11\" >90063</td>\n",
       "      <td id=\"T_7f69d_row1_col12\" class=\"data row1 col12\" >34.044271</td>\n",
       "      <td id=\"T_7f69d_row1_col13\" class=\"data row1 col13\" >-118.185237</td>\n",
       "      <td id=\"T_7f69d_row1_col14\" class=\"data row1 col14\" >55668</td>\n",
       "      <td id=\"T_7f69d_row1_col15\" class=\"data row1 col15\" >Q3</td>\n",
       "      <td id=\"T_7f69d_row1_col16\" class=\"data row1 col16\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row1_col17\" class=\"data row1 col17\" >1</td>\n",
       "      <td id=\"T_7f69d_row1_col18\" class=\"data row1 col18\" >8</td>\n",
       "      <td id=\"T_7f69d_row1_col19\" class=\"data row1 col19\" >Offer E</td>\n",
       "      <td id=\"T_7f69d_row1_col20\" class=\"data row1 col20\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row1_col21\" class=\"data row1 col21\" >48.850000</td>\n",
       "      <td id=\"T_7f69d_row1_col22\" class=\"data row1 col22\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row1_col23\" class=\"data row1 col23\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row1_col24\" class=\"data row1 col24\" >Fiber Optic</td>\n",
       "      <td id=\"T_7f69d_row1_col25\" class=\"data row1 col25\" >17</td>\n",
       "      <td id=\"T_7f69d_row1_col26\" class=\"data row1 col26\" >No</td>\n",
       "      <td id=\"T_7f69d_row1_col27\" class=\"data row1 col27\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row1_col28\" class=\"data row1 col28\" >No</td>\n",
       "      <td id=\"T_7f69d_row1_col29\" class=\"data row1 col29\" >No</td>\n",
       "      <td id=\"T_7f69d_row1_col30\" class=\"data row1 col30\" >No</td>\n",
       "      <td id=\"T_7f69d_row1_col31\" class=\"data row1 col31\" >No</td>\n",
       "      <td id=\"T_7f69d_row1_col32\" class=\"data row1 col32\" >No</td>\n",
       "      <td id=\"T_7f69d_row1_col33\" class=\"data row1 col33\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row1_col34\" class=\"data row1 col34\" >Month-to-Month</td>\n",
       "      <td id=\"T_7f69d_row1_col35\" class=\"data row1 col35\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row1_col36\" class=\"data row1 col36\" >Credit Card</td>\n",
       "      <td id=\"T_7f69d_row1_col37\" class=\"data row1 col37\" >80.650000</td>\n",
       "      <td id=\"T_7f69d_row1_col38\" class=\"data row1 col38\" >633.300000</td>\n",
       "      <td id=\"T_7f69d_row1_col39\" class=\"data row1 col39\" >0.000000</td>\n",
       "      <td id=\"T_7f69d_row1_col40\" class=\"data row1 col40\" >0</td>\n",
       "      <td id=\"T_7f69d_row1_col41\" class=\"data row1 col41\" >390.800000</td>\n",
       "      <td id=\"T_7f69d_row1_col42\" class=\"data row1 col42\" >1024.100000</td>\n",
       "      <td id=\"T_7f69d_row1_col43\" class=\"data row1 col43\" >3</td>\n",
       "      <td id=\"T_7f69d_row1_col44\" class=\"data row1 col44\" >Churned</td>\n",
       "      <td id=\"T_7f69d_row1_col45\" class=\"data row1 col45\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row1_col46\" class=\"data row1 col46\" >69</td>\n",
       "      <td id=\"T_7f69d_row1_col47\" class=\"data row1 col47\" >5302</td>\n",
       "      <td id=\"T_7f69d_row1_col48\" class=\"data row1 col48\" >Competitor</td>\n",
       "      <td id=\"T_7f69d_row1_col49\" class=\"data row1 col49\" >Competitor made better offer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f69d_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_7f69d_row2_col0\" class=\"data row2 col0\" >1658-BYGOY</td>\n",
       "      <td id=\"T_7f69d_row2_col1\" class=\"data row2 col1\" >Male</td>\n",
       "      <td id=\"T_7f69d_row2_col2\" class=\"data row2 col2\" >71</td>\n",
       "      <td id=\"T_7f69d_row2_col3\" class=\"data row2 col3\" >No</td>\n",
       "      <td id=\"T_7f69d_row2_col4\" class=\"data row2 col4\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row2_col5\" class=\"data row2 col5\" >No</td>\n",
       "      <td id=\"T_7f69d_row2_col6\" class=\"data row2 col6\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row2_col7\" class=\"data row2 col7\" >3</td>\n",
       "      <td id=\"T_7f69d_row2_col8\" class=\"data row2 col8\" >United States</td>\n",
       "      <td id=\"T_7f69d_row2_col9\" class=\"data row2 col9\" >California</td>\n",
       "      <td id=\"T_7f69d_row2_col10\" class=\"data row2 col10\" >Los Angeles</td>\n",
       "      <td id=\"T_7f69d_row2_col11\" class=\"data row2 col11\" >90065</td>\n",
       "      <td id=\"T_7f69d_row2_col12\" class=\"data row2 col12\" >34.108833</td>\n",
       "      <td id=\"T_7f69d_row2_col13\" class=\"data row2 col13\" >-118.229715</td>\n",
       "      <td id=\"T_7f69d_row2_col14\" class=\"data row2 col14\" >47534</td>\n",
       "      <td id=\"T_7f69d_row2_col15\" class=\"data row2 col15\" >Q3</td>\n",
       "      <td id=\"T_7f69d_row2_col16\" class=\"data row2 col16\" >No</td>\n",
       "      <td id=\"T_7f69d_row2_col17\" class=\"data row2 col17\" >0</td>\n",
       "      <td id=\"T_7f69d_row2_col18\" class=\"data row2 col18\" >18</td>\n",
       "      <td id=\"T_7f69d_row2_col19\" class=\"data row2 col19\" >Offer D</td>\n",
       "      <td id=\"T_7f69d_row2_col20\" class=\"data row2 col20\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row2_col21\" class=\"data row2 col21\" >11.330000</td>\n",
       "      <td id=\"T_7f69d_row2_col22\" class=\"data row2 col22\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row2_col23\" class=\"data row2 col23\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row2_col24\" class=\"data row2 col24\" >Fiber Optic</td>\n",
       "      <td id=\"T_7f69d_row2_col25\" class=\"data row2 col25\" >52</td>\n",
       "      <td id=\"T_7f69d_row2_col26\" class=\"data row2 col26\" >No</td>\n",
       "      <td id=\"T_7f69d_row2_col27\" class=\"data row2 col27\" >No</td>\n",
       "      <td id=\"T_7f69d_row2_col28\" class=\"data row2 col28\" >No</td>\n",
       "      <td id=\"T_7f69d_row2_col29\" class=\"data row2 col29\" >No</td>\n",
       "      <td id=\"T_7f69d_row2_col30\" class=\"data row2 col30\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row2_col31\" class=\"data row2 col31\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row2_col32\" class=\"data row2 col32\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row2_col33\" class=\"data row2 col33\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row2_col34\" class=\"data row2 col34\" >Month-to-Month</td>\n",
       "      <td id=\"T_7f69d_row2_col35\" class=\"data row2 col35\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row2_col36\" class=\"data row2 col36\" >Bank Withdrawal</td>\n",
       "      <td id=\"T_7f69d_row2_col37\" class=\"data row2 col37\" >95.450000</td>\n",
       "      <td id=\"T_7f69d_row2_col38\" class=\"data row2 col38\" >1752.550000</td>\n",
       "      <td id=\"T_7f69d_row2_col39\" class=\"data row2 col39\" >45.610000</td>\n",
       "      <td id=\"T_7f69d_row2_col40\" class=\"data row2 col40\" >0</td>\n",
       "      <td id=\"T_7f69d_row2_col41\" class=\"data row2 col41\" >203.940000</td>\n",
       "      <td id=\"T_7f69d_row2_col42\" class=\"data row2 col42\" >1910.880000</td>\n",
       "      <td id=\"T_7f69d_row2_col43\" class=\"data row2 col43\" >2</td>\n",
       "      <td id=\"T_7f69d_row2_col44\" class=\"data row2 col44\" >Churned</td>\n",
       "      <td id=\"T_7f69d_row2_col45\" class=\"data row2 col45\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row2_col46\" class=\"data row2 col46\" >81</td>\n",
       "      <td id=\"T_7f69d_row2_col47\" class=\"data row2 col47\" >3179</td>\n",
       "      <td id=\"T_7f69d_row2_col48\" class=\"data row2 col48\" >Competitor</td>\n",
       "      <td id=\"T_7f69d_row2_col49\" class=\"data row2 col49\" >Competitor made better offer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f69d_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_7f69d_row3_col0\" class=\"data row3 col0\" >4598-XLKNJ</td>\n",
       "      <td id=\"T_7f69d_row3_col1\" class=\"data row3 col1\" >Female</td>\n",
       "      <td id=\"T_7f69d_row3_col2\" class=\"data row3 col2\" >78</td>\n",
       "      <td id=\"T_7f69d_row3_col3\" class=\"data row3 col3\" >No</td>\n",
       "      <td id=\"T_7f69d_row3_col4\" class=\"data row3 col4\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row3_col5\" class=\"data row3 col5\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row3_col6\" class=\"data row3 col6\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row3_col7\" class=\"data row3 col7\" >1</td>\n",
       "      <td id=\"T_7f69d_row3_col8\" class=\"data row3 col8\" >United States</td>\n",
       "      <td id=\"T_7f69d_row3_col9\" class=\"data row3 col9\" >California</td>\n",
       "      <td id=\"T_7f69d_row3_col10\" class=\"data row3 col10\" >Inglewood</td>\n",
       "      <td id=\"T_7f69d_row3_col11\" class=\"data row3 col11\" >90303</td>\n",
       "      <td id=\"T_7f69d_row3_col12\" class=\"data row3 col12\" >33.936291</td>\n",
       "      <td id=\"T_7f69d_row3_col13\" class=\"data row3 col13\" >-118.332639</td>\n",
       "      <td id=\"T_7f69d_row3_col14\" class=\"data row3 col14\" >27778</td>\n",
       "      <td id=\"T_7f69d_row3_col15\" class=\"data row3 col15\" >Q3</td>\n",
       "      <td id=\"T_7f69d_row3_col16\" class=\"data row3 col16\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row3_col17\" class=\"data row3 col17\" >1</td>\n",
       "      <td id=\"T_7f69d_row3_col18\" class=\"data row3 col18\" >25</td>\n",
       "      <td id=\"T_7f69d_row3_col19\" class=\"data row3 col19\" >Offer C</td>\n",
       "      <td id=\"T_7f69d_row3_col20\" class=\"data row3 col20\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row3_col21\" class=\"data row3 col21\" >19.760000</td>\n",
       "      <td id=\"T_7f69d_row3_col22\" class=\"data row3 col22\" >No</td>\n",
       "      <td id=\"T_7f69d_row3_col23\" class=\"data row3 col23\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row3_col24\" class=\"data row3 col24\" >Fiber Optic</td>\n",
       "      <td id=\"T_7f69d_row3_col25\" class=\"data row3 col25\" >12</td>\n",
       "      <td id=\"T_7f69d_row3_col26\" class=\"data row3 col26\" >No</td>\n",
       "      <td id=\"T_7f69d_row3_col27\" class=\"data row3 col27\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row3_col28\" class=\"data row3 col28\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row3_col29\" class=\"data row3 col29\" >No</td>\n",
       "      <td id=\"T_7f69d_row3_col30\" class=\"data row3 col30\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row3_col31\" class=\"data row3 col31\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row3_col32\" class=\"data row3 col32\" >No</td>\n",
       "      <td id=\"T_7f69d_row3_col33\" class=\"data row3 col33\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row3_col34\" class=\"data row3 col34\" >Month-to-Month</td>\n",
       "      <td id=\"T_7f69d_row3_col35\" class=\"data row3 col35\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row3_col36\" class=\"data row3 col36\" >Bank Withdrawal</td>\n",
       "      <td id=\"T_7f69d_row3_col37\" class=\"data row3 col37\" >98.500000</td>\n",
       "      <td id=\"T_7f69d_row3_col38\" class=\"data row3 col38\" >2514.500000</td>\n",
       "      <td id=\"T_7f69d_row3_col39\" class=\"data row3 col39\" >13.430000</td>\n",
       "      <td id=\"T_7f69d_row3_col40\" class=\"data row3 col40\" >0</td>\n",
       "      <td id=\"T_7f69d_row3_col41\" class=\"data row3 col41\" >494.000000</td>\n",
       "      <td id=\"T_7f69d_row3_col42\" class=\"data row3 col42\" >2995.070000</td>\n",
       "      <td id=\"T_7f69d_row3_col43\" class=\"data row3 col43\" >2</td>\n",
       "      <td id=\"T_7f69d_row3_col44\" class=\"data row3 col44\" >Churned</td>\n",
       "      <td id=\"T_7f69d_row3_col45\" class=\"data row3 col45\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row3_col46\" class=\"data row3 col46\" >88</td>\n",
       "      <td id=\"T_7f69d_row3_col47\" class=\"data row3 col47\" >5337</td>\n",
       "      <td id=\"T_7f69d_row3_col48\" class=\"data row3 col48\" >Dissatisfaction</td>\n",
       "      <td id=\"T_7f69d_row3_col49\" class=\"data row3 col49\" >Limited range of services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7f69d_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_7f69d_row4_col0\" class=\"data row4 col0\" >4846-WHAFZ</td>\n",
       "      <td id=\"T_7f69d_row4_col1\" class=\"data row4 col1\" >Female</td>\n",
       "      <td id=\"T_7f69d_row4_col2\" class=\"data row4 col2\" >80</td>\n",
       "      <td id=\"T_7f69d_row4_col3\" class=\"data row4 col3\" >No</td>\n",
       "      <td id=\"T_7f69d_row4_col4\" class=\"data row4 col4\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row4_col5\" class=\"data row4 col5\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row4_col6\" class=\"data row4 col6\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row4_col7\" class=\"data row4 col7\" >1</td>\n",
       "      <td id=\"T_7f69d_row4_col8\" class=\"data row4 col8\" >United States</td>\n",
       "      <td id=\"T_7f69d_row4_col9\" class=\"data row4 col9\" >California</td>\n",
       "      <td id=\"T_7f69d_row4_col10\" class=\"data row4 col10\" >Whittier</td>\n",
       "      <td id=\"T_7f69d_row4_col11\" class=\"data row4 col11\" >90602</td>\n",
       "      <td id=\"T_7f69d_row4_col12\" class=\"data row4 col12\" >33.972119</td>\n",
       "      <td id=\"T_7f69d_row4_col13\" class=\"data row4 col13\" >-118.020188</td>\n",
       "      <td id=\"T_7f69d_row4_col14\" class=\"data row4 col14\" >26265</td>\n",
       "      <td id=\"T_7f69d_row4_col15\" class=\"data row4 col15\" >Q3</td>\n",
       "      <td id=\"T_7f69d_row4_col16\" class=\"data row4 col16\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row4_col17\" class=\"data row4 col17\" >1</td>\n",
       "      <td id=\"T_7f69d_row4_col18\" class=\"data row4 col18\" >37</td>\n",
       "      <td id=\"T_7f69d_row4_col19\" class=\"data row4 col19\" >Offer C</td>\n",
       "      <td id=\"T_7f69d_row4_col20\" class=\"data row4 col20\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row4_col21\" class=\"data row4 col21\" >6.330000</td>\n",
       "      <td id=\"T_7f69d_row4_col22\" class=\"data row4 col22\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row4_col23\" class=\"data row4 col23\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row4_col24\" class=\"data row4 col24\" >Fiber Optic</td>\n",
       "      <td id=\"T_7f69d_row4_col25\" class=\"data row4 col25\" >14</td>\n",
       "      <td id=\"T_7f69d_row4_col26\" class=\"data row4 col26\" >No</td>\n",
       "      <td id=\"T_7f69d_row4_col27\" class=\"data row4 col27\" >No</td>\n",
       "      <td id=\"T_7f69d_row4_col28\" class=\"data row4 col28\" >No</td>\n",
       "      <td id=\"T_7f69d_row4_col29\" class=\"data row4 col29\" >No</td>\n",
       "      <td id=\"T_7f69d_row4_col30\" class=\"data row4 col30\" >No</td>\n",
       "      <td id=\"T_7f69d_row4_col31\" class=\"data row4 col31\" >No</td>\n",
       "      <td id=\"T_7f69d_row4_col32\" class=\"data row4 col32\" >No</td>\n",
       "      <td id=\"T_7f69d_row4_col33\" class=\"data row4 col33\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row4_col34\" class=\"data row4 col34\" >Month-to-Month</td>\n",
       "      <td id=\"T_7f69d_row4_col35\" class=\"data row4 col35\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row4_col36\" class=\"data row4 col36\" >Bank Withdrawal</td>\n",
       "      <td id=\"T_7f69d_row4_col37\" class=\"data row4 col37\" >76.500000</td>\n",
       "      <td id=\"T_7f69d_row4_col38\" class=\"data row4 col38\" >2868.150000</td>\n",
       "      <td id=\"T_7f69d_row4_col39\" class=\"data row4 col39\" >0.000000</td>\n",
       "      <td id=\"T_7f69d_row4_col40\" class=\"data row4 col40\" >0</td>\n",
       "      <td id=\"T_7f69d_row4_col41\" class=\"data row4 col41\" >234.210000</td>\n",
       "      <td id=\"T_7f69d_row4_col42\" class=\"data row4 col42\" >3102.360000</td>\n",
       "      <td id=\"T_7f69d_row4_col43\" class=\"data row4 col43\" >2</td>\n",
       "      <td id=\"T_7f69d_row4_col44\" class=\"data row4 col44\" >Churned</td>\n",
       "      <td id=\"T_7f69d_row4_col45\" class=\"data row4 col45\" >Yes</td>\n",
       "      <td id=\"T_7f69d_row4_col46\" class=\"data row4 col46\" >67</td>\n",
       "      <td id=\"T_7f69d_row4_col47\" class=\"data row4 col47\" >2793</td>\n",
       "      <td id=\"T_7f69d_row4_col48\" class=\"data row4 col48\" >Price</td>\n",
       "      <td id=\"T_7f69d_row4_col49\" class=\"data row4 col49\" >Extra data charges</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x166aa91c160>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first 10 rows of the dataset.\n",
    "data.head(5).style.set_caption(\n",
    "    \"<b>IBM Telco Customer Churn Dataset (First 5 Rows)</b>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Rename and Reorder the Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groups, flattens, and reorganizes dataset columns based on domain-specific categories (e.g., demographic, service usage, billing) to improve interpretability and facilitate targeted cleaning and feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8mTCr3-uHHbW"
   },
   "outputs": [],
   "source": [
    "# Define the feature mapping for the dataset.\n",
    "feature_mapping = {\n",
    "    \"Customer Info\": [\n",
    "        \"Customer ID\", \"Gender\", \"Age\", \"Under 30\", \"Senior Citizen\", \"Married\", \"Dependents\", \"Number of Dependents\"\n",
    "    ],\n",
    "    \"Location Info\": [\n",
    "        \"Country\", \"State\", \"City\", \"Zip Code\", \"Latitude\", \"Longitude\", \"Population\"\n",
    "    ],\n",
    "    \"Referral & Tenure\": [\n",
    "        \"Quarter\", \"Referred a Friend\", \"Number of Referrals\", \"Tenure in Months\", \"Offer\"\n",
    "    ],\n",
    "    \"Services Signed Up\": [\n",
    "        \"Phone Service\", \"Multiple Lines\", \"Internet Service\", \"Internet Type\", \"Unlimited Data\"\n",
    "    ],\n",
    "    \"Internet Features\": [\n",
    "        \"Online Security\", \"Online Backup\", \"Device Protection Plan\", \"Premium Tech Support\",\n",
    "        \"Streaming TV\", \"Streaming Movies\", \"Streaming Music\"\n",
    "    ],\n",
    "    \"Billing & Payment\": [\n",
    "        \"Avg Monthly Long Distance Charges\", \"Avg Monthly GB Download\", \"Monthly Charge\",\n",
    "        \"Total Charges\", \"Total Refunds\", \"Total Extra Data Charges\",\n",
    "        \"Total Long Distance Charges\", \"Total Revenue\", \"Paperless Billing\", \"Payment Method\"\n",
    "    ],\n",
    "    \"Customer Scores\": [\n",
    "        \"Satisfaction Score\", \"CLTV\", \"Churn Score\"\n",
    "    ],\n",
    "    \"Churn Info\": [\n",
    "        \"Customer Status\", \"Churn Label\", \"Churn Category\", \"Churn Reason\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZrbQG0MaHHbW"
   },
   "outputs": [],
   "source": [
    "# Flatten the feature_mapping into a single list.\n",
    "desired_order = [col for group in feature_mapping.values() for col in group]\n",
    "\n",
    "# Reorder the DataFrame.\n",
    "data = data[desired_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Remove the Unnecessary Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifies and removes features based on redundancy, low variance, potential data leakage, high cardinality, and irrelevance. This ensures the modeling pipeline remains efficient and statistically sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "4YmalLFDHHbW"
   },
   "outputs": [],
   "source": [
    "# Define the features to remove from the dataset.\n",
    "features_to_remove = [\n",
    "    # Identifiers & Redundant Demographics\n",
    "    \"Customer ID\",           # High cardinality identifier\n",
    "    \"Under 30\",              # Redundant (derivable from Age)\n",
    "    \"Dependents\",            # Redundant (derivable from Number of Dependents)\n",
    "\n",
    "    # Location Info (low variance or low utility)\n",
    "    \"Country\",               # Constant (all United States)\n",
    "    \"State\",                 # Constant (all California)\n",
    "    \"Zip Code\",              # Too granular\n",
    "    \"City\",                  # High cardinality, many unique values\n",
    "    \"Latitude\",              # Granular\n",
    "    \"Longitude\",             # Granular\n",
    "    \"Population\",            # Possibly low variation or correlated with city\n",
    "\n",
    "    # Referral\n",
    "    \"Referred a Friend\",     # Redundant (derivable from Number of Referrals)\n",
    "\n",
    "    # Subscription Redundancy\n",
    "    \"Internet Service\",      # Redundant (inferable from Internet Type)\n",
    "\n",
    "    # Derived or Leaky Features\n",
    "    \"Customer Status\",       # Leaks churn label\n",
    "    \"Churn Score\",           # Usually post-hoc score, potential leakage\n",
    "    \"Churn Category\",        # Sparse & derived from churn\n",
    "    \"Churn Reason\",          # Sparse & derived from churn\n",
    "    \"CLTV\",                  # Leaks churn label\n",
    "    \"Satisfaction Score\",    # Leaks churn label\n",
    "\n",
    "    # Time Feature\n",
    "    \"Quarter\",               # Possibly low relevance unless time modeling is intended\n",
    "\n",
    "    # Financial features removed in favor of only keeping Total Revenue\n",
    "    \"Avg Monthly Long Distance Charges\",    # Usage-level detail removed\n",
    "    \"Avg Monthly GB Download\",              # Usage-level detail removed\n",
    "    \"Monthly Charge\",                       # Snapshot charge removed\n",
    "    \"Total Charges\",                        # Cumulative but derived\n",
    "    \"Total Refunds\",                        # Post-hoc financial info\n",
    "    \"Total Extra Data Charges\",             # Specific fee detail removed\n",
    "    \"Total Long Distance Charges\"           # Specific usage-based revenue removed\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "qMu80bKkHHbX"
   },
   "outputs": [],
   "source": [
    "# Remove the specified columns from the DataFrame.\n",
    "data = data.drop(\n",
    "    columns=features_to_remove\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Distinguish the Categorical and Numeric Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines explicit lists of categorical and numeric features to enable precise and appropriate transformations in later preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "WLj9_zOTHHbX"
   },
   "outputs": [],
   "source": [
    "# Define the categorical features in the dataset.\n",
    "categorical_features = [\n",
    "    \"Gender\",\n",
    "    \"Senior Citizen\",\n",
    "    \"Married\",\n",
    "    \"Offer\",\n",
    "    \"Phone Service\",\n",
    "    \"Multiple Lines\",\n",
    "    \"Internet Type\",\n",
    "    \"Unlimited Data\",\n",
    "    \"Online Security\",\n",
    "    \"Online Backup\",\n",
    "    \"Device Protection Plan\",\n",
    "    \"Premium Tech Support\",\n",
    "    \"Streaming TV\",\n",
    "    \"Streaming Movies\",\n",
    "    \"Streaming Music\",\n",
    "    \"Paperless Billing\",\n",
    "    \"Payment Method\",\n",
    "    \"Churn Label\"\n",
    "]\n",
    "\n",
    "# Define the numeric features in the dataset.\n",
    "numeric_features = [\n",
    "    \"Age\",\n",
    "    \"Number of Dependents\",\n",
    "    \"Number of Referrals\",\n",
    "    \"Tenure in Months\",\n",
    "    \"Total Revenue\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6. Handle Outlier Values for Numeric Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applies interquartile range (IQR) based clipping to cap outliers in numeric features, mitigating their impact on modeling algorithms while preserving core data distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each numeric feature to compute and apply IQR-based clipping.\n",
    "for feature in numeric_features:\n",
    "    if feature != \"Number of Dependents\":\n",
    "        q1, q3 = data[feature].quantile(\n",
    "            [0.25, 0.75]\n",
    "        )\n",
    "        \n",
    "        iqr = q3 - q1\n",
    "\n",
    "        lower_bound, upper_bound = (\n",
    "            q1 - 1.5 * iqr,\n",
    "            q3 + 1.5 * iqr\n",
    "        )\n",
    "\n",
    "        data[feature] = data[feature].clip(\n",
    "            lower=lower_bound,\n",
    "            upper=upper_bound\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7. Check the Categorical Features' Unique Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes frequency and percentage distributions for unique values in categorical features, aiding in understanding class distributions and guiding encoding strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_l1AgcPJHHbX"
   },
   "outputs": [],
   "source": [
    "# Define a vacant list to store the rows.\n",
    "rows = []\n",
    "\n",
    "# Iterate through the categorical features.\n",
    "for feature in categorical_features:\n",
    "\n",
    "    # Get the unique values and their counts.\n",
    "    value_counts = data[feature].value_counts()\n",
    "    first_row = True\n",
    "\n",
    "    # Iterate through the unique values.\n",
    "    for value, count in value_counts.items():\n",
    "\n",
    "        # Calculate the percentage.\n",
    "        percentage = str(round((count / len(data)) * 100, 2))\n",
    "        # Set the feature name.\n",
    "        feature_name = feature if first_row else \"\"\n",
    "        # Append the unique value, count, and percentage to the rows.\n",
    "        rows.append([\n",
    "            feature_name,\n",
    "            value,\n",
    "            count,\n",
    "            percentage\n",
    "        ])\n",
    "        # Set the first row to False.\n",
    "        first_row = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3Nje11wEHHbX"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_16a36\">\n",
       "  <caption><b>Unique Values in Categorical Features</b></caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_16a36_level0_col0\" class=\"col_heading level0 col0\" >Feature</th>\n",
       "      <th id=\"T_16a36_level0_col1\" class=\"col_heading level0 col1\" >Unique Value</th>\n",
       "      <th id=\"T_16a36_level0_col2\" class=\"col_heading level0 col2\" >Frequency</th>\n",
       "      <th id=\"T_16a36_level0_col3\" class=\"col_heading level0 col3\" >Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row0_col0\" class=\"data row0 col0\" >Gender</td>\n",
       "      <td id=\"T_16a36_row0_col1\" class=\"data row0 col1\" >Male</td>\n",
       "      <td id=\"T_16a36_row0_col2\" class=\"data row0 col2\" >3555</td>\n",
       "      <td id=\"T_16a36_row0_col3\" class=\"data row0 col3\" >50.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row1_col0\" class=\"data row1 col0\" ></td>\n",
       "      <td id=\"T_16a36_row1_col1\" class=\"data row1 col1\" >Female</td>\n",
       "      <td id=\"T_16a36_row1_col2\" class=\"data row1 col2\" >3488</td>\n",
       "      <td id=\"T_16a36_row1_col3\" class=\"data row1 col3\" >49.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row2_col0\" class=\"data row2 col0\" >Senior Citizen</td>\n",
       "      <td id=\"T_16a36_row2_col1\" class=\"data row2 col1\" >No</td>\n",
       "      <td id=\"T_16a36_row2_col2\" class=\"data row2 col2\" >5901</td>\n",
       "      <td id=\"T_16a36_row2_col3\" class=\"data row2 col3\" >83.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row3_col0\" class=\"data row3 col0\" ></td>\n",
       "      <td id=\"T_16a36_row3_col1\" class=\"data row3 col1\" >Yes</td>\n",
       "      <td id=\"T_16a36_row3_col2\" class=\"data row3 col2\" >1142</td>\n",
       "      <td id=\"T_16a36_row3_col3\" class=\"data row3 col3\" >16.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row4_col0\" class=\"data row4 col0\" >Married</td>\n",
       "      <td id=\"T_16a36_row4_col1\" class=\"data row4 col1\" >No</td>\n",
       "      <td id=\"T_16a36_row4_col2\" class=\"data row4 col2\" >3641</td>\n",
       "      <td id=\"T_16a36_row4_col3\" class=\"data row4 col3\" >51.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row5_col0\" class=\"data row5 col0\" ></td>\n",
       "      <td id=\"T_16a36_row5_col1\" class=\"data row5 col1\" >Yes</td>\n",
       "      <td id=\"T_16a36_row5_col2\" class=\"data row5 col2\" >3402</td>\n",
       "      <td id=\"T_16a36_row5_col3\" class=\"data row5 col3\" >48.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row6_col0\" class=\"data row6 col0\" >Offer</td>\n",
       "      <td id=\"T_16a36_row6_col1\" class=\"data row6 col1\" >None</td>\n",
       "      <td id=\"T_16a36_row6_col2\" class=\"data row6 col2\" >3877</td>\n",
       "      <td id=\"T_16a36_row6_col3\" class=\"data row6 col3\" >55.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row7_col0\" class=\"data row7 col0\" ></td>\n",
       "      <td id=\"T_16a36_row7_col1\" class=\"data row7 col1\" >Offer B</td>\n",
       "      <td id=\"T_16a36_row7_col2\" class=\"data row7 col2\" >824</td>\n",
       "      <td id=\"T_16a36_row7_col3\" class=\"data row7 col3\" >11.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row8_col0\" class=\"data row8 col0\" ></td>\n",
       "      <td id=\"T_16a36_row8_col1\" class=\"data row8 col1\" >Offer E</td>\n",
       "      <td id=\"T_16a36_row8_col2\" class=\"data row8 col2\" >805</td>\n",
       "      <td id=\"T_16a36_row8_col3\" class=\"data row8 col3\" >11.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row9_col0\" class=\"data row9 col0\" ></td>\n",
       "      <td id=\"T_16a36_row9_col1\" class=\"data row9 col1\" >Offer D</td>\n",
       "      <td id=\"T_16a36_row9_col2\" class=\"data row9 col2\" >602</td>\n",
       "      <td id=\"T_16a36_row9_col3\" class=\"data row9 col3\" >8.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row10_col0\" class=\"data row10 col0\" ></td>\n",
       "      <td id=\"T_16a36_row10_col1\" class=\"data row10 col1\" >Offer A</td>\n",
       "      <td id=\"T_16a36_row10_col2\" class=\"data row10 col2\" >520</td>\n",
       "      <td id=\"T_16a36_row10_col3\" class=\"data row10 col3\" >7.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row11_col0\" class=\"data row11 col0\" ></td>\n",
       "      <td id=\"T_16a36_row11_col1\" class=\"data row11 col1\" >Offer C</td>\n",
       "      <td id=\"T_16a36_row11_col2\" class=\"data row11 col2\" >415</td>\n",
       "      <td id=\"T_16a36_row11_col3\" class=\"data row11 col3\" >5.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row12_col0\" class=\"data row12 col0\" >Phone Service</td>\n",
       "      <td id=\"T_16a36_row12_col1\" class=\"data row12 col1\" >Yes</td>\n",
       "      <td id=\"T_16a36_row12_col2\" class=\"data row12 col2\" >6361</td>\n",
       "      <td id=\"T_16a36_row12_col3\" class=\"data row12 col3\" >90.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row13_col0\" class=\"data row13 col0\" ></td>\n",
       "      <td id=\"T_16a36_row13_col1\" class=\"data row13 col1\" >No</td>\n",
       "      <td id=\"T_16a36_row13_col2\" class=\"data row13 col2\" >682</td>\n",
       "      <td id=\"T_16a36_row13_col3\" class=\"data row13 col3\" >9.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row14_col0\" class=\"data row14 col0\" >Multiple Lines</td>\n",
       "      <td id=\"T_16a36_row14_col1\" class=\"data row14 col1\" >No</td>\n",
       "      <td id=\"T_16a36_row14_col2\" class=\"data row14 col2\" >4072</td>\n",
       "      <td id=\"T_16a36_row14_col3\" class=\"data row14 col3\" >57.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row15_col0\" class=\"data row15 col0\" ></td>\n",
       "      <td id=\"T_16a36_row15_col1\" class=\"data row15 col1\" >Yes</td>\n",
       "      <td id=\"T_16a36_row15_col2\" class=\"data row15 col2\" >2971</td>\n",
       "      <td id=\"T_16a36_row15_col3\" class=\"data row15 col3\" >42.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row16_col0\" class=\"data row16 col0\" >Internet Type</td>\n",
       "      <td id=\"T_16a36_row16_col1\" class=\"data row16 col1\" >Fiber Optic</td>\n",
       "      <td id=\"T_16a36_row16_col2\" class=\"data row16 col2\" >3035</td>\n",
       "      <td id=\"T_16a36_row16_col3\" class=\"data row16 col3\" >43.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row17_col0\" class=\"data row17 col0\" ></td>\n",
       "      <td id=\"T_16a36_row17_col1\" class=\"data row17 col1\" >DSL</td>\n",
       "      <td id=\"T_16a36_row17_col2\" class=\"data row17 col2\" >1652</td>\n",
       "      <td id=\"T_16a36_row17_col3\" class=\"data row17 col3\" >23.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row18_col0\" class=\"data row18 col0\" ></td>\n",
       "      <td id=\"T_16a36_row18_col1\" class=\"data row18 col1\" >None</td>\n",
       "      <td id=\"T_16a36_row18_col2\" class=\"data row18 col2\" >1526</td>\n",
       "      <td id=\"T_16a36_row18_col3\" class=\"data row18 col3\" >21.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row19_col0\" class=\"data row19 col0\" ></td>\n",
       "      <td id=\"T_16a36_row19_col1\" class=\"data row19 col1\" >Cable</td>\n",
       "      <td id=\"T_16a36_row19_col2\" class=\"data row19 col2\" >830</td>\n",
       "      <td id=\"T_16a36_row19_col3\" class=\"data row19 col3\" >11.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row20_col0\" class=\"data row20 col0\" >Unlimited Data</td>\n",
       "      <td id=\"T_16a36_row20_col1\" class=\"data row20 col1\" >Yes</td>\n",
       "      <td id=\"T_16a36_row20_col2\" class=\"data row20 col2\" >4745</td>\n",
       "      <td id=\"T_16a36_row20_col3\" class=\"data row20 col3\" >67.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row21_col0\" class=\"data row21 col0\" ></td>\n",
       "      <td id=\"T_16a36_row21_col1\" class=\"data row21 col1\" >No</td>\n",
       "      <td id=\"T_16a36_row21_col2\" class=\"data row21 col2\" >2298</td>\n",
       "      <td id=\"T_16a36_row21_col3\" class=\"data row21 col3\" >32.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row22_col0\" class=\"data row22 col0\" >Online Security</td>\n",
       "      <td id=\"T_16a36_row22_col1\" class=\"data row22 col1\" >No</td>\n",
       "      <td id=\"T_16a36_row22_col2\" class=\"data row22 col2\" >5024</td>\n",
       "      <td id=\"T_16a36_row22_col3\" class=\"data row22 col3\" >71.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row23_col0\" class=\"data row23 col0\" ></td>\n",
       "      <td id=\"T_16a36_row23_col1\" class=\"data row23 col1\" >Yes</td>\n",
       "      <td id=\"T_16a36_row23_col2\" class=\"data row23 col2\" >2019</td>\n",
       "      <td id=\"T_16a36_row23_col3\" class=\"data row23 col3\" >28.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row24_col0\" class=\"data row24 col0\" >Online Backup</td>\n",
       "      <td id=\"T_16a36_row24_col1\" class=\"data row24 col1\" >No</td>\n",
       "      <td id=\"T_16a36_row24_col2\" class=\"data row24 col2\" >4614</td>\n",
       "      <td id=\"T_16a36_row24_col3\" class=\"data row24 col3\" >65.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row25_col0\" class=\"data row25 col0\" ></td>\n",
       "      <td id=\"T_16a36_row25_col1\" class=\"data row25 col1\" >Yes</td>\n",
       "      <td id=\"T_16a36_row25_col2\" class=\"data row25 col2\" >2429</td>\n",
       "      <td id=\"T_16a36_row25_col3\" class=\"data row25 col3\" >34.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row26_col0\" class=\"data row26 col0\" >Device Protection Plan</td>\n",
       "      <td id=\"T_16a36_row26_col1\" class=\"data row26 col1\" >No</td>\n",
       "      <td id=\"T_16a36_row26_col2\" class=\"data row26 col2\" >4621</td>\n",
       "      <td id=\"T_16a36_row26_col3\" class=\"data row26 col3\" >65.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row27_col0\" class=\"data row27 col0\" ></td>\n",
       "      <td id=\"T_16a36_row27_col1\" class=\"data row27 col1\" >Yes</td>\n",
       "      <td id=\"T_16a36_row27_col2\" class=\"data row27 col2\" >2422</td>\n",
       "      <td id=\"T_16a36_row27_col3\" class=\"data row27 col3\" >34.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row28_col0\" class=\"data row28 col0\" >Premium Tech Support</td>\n",
       "      <td id=\"T_16a36_row28_col1\" class=\"data row28 col1\" >No</td>\n",
       "      <td id=\"T_16a36_row28_col2\" class=\"data row28 col2\" >4999</td>\n",
       "      <td id=\"T_16a36_row28_col3\" class=\"data row28 col3\" >70.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row29_col0\" class=\"data row29 col0\" ></td>\n",
       "      <td id=\"T_16a36_row29_col1\" class=\"data row29 col1\" >Yes</td>\n",
       "      <td id=\"T_16a36_row29_col2\" class=\"data row29 col2\" >2044</td>\n",
       "      <td id=\"T_16a36_row29_col3\" class=\"data row29 col3\" >29.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row30_col0\" class=\"data row30 col0\" >Streaming TV</td>\n",
       "      <td id=\"T_16a36_row30_col1\" class=\"data row30 col1\" >No</td>\n",
       "      <td id=\"T_16a36_row30_col2\" class=\"data row30 col2\" >4336</td>\n",
       "      <td id=\"T_16a36_row30_col3\" class=\"data row30 col3\" >61.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row31_col0\" class=\"data row31 col0\" ></td>\n",
       "      <td id=\"T_16a36_row31_col1\" class=\"data row31 col1\" >Yes</td>\n",
       "      <td id=\"T_16a36_row31_col2\" class=\"data row31 col2\" >2707</td>\n",
       "      <td id=\"T_16a36_row31_col3\" class=\"data row31 col3\" >38.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row32_col0\" class=\"data row32 col0\" >Streaming Movies</td>\n",
       "      <td id=\"T_16a36_row32_col1\" class=\"data row32 col1\" >No</td>\n",
       "      <td id=\"T_16a36_row32_col2\" class=\"data row32 col2\" >4311</td>\n",
       "      <td id=\"T_16a36_row32_col3\" class=\"data row32 col3\" >61.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row33_col0\" class=\"data row33 col0\" ></td>\n",
       "      <td id=\"T_16a36_row33_col1\" class=\"data row33 col1\" >Yes</td>\n",
       "      <td id=\"T_16a36_row33_col2\" class=\"data row33 col2\" >2732</td>\n",
       "      <td id=\"T_16a36_row33_col3\" class=\"data row33 col3\" >38.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row34_col0\" class=\"data row34 col0\" >Streaming Music</td>\n",
       "      <td id=\"T_16a36_row34_col1\" class=\"data row34 col1\" >No</td>\n",
       "      <td id=\"T_16a36_row34_col2\" class=\"data row34 col2\" >4555</td>\n",
       "      <td id=\"T_16a36_row34_col3\" class=\"data row34 col3\" >64.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row35_col0\" class=\"data row35 col0\" ></td>\n",
       "      <td id=\"T_16a36_row35_col1\" class=\"data row35 col1\" >Yes</td>\n",
       "      <td id=\"T_16a36_row35_col2\" class=\"data row35 col2\" >2488</td>\n",
       "      <td id=\"T_16a36_row35_col3\" class=\"data row35 col3\" >35.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row36_col0\" class=\"data row36 col0\" >Paperless Billing</td>\n",
       "      <td id=\"T_16a36_row36_col1\" class=\"data row36 col1\" >Yes</td>\n",
       "      <td id=\"T_16a36_row36_col2\" class=\"data row36 col2\" >4171</td>\n",
       "      <td id=\"T_16a36_row36_col3\" class=\"data row36 col3\" >59.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row37_col0\" class=\"data row37 col0\" ></td>\n",
       "      <td id=\"T_16a36_row37_col1\" class=\"data row37 col1\" >No</td>\n",
       "      <td id=\"T_16a36_row37_col2\" class=\"data row37 col2\" >2872</td>\n",
       "      <td id=\"T_16a36_row37_col3\" class=\"data row37 col3\" >40.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row38_col0\" class=\"data row38 col0\" >Payment Method</td>\n",
       "      <td id=\"T_16a36_row38_col1\" class=\"data row38 col1\" >Bank Withdrawal</td>\n",
       "      <td id=\"T_16a36_row38_col2\" class=\"data row38 col2\" >3909</td>\n",
       "      <td id=\"T_16a36_row38_col3\" class=\"data row38 col3\" >55.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row39_col0\" class=\"data row39 col0\" ></td>\n",
       "      <td id=\"T_16a36_row39_col1\" class=\"data row39 col1\" >Credit Card</td>\n",
       "      <td id=\"T_16a36_row39_col2\" class=\"data row39 col2\" >2749</td>\n",
       "      <td id=\"T_16a36_row39_col3\" class=\"data row39 col3\" >39.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row40_col0\" class=\"data row40 col0\" ></td>\n",
       "      <td id=\"T_16a36_row40_col1\" class=\"data row40 col1\" >Mailed Check</td>\n",
       "      <td id=\"T_16a36_row40_col2\" class=\"data row40 col2\" >385</td>\n",
       "      <td id=\"T_16a36_row40_col3\" class=\"data row40 col3\" >5.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row41_col0\" class=\"data row41 col0\" >Churn Label</td>\n",
       "      <td id=\"T_16a36_row41_col1\" class=\"data row41 col1\" >No</td>\n",
       "      <td id=\"T_16a36_row41_col2\" class=\"data row41 col2\" >5174</td>\n",
       "      <td id=\"T_16a36_row41_col3\" class=\"data row41 col3\" >73.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_16a36_row42_col0\" class=\"data row42 col0\" ></td>\n",
       "      <td id=\"T_16a36_row42_col1\" class=\"data row42 col1\" >Yes</td>\n",
       "      <td id=\"T_16a36_row42_col2\" class=\"data row42 col2\" >1869</td>\n",
       "      <td id=\"T_16a36_row42_col3\" class=\"data row42 col3\" >26.54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x166ec891720>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataFrame of unique values.\n",
    "unique_values_df = pd.DataFrame(\n",
    "    rows,\n",
    "    columns=[\"Feature\", \"Unique Value\", \"Frequency\", \"Percentage\"]\n",
    ")\n",
    "\n",
    "# Round the percentage to 2 decimal places.\n",
    "# unique_values_df[\"Percentage\"] = unique_values_df[\"Percentage\"].round(2)\n",
    "unique_values_df[\"Percentage\"] = unique_values_df[\"Percentage\"]\n",
    "\n",
    "# Style the unique values dataFrame.\n",
    "unique_values_df = (\n",
    "    unique_values_df.style\n",
    "    .set_caption(\"<b>Unique Values in Categorical Features</b>\")\n",
    "    .hide(axis=\"index\")\n",
    ")\n",
    "\n",
    "# Display the unique values in the categorical features.\n",
    "unique_values_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIkLFaaFHHbX"
   },
   "source": [
    "## 3.8. Check the Numeric Features' Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates summary statistics (e.g., mean, std, min, max) for numeric features to inspect data ranges, central tendencies, and spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "bvXCgre6HHbX"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Number of Dependents</th>\n",
       "      <th>Number of Referrals</th>\n",
       "      <th>Tenure in Months</th>\n",
       "      <th>Total Revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7043.00</td>\n",
       "      <td>7043.00</td>\n",
       "      <td>7043.00</td>\n",
       "      <td>7043.00</td>\n",
       "      <td>7043.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>46.51</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1.81</td>\n",
       "      <td>32.39</td>\n",
       "      <td>3033.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>16.75</td>\n",
       "      <td>0.96</td>\n",
       "      <td>2.66</td>\n",
       "      <td>24.54</td>\n",
       "      <td>2861.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>19.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>21.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>32.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>605.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>46.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>2108.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>60.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>4801.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>80.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>7.50</td>\n",
       "      <td>72.00</td>\n",
       "      <td>11094.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Age  Number of Dependents  Number of Referrals  Tenure in Months  \\\n",
       "count  7043.00               7043.00              7043.00           7043.00   \n",
       "mean     46.51                  0.47                 1.81             32.39   \n",
       "std      16.75                  0.96                 2.66             24.54   \n",
       "min      19.00                  0.00                 0.00              1.00   \n",
       "25%      32.00                  0.00                 0.00              9.00   \n",
       "50%      46.00                  0.00                 0.00             29.00   \n",
       "75%      60.00                  0.00                 3.00             55.00   \n",
       "max      80.00                  9.00                 7.50             72.00   \n",
       "\n",
       "       Total Revenue  \n",
       "count        7043.00  \n",
       "mean         3033.27  \n",
       "std          2861.98  \n",
       "min            21.36  \n",
       "25%           605.61  \n",
       "50%          2108.64  \n",
       "75%          4801.15  \n",
       "max         11094.45  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[numeric_features].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9. Check for Missing Values in the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes the percentage of non-missing entries per feature, presented in a tabular format, to diagnose data completeness and identify missing value patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "VGmYfnhJHHbY"
   },
   "outputs": [],
   "source": [
    "# Find out the missing values percentage in the dataset.\n",
    "non_missing_percentage = data.notnull().mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "YLU8qea3HHbY"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_1ad45\">\n",
       "  <caption><b>Non-Missing Percentage of Features</b></caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_1ad45_level0_col0\" class=\"col_heading level0 col0\" >Feature</th>\n",
       "      <th id=\"T_1ad45_level0_col1\" class=\"col_heading level0 col1\" >Non-Missing Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad45_level0_row0\" class=\"row_heading level0 row0\" >1</th>\n",
       "      <td id=\"T_1ad45_row0_col0\" class=\"data row0 col0\" >Gender</td>\n",
       "      <td id=\"T_1ad45_row0_col1\" class=\"data row0 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad45_level0_row1\" class=\"row_heading level0 row1\" >2</th>\n",
       "      <td id=\"T_1ad45_row1_col0\" class=\"data row1 col0\" >Age</td>\n",
       "      <td id=\"T_1ad45_row1_col1\" class=\"data row1 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad45_level0_row2\" class=\"row_heading level0 row2\" >3</th>\n",
       "      <td id=\"T_1ad45_row2_col0\" class=\"data row2 col0\" >Senior Citizen</td>\n",
       "      <td id=\"T_1ad45_row2_col1\" class=\"data row2 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad45_level0_row3\" class=\"row_heading level0 row3\" >4</th>\n",
       "      <td id=\"T_1ad45_row3_col0\" class=\"data row3 col0\" >Married</td>\n",
       "      <td id=\"T_1ad45_row3_col1\" class=\"data row3 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad45_level0_row4\" class=\"row_heading level0 row4\" >5</th>\n",
       "      <td id=\"T_1ad45_row4_col0\" class=\"data row4 col0\" >Number of Dependents</td>\n",
       "      <td id=\"T_1ad45_row4_col1\" class=\"data row4 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad45_level0_row5\" class=\"row_heading level0 row5\" >6</th>\n",
       "      <td id=\"T_1ad45_row5_col0\" class=\"data row5 col0\" >Number of Referrals</td>\n",
       "      <td id=\"T_1ad45_row5_col1\" class=\"data row5 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad45_level0_row6\" class=\"row_heading level0 row6\" >7</th>\n",
       "      <td id=\"T_1ad45_row6_col0\" class=\"data row6 col0\" >Tenure in Months</td>\n",
       "      <td id=\"T_1ad45_row6_col1\" class=\"data row6 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad45_level0_row7\" class=\"row_heading level0 row7\" >8</th>\n",
       "      <td id=\"T_1ad45_row7_col0\" class=\"data row7 col0\" >Offer</td>\n",
       "      <td id=\"T_1ad45_row7_col1\" class=\"data row7 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad45_level0_row8\" class=\"row_heading level0 row8\" >9</th>\n",
       "      <td id=\"T_1ad45_row8_col0\" class=\"data row8 col0\" >Phone Service</td>\n",
       "      <td id=\"T_1ad45_row8_col1\" class=\"data row8 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad45_level0_row9\" class=\"row_heading level0 row9\" >10</th>\n",
       "      <td id=\"T_1ad45_row9_col0\" class=\"data row9 col0\" >Multiple Lines</td>\n",
       "      <td id=\"T_1ad45_row9_col1\" class=\"data row9 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad45_level0_row10\" class=\"row_heading level0 row10\" >11</th>\n",
       "      <td id=\"T_1ad45_row10_col0\" class=\"data row10 col0\" >Internet Type</td>\n",
       "      <td id=\"T_1ad45_row10_col1\" class=\"data row10 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad45_level0_row11\" class=\"row_heading level0 row11\" >12</th>\n",
       "      <td id=\"T_1ad45_row11_col0\" class=\"data row11 col0\" >Unlimited Data</td>\n",
       "      <td id=\"T_1ad45_row11_col1\" class=\"data row11 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad45_level0_row12\" class=\"row_heading level0 row12\" >13</th>\n",
       "      <td id=\"T_1ad45_row12_col0\" class=\"data row12 col0\" >Online Security</td>\n",
       "      <td id=\"T_1ad45_row12_col1\" class=\"data row12 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad45_level0_row13\" class=\"row_heading level0 row13\" >14</th>\n",
       "      <td id=\"T_1ad45_row13_col0\" class=\"data row13 col0\" >Online Backup</td>\n",
       "      <td id=\"T_1ad45_row13_col1\" class=\"data row13 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad45_level0_row14\" class=\"row_heading level0 row14\" >15</th>\n",
       "      <td id=\"T_1ad45_row14_col0\" class=\"data row14 col0\" >Device Protection Plan</td>\n",
       "      <td id=\"T_1ad45_row14_col1\" class=\"data row14 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad45_level0_row15\" class=\"row_heading level0 row15\" >16</th>\n",
       "      <td id=\"T_1ad45_row15_col0\" class=\"data row15 col0\" >Premium Tech Support</td>\n",
       "      <td id=\"T_1ad45_row15_col1\" class=\"data row15 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad45_level0_row16\" class=\"row_heading level0 row16\" >17</th>\n",
       "      <td id=\"T_1ad45_row16_col0\" class=\"data row16 col0\" >Streaming TV</td>\n",
       "      <td id=\"T_1ad45_row16_col1\" class=\"data row16 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad45_level0_row17\" class=\"row_heading level0 row17\" >18</th>\n",
       "      <td id=\"T_1ad45_row17_col0\" class=\"data row17 col0\" >Streaming Movies</td>\n",
       "      <td id=\"T_1ad45_row17_col1\" class=\"data row17 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad45_level0_row18\" class=\"row_heading level0 row18\" >19</th>\n",
       "      <td id=\"T_1ad45_row18_col0\" class=\"data row18 col0\" >Streaming Music</td>\n",
       "      <td id=\"T_1ad45_row18_col1\" class=\"data row18 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad45_level0_row19\" class=\"row_heading level0 row19\" >20</th>\n",
       "      <td id=\"T_1ad45_row19_col0\" class=\"data row19 col0\" >Total Revenue</td>\n",
       "      <td id=\"T_1ad45_row19_col1\" class=\"data row19 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad45_level0_row20\" class=\"row_heading level0 row20\" >21</th>\n",
       "      <td id=\"T_1ad45_row20_col0\" class=\"data row20 col0\" >Paperless Billing</td>\n",
       "      <td id=\"T_1ad45_row20_col1\" class=\"data row20 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad45_level0_row21\" class=\"row_heading level0 row21\" >22</th>\n",
       "      <td id=\"T_1ad45_row21_col0\" class=\"data row21 col0\" >Payment Method</td>\n",
       "      <td id=\"T_1ad45_row21_col1\" class=\"data row21 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad45_level0_row22\" class=\"row_heading level0 row22\" >23</th>\n",
       "      <td id=\"T_1ad45_row22_col0\" class=\"data row22 col0\" >Churn Label</td>\n",
       "      <td id=\"T_1ad45_row22_col1\" class=\"data row22 col1\" >100.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x166ec33b160>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame from the non-missing percentage series.\n",
    "non_missing_df = pd.DataFrame(\n",
    "    non_missing_percentage,\n",
    "    columns=[\"Non-Missing Percentage\"]\n",
    ")\n",
    "\n",
    "# Change the index to a column named \"Feature\".\n",
    "non_missing_df = non_missing_df.reset_index().rename(\n",
    "    columns={\n",
    "        \"index\": \"Feature\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Increment the DataFrame index to start from 1.\n",
    "non_missing_df.index = non_missing_df.index + 1\n",
    "\n",
    "# Display the non-missing percentage table with two decimal places.\n",
    "non_missing_df.style.set_caption(\n",
    "    \"<b>Non-Missing Percentage of Features</b>\"\n",
    ").format(\n",
    "    {\n",
    "        \"Non-Missing Percentage\": \"{:.2f}\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.10. Fill the Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fills missing values in specific categorical features using domain-appropriate default values (e.g., \"No Offer\" for offers, \"No Internet\" for connection type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "cPvJMXngHHbe"
   },
   "outputs": [],
   "source": [
    "# Fill missing values in the dataset.\n",
    "data[\"Offer\"] = data[\"Offer\"].fillna(\"No Offer\")\n",
    "data[\"Internet Type\"] = data[\"Internet Type\"].fillna(\"No Internet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Oj466CmHHbe"
   },
   "source": [
    "## 3.11. Categoirze and Display the Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a side-by-side tabular display of the final sets of categorical and numeric features, providing a transparent overview of feature classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "fPI5FWvbHHbe"
   },
   "outputs": [],
   "source": [
    "# Pad the shorter list with empty strings.\n",
    "max_length = max(\n",
    "    len(categorical_features),\n",
    "    len(numeric_features)\n",
    ")\n",
    "\n",
    "categorical_features_feature = categorical_features.copy()\n",
    "numeric_features_feature = numeric_features.copy()\n",
    "\n",
    "categorical_features_feature += [\"\"] * (\n",
    "    max_length - len(categorical_features)\n",
    ")\n",
    "numeric_features_feature += [\"\"] * (\n",
    "    max_length - len(numeric_features)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "R72bNI83HHbe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_cc0eb\">\n",
       "  <caption><b>Categorization of Features by Type</b></caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_cc0eb_level0_col0\" class=\"col_heading level0 col0\" >Categorical Features</th>\n",
       "      <th id=\"T_cc0eb_level0_col1\" class=\"col_heading level0 col1\" >Numeric Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_cc0eb_level0_row0\" class=\"row_heading level0 row0\" >1</th>\n",
       "      <td id=\"T_cc0eb_row0_col0\" class=\"data row0 col0\" >Gender</td>\n",
       "      <td id=\"T_cc0eb_row0_col1\" class=\"data row0 col1\" >Age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cc0eb_level0_row1\" class=\"row_heading level0 row1\" >2</th>\n",
       "      <td id=\"T_cc0eb_row1_col0\" class=\"data row1 col0\" >Senior Citizen</td>\n",
       "      <td id=\"T_cc0eb_row1_col1\" class=\"data row1 col1\" >Number of Dependents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cc0eb_level0_row2\" class=\"row_heading level0 row2\" >3</th>\n",
       "      <td id=\"T_cc0eb_row2_col0\" class=\"data row2 col0\" >Married</td>\n",
       "      <td id=\"T_cc0eb_row2_col1\" class=\"data row2 col1\" >Number of Referrals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cc0eb_level0_row3\" class=\"row_heading level0 row3\" >4</th>\n",
       "      <td id=\"T_cc0eb_row3_col0\" class=\"data row3 col0\" >Offer</td>\n",
       "      <td id=\"T_cc0eb_row3_col1\" class=\"data row3 col1\" >Tenure in Months</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cc0eb_level0_row4\" class=\"row_heading level0 row4\" >5</th>\n",
       "      <td id=\"T_cc0eb_row4_col0\" class=\"data row4 col0\" >Phone Service</td>\n",
       "      <td id=\"T_cc0eb_row4_col1\" class=\"data row4 col1\" >Total Revenue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cc0eb_level0_row5\" class=\"row_heading level0 row5\" >6</th>\n",
       "      <td id=\"T_cc0eb_row5_col0\" class=\"data row5 col0\" >Multiple Lines</td>\n",
       "      <td id=\"T_cc0eb_row5_col1\" class=\"data row5 col1\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cc0eb_level0_row6\" class=\"row_heading level0 row6\" >7</th>\n",
       "      <td id=\"T_cc0eb_row6_col0\" class=\"data row6 col0\" >Internet Type</td>\n",
       "      <td id=\"T_cc0eb_row6_col1\" class=\"data row6 col1\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cc0eb_level0_row7\" class=\"row_heading level0 row7\" >8</th>\n",
       "      <td id=\"T_cc0eb_row7_col0\" class=\"data row7 col0\" >Unlimited Data</td>\n",
       "      <td id=\"T_cc0eb_row7_col1\" class=\"data row7 col1\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cc0eb_level0_row8\" class=\"row_heading level0 row8\" >9</th>\n",
       "      <td id=\"T_cc0eb_row8_col0\" class=\"data row8 col0\" >Online Security</td>\n",
       "      <td id=\"T_cc0eb_row8_col1\" class=\"data row8 col1\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cc0eb_level0_row9\" class=\"row_heading level0 row9\" >10</th>\n",
       "      <td id=\"T_cc0eb_row9_col0\" class=\"data row9 col0\" >Online Backup</td>\n",
       "      <td id=\"T_cc0eb_row9_col1\" class=\"data row9 col1\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cc0eb_level0_row10\" class=\"row_heading level0 row10\" >11</th>\n",
       "      <td id=\"T_cc0eb_row10_col0\" class=\"data row10 col0\" >Device Protection Plan</td>\n",
       "      <td id=\"T_cc0eb_row10_col1\" class=\"data row10 col1\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cc0eb_level0_row11\" class=\"row_heading level0 row11\" >12</th>\n",
       "      <td id=\"T_cc0eb_row11_col0\" class=\"data row11 col0\" >Premium Tech Support</td>\n",
       "      <td id=\"T_cc0eb_row11_col1\" class=\"data row11 col1\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cc0eb_level0_row12\" class=\"row_heading level0 row12\" >13</th>\n",
       "      <td id=\"T_cc0eb_row12_col0\" class=\"data row12 col0\" >Streaming TV</td>\n",
       "      <td id=\"T_cc0eb_row12_col1\" class=\"data row12 col1\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cc0eb_level0_row13\" class=\"row_heading level0 row13\" >14</th>\n",
       "      <td id=\"T_cc0eb_row13_col0\" class=\"data row13 col0\" >Streaming Movies</td>\n",
       "      <td id=\"T_cc0eb_row13_col1\" class=\"data row13 col1\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cc0eb_level0_row14\" class=\"row_heading level0 row14\" >15</th>\n",
       "      <td id=\"T_cc0eb_row14_col0\" class=\"data row14 col0\" >Streaming Music</td>\n",
       "      <td id=\"T_cc0eb_row14_col1\" class=\"data row14 col1\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cc0eb_level0_row15\" class=\"row_heading level0 row15\" >16</th>\n",
       "      <td id=\"T_cc0eb_row15_col0\" class=\"data row15 col0\" >Paperless Billing</td>\n",
       "      <td id=\"T_cc0eb_row15_col1\" class=\"data row15 col1\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cc0eb_level0_row16\" class=\"row_heading level0 row16\" >17</th>\n",
       "      <td id=\"T_cc0eb_row16_col0\" class=\"data row16 col0\" >Payment Method</td>\n",
       "      <td id=\"T_cc0eb_row16_col1\" class=\"data row16 col1\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cc0eb_level0_row17\" class=\"row_heading level0 row17\" >18</th>\n",
       "      <td id=\"T_cc0eb_row17_col0\" class=\"data row17 col0\" >Churn Label</td>\n",
       "      <td id=\"T_cc0eb_row17_col1\" class=\"data row17 col1\" ></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x166ec892c80>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame to display feature categorization.\n",
    "feature_types_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Categorical Features\": categorical_features_feature,\n",
    "        \"Numeric Features\": numeric_features_feature\n",
    "    }\n",
    ")\n",
    "\n",
    "# Increment the DataFrame index to start from 1.\n",
    "feature_types_df.index = feature_types_df.index + 1\n",
    "\n",
    "# Display the feature categorization table.\n",
    "feature_types_df.style.set_caption(\n",
    "    \"<b>Categorization of Features by Type</b>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.12. Check the Features' Datatypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displays data types of both categorical and numeric features to verify type correctness and compatibility for encoding and modeling operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "BtadamrmHHbf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_cac09\">\n",
       "  <caption><b>Categorical Features' Data Types</b></caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_cac09_level0_col0\" class=\"col_heading level0 col0\" >Categorical Features' Data Types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_cac09_level0_row0\" class=\"row_heading level0 row0\" >Gender</th>\n",
       "      <td id=\"T_cac09_row0_col0\" class=\"data row0 col0\" >object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cac09_level0_row1\" class=\"row_heading level0 row1\" >Senior Citizen</th>\n",
       "      <td id=\"T_cac09_row1_col0\" class=\"data row1 col0\" >object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cac09_level0_row2\" class=\"row_heading level0 row2\" >Married</th>\n",
       "      <td id=\"T_cac09_row2_col0\" class=\"data row2 col0\" >object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cac09_level0_row3\" class=\"row_heading level0 row3\" >Offer</th>\n",
       "      <td id=\"T_cac09_row3_col0\" class=\"data row3 col0\" >object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cac09_level0_row4\" class=\"row_heading level0 row4\" >Phone Service</th>\n",
       "      <td id=\"T_cac09_row4_col0\" class=\"data row4 col0\" >object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cac09_level0_row5\" class=\"row_heading level0 row5\" >Multiple Lines</th>\n",
       "      <td id=\"T_cac09_row5_col0\" class=\"data row5 col0\" >object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cac09_level0_row6\" class=\"row_heading level0 row6\" >Internet Type</th>\n",
       "      <td id=\"T_cac09_row6_col0\" class=\"data row6 col0\" >object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cac09_level0_row7\" class=\"row_heading level0 row7\" >Unlimited Data</th>\n",
       "      <td id=\"T_cac09_row7_col0\" class=\"data row7 col0\" >object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cac09_level0_row8\" class=\"row_heading level0 row8\" >Online Security</th>\n",
       "      <td id=\"T_cac09_row8_col0\" class=\"data row8 col0\" >object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cac09_level0_row9\" class=\"row_heading level0 row9\" >Online Backup</th>\n",
       "      <td id=\"T_cac09_row9_col0\" class=\"data row9 col0\" >object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cac09_level0_row10\" class=\"row_heading level0 row10\" >Device Protection Plan</th>\n",
       "      <td id=\"T_cac09_row10_col0\" class=\"data row10 col0\" >object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cac09_level0_row11\" class=\"row_heading level0 row11\" >Premium Tech Support</th>\n",
       "      <td id=\"T_cac09_row11_col0\" class=\"data row11 col0\" >object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cac09_level0_row12\" class=\"row_heading level0 row12\" >Streaming TV</th>\n",
       "      <td id=\"T_cac09_row12_col0\" class=\"data row12 col0\" >object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cac09_level0_row13\" class=\"row_heading level0 row13\" >Streaming Movies</th>\n",
       "      <td id=\"T_cac09_row13_col0\" class=\"data row13 col0\" >object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cac09_level0_row14\" class=\"row_heading level0 row14\" >Streaming Music</th>\n",
       "      <td id=\"T_cac09_row14_col0\" class=\"data row14 col0\" >object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cac09_level0_row15\" class=\"row_heading level0 row15\" >Paperless Billing</th>\n",
       "      <td id=\"T_cac09_row15_col0\" class=\"data row15 col0\" >object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cac09_level0_row16\" class=\"row_heading level0 row16\" >Payment Method</th>\n",
       "      <td id=\"T_cac09_row16_col0\" class=\"data row16 col0\" >object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cac09_level0_row17\" class=\"row_heading level0 row17\" >Churn Label</th>\n",
       "      <td id=\"T_cac09_row17_col0\" class=\"data row17 col0\" >object</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x166ec33b8b0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame to display the data types of categorical features.\n",
    "categorical_features_data_types = pd.DataFrame(\n",
    "    data[categorical_features].dtypes,\n",
    "    columns=[\"Categorical Features' Data Types\"]\n",
    ")\n",
    "\n",
    "# Display the data types table with a caption.\n",
    "categorical_features_data_types.style.set_caption(\n",
    "    \"<b>Categorical Features' Data Types</b>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "bmAbqAo4HHbf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_91d85\">\n",
       "  <caption>Numeric Features' Data Types</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_91d85_level0_col0\" class=\"col_heading level0 col0\" >Numeric Features' Data Types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_91d85_level0_row0\" class=\"row_heading level0 row0\" >Age</th>\n",
       "      <td id=\"T_91d85_row0_col0\" class=\"data row0 col0\" >int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_91d85_level0_row1\" class=\"row_heading level0 row1\" >Number of Dependents</th>\n",
       "      <td id=\"T_91d85_row1_col0\" class=\"data row1 col0\" >int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_91d85_level0_row2\" class=\"row_heading level0 row2\" >Number of Referrals</th>\n",
       "      <td id=\"T_91d85_row2_col0\" class=\"data row2 col0\" >float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_91d85_level0_row3\" class=\"row_heading level0 row3\" >Tenure in Months</th>\n",
       "      <td id=\"T_91d85_row3_col0\" class=\"data row3 col0\" >int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_91d85_level0_row4\" class=\"row_heading level0 row4\" >Total Revenue</th>\n",
       "      <td id=\"T_91d85_row4_col0\" class=\"data row4 col0\" >float64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x166ec8919c0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame to display the data types of numeric features.\n",
    "numeric_features_data_types = pd.DataFrame(\n",
    "    data[numeric_features].dtypes,\n",
    "    columns=[\"Numeric Features' Data Types\"]\n",
    ")\n",
    "\n",
    "# Display the data types table with a caption.\n",
    "numeric_features_data_types.style.set_caption(\n",
    "    \"Numeric Features' Data Types\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.13. Encode the Binary Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines and applies a function to map binary categorical values (e.g., \"Yes\"/\"No\", \"Male\"/\"Female\") to numeric binary format (1/0), ensuring compatibility with ML algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to encode binary features.\n",
    "def encode_binary_features(datasets, features, mapping):\n",
    "    \"\"\"\n",
    "    Applies binary encoding to specified features across multiple datasets.\n",
    "\n",
    "    Args:\n",
    "        datasets (List[pd.DataFrame]): A list of DataFrames to be modified in-place.\n",
    "        features (List[str]): The names of binary categorical features to encode.\n",
    "        mapping (dict): A dictionary mapping string categories to binary values.\n",
    "    \"\"\"\n",
    "    for df in datasets:\n",
    "        for feature in features:\n",
    "            df[feature] = df[feature].astype(str).map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define binary categorical features to be encoded.\n",
    "binary_features = [\n",
    "    \"Gender\",\n",
    "    \"Senior Citizen\",\n",
    "    \"Married\",\n",
    "    \"Phone Service\",\n",
    "    \"Multiple Lines\",\n",
    "    \"Unlimited Data\",\n",
    "    \"Online Security\",\n",
    "    \"Online Backup\",\n",
    "    \"Device Protection Plan\",\n",
    "    \"Premium Tech Support\",\n",
    "    \"Streaming TV\",\n",
    "    \"Streaming Movies\",\n",
    "    \"Streaming Music\",\n",
    "    \"Paperless Billing\",\n",
    "    \"Churn Label\"\n",
    "]\n",
    "\n",
    "# Define mapping for binary categories.\n",
    "binary_mapping = {\n",
    "    \"Yes\": 1,\n",
    "    \"No\": 0,\n",
    "    \"Male\": 1,\n",
    "    \"Female\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "aHqG0iPFHHbf"
   },
   "outputs": [],
   "source": [
    "# Apply binary encoding to all datasets.\n",
    "encode_binary_features(\n",
    "    datasets=[data],\n",
    "    features=binary_features,\n",
    "    mapping=binary_mapping\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.14. Encode the Ordinal Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encodes ordinal categorical features based on domain-informed hierarchical mappings (e.g., levels of service offers or payment methods), preserving their intrinsic order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "WOA9F4QFHHbf"
   },
   "outputs": [],
   "source": [
    "# Define a function to encode ordinal features.\n",
    "def encode_ordinal_features(datasets, mappings):\n",
    "    \"\"\"\n",
    "    Applies ordinal encoding to specified features across multiple datasets.\n",
    "\n",
    "    Args:\n",
    "        datasets (List[pd.DataFrame]): A list of DataFrames to be modified in-place.\n",
    "        mappings (dict): A dictionary where keys are feature names and values are mapping dicts.\n",
    "    \"\"\"\n",
    "    for df in datasets:\n",
    "        for feature, mapping in mappings.items():\n",
    "            df[feature] = df[feature].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "-SsRJfXCHHbf"
   },
   "outputs": [],
   "source": [
    "# Define the mappings for ordinal features.\n",
    "offer_mapping = {\n",
    "    \"No Offer\": 0,\n",
    "    \"Offer A\": 1,\n",
    "    \"Offer B\": 2,\n",
    "    \"Offer C\": 3,\n",
    "    \"Offer D\": 4,\n",
    "    \"Offer E\": 5\n",
    "}\n",
    "\n",
    "internet_type_mapping = {\n",
    "    \"No Internet\": 0,\n",
    "    \"DSL\": 1,\n",
    "    \"Cable\": 2,\n",
    "    \"Fiber Optic\": 3\n",
    "}\n",
    "\n",
    "payment_method_mapping = {\n",
    "    \"Mailed Check\": 1,\n",
    "    \"Bank Withdrawal\": 2,\n",
    "    \"Credit Card\": 3\n",
    "}\n",
    "\n",
    "# Create a dictionary of ordinal mappings.\n",
    "ordinal_mappings = {\n",
    "    \"Offer\": offer_mapping,\n",
    "    \"Internet Type\": internet_type_mapping,\n",
    "    \"Payment Method\": payment_method_mapping\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "f1VrFHsSHHbf"
   },
   "outputs": [],
   "source": [
    "# Apply ordinal encoding to all datasets.\n",
    "encode_ordinal_features(\n",
    "    datasets=[data],\n",
    "    mappings=ordinal_mappings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JMVMd3t4HHbf"
   },
   "source": [
    "## 3.15. Standard Scale the Numeric Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applies standard scaling to numeric features using StandardScaler to normalize distributions and ensure numerical comparability across features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "6JE5vbAMHHbg"
   },
   "outputs": [],
   "source": [
    "# Apply the scaler to normalize all numeric features in the dataset.\n",
    "scaler = StandardScaler()\n",
    "\n",
    "data[numeric_features] = scaler.fit_transform(data[numeric_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.16. Split the Data into Train and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divides the cleaned and transformed dataset into training and testing subsets with a reproducible 80/20 split, preparing for model development and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "train_data, test_data = train_test_split(\n",
    "    data,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **C. Define the Functions to Use in the Pypeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This primary section initiates the modular construction of a causal discovery pipeline. It encapsulates function definitions and abstractions critical to utility, validation, visualization, and algorithmic implementation. The ensuing subsections provide the computational foundation for generating constraint-aware causal graphs, analyzing their structural properties, and preparing the environment for comparative algorithm execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5be8wSS-HHbg"
   },
   "source": [
    "# 4. Define Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This subsection introduces a suite of foundational utility functions that serve to enforce domain-specific constraints, validate structural integrity, visualize causal graphs, extract interpretable relationships, and evaluate algorithmic outputs. Each function is essential for preprocessing, diagnostics, and ensuring consistency throughout the causal modeling workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Define the Constraint Matrix Creator Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function establishes a constraint matrix that encodes permissible and forbidden causal relationships among variables based on predefined hierarchical tiers and user-specified restrictions. The matrix (with NaN for allowed and 0.0 for disallowed edges) ensures that the causal discovery algorithms conform to domain knowledge, including treating demographic variables as root nodes and the churn label as a terminal node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "### Purpose and Context\n",
    "\n",
    "The purpose of the constraint matrix construction procedure is to encode prior knowledge and domain-specific assumptions about the causal structure among a set of variables. This matrix serves as a foundational component for constraint-based and score-based causal discovery algorithms by explicitly delineating which directed edges are permitted or forbidden. The encoded constraints enforce logical tier-based causal ordering, restrict self-contradictory relationships, and preserve interpretability of the inferred graph.\n",
    "\n",
    "### Inputs and Parameters\n",
    "\n",
    "Let $\\mathcal{N} = \\{v_1, \\dots, v_d\\}$ denote the set of variable names, partitioned into $T$ hierarchical tiers $\\mathcal{T}_1, \\dots, \\mathcal{T}_T$. The function takes the following inputs:\n",
    "\n",
    "- $\\mathcal{N}$: Ordered list of variable names.\n",
    "- $\\mathcal{T} = [\\mathcal{T}_1, \\dots, \\mathcal{T}_T]$: List of tier sets, where $\\mathcal{T}_k \\subseteq \\mathcal{N}$.\n",
    "- $\\mathcal{C}_{\\text{spec}} = (\\mathcal{C}_{\\text{forbid}}, \\mathcal{C}_{\\text{allow}})$: Optional set of user-defined constraints.\n",
    "\n",
    "The output is a matrix $\\mathcal{C} \\in \\mathbb{R}^{d \\times d}$ such that:\n",
    "\n",
    "- $\\mathcal{C}_{ij} = 0.0$: edge $v_i \\to v_j$ is forbidden.\n",
    "- $\\mathcal{C}_{ij} = \\text{NaN}$: edge $v_i \\to v_j$ is allowed.\n",
    "\n",
    "### Algorithmic Procedure\n",
    "\n",
    "1. **Matrix Initialization**:\n",
    "   - Initialize $\\mathcal{C}$ as a $d \\times d$ matrix with all entries set to $\\text{NaN}$ (permitting all edges by default).\n",
    "\n",
    "2. **Terminal Node Enforcement**:\n",
    "   - Identify the index $i^*$ of the \"Churn Label\" variable (if present).\n",
    "   - Set $\\mathcal{C}_{i^*, j} = 0.0$ for all $j$, prohibiting outgoing edges from \"Churn Label\".\n",
    "\n",
    "3. **Root Node Constraints**:\n",
    "   - For all variables $v_j \\in \\mathcal{T}_1$ (demographic tier), enforce:\n",
    "     $$\n",
    "     \\mathcal{C}_{i, j} = 0.0, \\quad \\forall i\n",
    "     $$\n",
    "     thereby restricting any incoming edges to demographic variables.\n",
    "\n",
    "4. **Intra-Tier Edge Blocking**:\n",
    "   - For $v_i, v_j \\in \\mathcal{T}_1$, $i \\neq j$, set $\\mathcal{C}_{ij} = 0.0$ to forbid causal edges among demographic variables.\n",
    "\n",
    "5. **Inter-Tier Edge Permissions**:\n",
    "   - For each tier $\\mathcal{T}_k$ and its subsequent tier $\\mathcal{T}_{k+1}$:\n",
    "     - Allow edges from $\\mathcal{T}_k$ to $\\mathcal{T}_{k+1}$ by maintaining $\\mathcal{C}_{ij} = \\text{NaN}$ for $v_i \\in \\mathcal{T}_k$, $v_j \\in \\mathcal{T}_{k+1}$.\n",
    "     - For all $v_j \\notin \\mathcal{T}_{k+1}$, set $\\mathcal{C}_{ij} = 0.0$ to restrict edge targets to the immediate downstream tier.\n",
    "\n",
    "6. **Specific Constraints Incorporation**:\n",
    "   - Apply custom constraints:\n",
    "     - For $(v_i, v_j) \\in \\mathcal{C}_{\\text{forbid}}$, set $\\mathcal{C}_{ij} = 0.0$.\n",
    "     - For $(v_i, v_j) \\in \\mathcal{C}_{\\text{allow}}$, set $\\mathcal{C}_{ij} = \\text{NaN}$.\n",
    "\n",
    "### Theoretical Justification\n",
    "\n",
    "This constraint encoding strategy ensures that the causal discovery process adheres to essential prior knowledge, including known causal ordering (tiers), variable roles (e.g., exogenous vs endogenous), and explicit domain rules. By defining the admissible edge set $\\mathcal{E}_{\\text{valid}} = \\{(i,j) : \\mathcal{C}_{ij} = \\text{NaN}\\}$, the matrix $\\mathcal{C}$ facilitates efficient search or pruning in downstream algorithms, thereby reducing the hypothesis space and improving estimation accuracy.\n",
    "\n",
    "### Outcomes and Limitations\n",
    "\n",
    "The output includes:\n",
    "- Constraint matrix $\\mathcal{C} \\in \\mathbb{R}^{d \\times d}$.\n",
    "- Mapping $\\phi: \\mathcal{N} \\rightarrow \\{1, \\dots, d\\}$ for node indices.\n",
    "\n",
    "This approach assumes that tiers and specific constraints are accurate and comprehensive. Errors or omissions in the provided tier structure or specific constraints may lead to incorrect causal exclusion or inclusion. The method does not support probabilistic or soft constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "g8edpdRXHHbg"
   },
   "outputs": [],
   "source": [
    "def create_constraint_matrix(node_names, tiers, specific_constraints=None):\n",
    "    \"\"\"\n",
    "    Create a constraint matrix for causal discovery algorithms.\n",
    "    \n",
    "    Args:\n",
    "        node_names (list): List of variable names.\n",
    "        tiers (list): List of tier lists (e.g., [demographic, customer, ...]).\n",
    "        specific_constraints (dict): Additional constraints (e.g., {\"forbidden\": [(src, dst), ...]}).\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Constraint matrix (np.nan for allowed edges, 0.0 for forbidden).\n",
    "    \"\"\"\n",
    "    num_nodes = len(node_names)\n",
    "    node_name_to_idx = {name: i for i, name in enumerate(node_names)}\n",
    "    constraint_matrix = np.full((num_nodes, num_nodes), np.nan, dtype=np.float32)\n",
    "    \n",
    "    # Set Churn Label as sink node (no outgoing edges)\n",
    "    churn_idx = node_name_to_idx.get(\"Churn Label\")\n",
    "    if churn_idx is not None:\n",
    "        constraint_matrix[churn_idx, :] = 0.0\n",
    "    \n",
    "    # Set demographic variables as root nodes (no incoming edges)\n",
    "    for feature in tiers[0]:  # Tier 1: Demographic\n",
    "        if feature in node_name_to_idx:\n",
    "            feature_idx = node_name_to_idx[feature]\n",
    "            constraint_matrix[:, feature_idx] = 0.0\n",
    "    \n",
    "    # Prevent edges within Tier 1\n",
    "    for src in tiers[0]:\n",
    "        for dst in tiers[0]:\n",
    "            if src != dst and src in node_name_to_idx and dst in node_name_to_idx:\n",
    "                constraint_matrix[node_name_to_idx[src], node_name_to_idx[dst]] = 0.0\n",
    "    \n",
    "    # Allow edges only from Tier N to Tier N+1\n",
    "    for src_tier_idx, src_tier in enumerate(tiers[:-1]):\n",
    "        dst_tier = tiers[src_tier_idx + 1]\n",
    "        for src in src_tier:\n",
    "            for dst in dst_tier:\n",
    "                if src in node_name_to_idx and dst in node_name_to_idx:\n",
    "                    constraint_matrix[node_name_to_idx[src], node_name_to_idx[dst]] = np.nan\n",
    "        # Block edges to other tiers\n",
    "        for other_tier_idx, other_tier in enumerate(tiers):\n",
    "            if other_tier_idx != src_tier_idx + 1:\n",
    "                for src in src_tier:\n",
    "                    for dst in other_tier:\n",
    "                        if src in node_name_to_idx and dst in node_name_to_idx:\n",
    "                            constraint_matrix[node_name_to_idx[src], node_name_to_idx[dst]] = 0.0\n",
    "    \n",
    "    # Apply specific constraints (e.g., Gender â†’ Service/Billing forbidden)\n",
    "    if specific_constraints:\n",
    "        for src, dst in specific_constraints.get(\"forbidden\", []):\n",
    "            if src in node_name_to_idx and dst in node_name_to_idx:\n",
    "                constraint_matrix[node_name_to_idx[src], node_name_to_idx[dst]] = 0.0\n",
    "        for src, dst in specific_constraints.get(\"allowed\", []):\n",
    "            if src in node_name_to_idx and dst in node_name_to_idx:\n",
    "                constraint_matrix[node_name_to_idx[src], node_name_to_idx[dst]] = np.nan\n",
    "    \n",
    "    logger.info(\"Constraint matrix created with shape: %s\", constraint_matrix.shape)\n",
    "    return constraint_matrix, node_name_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Define the Constraint Matrix Validator Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This variant of the constraint matrix creator is provided with enhanced commentary and clarity. It ensures that each constraintâ€”root variables, terminal nodes, tier transitions, and explicitly forbidden relationshipsâ€”is systematically imposed on the adjacency space of causal discovery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "### Purpose and Context\n",
    "\n",
    "The constraint matrix construction function is a systematic approach for encoding prior structural knowledge into the causal discovery process. It supports tier-based hierarchical modeling and the enforcement of both general and specific domain constraints. The resulting matrix is instrumental for constraining the hypothesis space of causal discovery algorithms, improving both interpretability and computational efficiency.\n",
    "\n",
    "### Inputs and Parameters\n",
    "\n",
    "Let $\\mathcal{N} = \\{v_1, \\dots, v_d\\}$ denote a set of $d$ variable names and $\\mathcal{T} = [\\mathcal{T}_1, \\dots, \\mathcal{T}_K]$ a list of $K$ tiers. The function accepts:\n",
    "\n",
    "- $\\mathcal{N}$: List of variable names.\n",
    "- $\\mathcal{T}$: List of disjoint variable subsets (tiers).\n",
    "- $\\mathcal{C}_{\\text{spec}}$: A dictionary of user-specified constraints containing:\n",
    "  - $\\mathcal{C}_{\\text{forbid}} = \\{(v_i, v_j)\\}$: Edges to forbid,\n",
    "  - $\\mathcal{C}_{\\text{allow}} = \\{(v_i, v_j)\\}$: Edges to explicitly permit.\n",
    "\n",
    "The output is:\n",
    "\n",
    "- $\\mathcal{C} \\in \\mathbb{R}^{d \\times d}$: A constraint matrix with:\n",
    "  - $\\mathcal{C}_{ij} = 0.0$ if edge $v_i \\to v_j$ is forbidden,\n",
    "  - $\\mathcal{C}_{ij} = \\text{NaN}$ if edge $v_i \\to v_j$ is allowed.\n",
    "- $\\phi: \\mathcal{N} \\rightarrow \\{1, \\dots, d\\}$: Mapping from variable names to indices.\n",
    "\n",
    "### Algorithmic Procedure\n",
    "\n",
    "1. **Initialization**:\n",
    "   - Create a $d \\times d$ matrix $\\mathcal{C}$ initialized to $\\text{NaN}$.\n",
    "\n",
    "2. **Terminal Variable Constraints**:\n",
    "   - Identify \"Churn Label\" as a terminal (sink) node. For its index $i$, set:\n",
    "     $$\n",
    "     \\mathcal{C}_{ij} = 0.0 \\quad \\forall j\n",
    "     $$\n",
    "\n",
    "3. **Exogenous Variable Constraints**:\n",
    "   - For each feature $v_j \\in \\mathcal{T}_1$ (demographics), enforce:\n",
    "     $$\n",
    "     \\mathcal{C}_{ij} = 0.0 \\quad \\forall i\n",
    "     $$\n",
    "   - Prevent intra-tier edges in $\\mathcal{T}_1$: for $v_i, v_j \\in \\mathcal{T}_1$, $i \\neq j$,\n",
    "     $$\n",
    "     \\mathcal{C}_{ij} = 0.0\n",
    "     $$\n",
    "\n",
    "4. **Inter-Tier Causal Flow**:\n",
    "   - For each tier index $k$, permit causal links only to the immediate downstream tier $\\mathcal{T}_{k+1}$:\n",
    "     $$\n",
    "     \\mathcal{C}_{ij} = \\text{NaN} \\quad \\text{if } v_i \\in \\mathcal{T}_k, v_j \\in \\mathcal{T}_{k+1}\n",
    "     $$\n",
    "   - Block edges from $\\mathcal{T}_k$ to non-adjacent tiers:\n",
    "     $$\n",
    "     \\mathcal{C}_{ij} = 0.0 \\quad \\text{if } v_i \\in \\mathcal{T}_k, v_j \\in \\mathcal{T}_\\ell, \\ell \\neq k+1\n",
    "     $$\n",
    "\n",
    "5. **Application of Specific Constraints**:\n",
    "   - For $(v_i, v_j) \\in \\mathcal{C}_{\\text{forbid}}$, set $\\mathcal{C}_{ij} = 0.0$.\n",
    "   - For $(v_i, v_j) \\in \\mathcal{C}_{\\text{allow}}$, set $\\mathcal{C}_{ij} = \\text{NaN}$.\n",
    "\n",
    "### Theoretical Justification\n",
    "\n",
    "The constraint matrix $\\mathcal{C}$ guides causal discovery algorithms by eliminating implausible edges and encoding domain knowledge. This framework is especially effective for settings with known partial orderings (e.g., time-series or hierarchical data) or where certain causal mechanisms are theoretically or empirically refuted. By reducing the hypothesis space, it improves both accuracy and interpretability of the lea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "_j9kg9zBHHbg"
   },
   "outputs": [],
   "source": [
    "# Define a function to create a constraint matrix based on node names, tiers, and specific constraints.\n",
    "def create_constraint_matrix(\n",
    "    node_names,\n",
    "    tiers,\n",
    "    specific_constraints=None\n",
    "):\n",
    "\n",
    "    # Determine the number of nodes.\n",
    "    num_nodes = len(node_names)\n",
    "\n",
    "    # Map each node name to its index.\n",
    "    node_name_to_idx = {\n",
    "        name: i\n",
    "        for i, name in enumerate(node_names)\n",
    "    }\n",
    "\n",
    "    # Initialize the constraint matrix with NaN values.\n",
    "    constraint_matrix = np.full(\n",
    "        (num_nodes, num_nodes),\n",
    "        np.nan,\n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "    # Set Churn Label as sink node (no outgoing edges)\n",
    "    churn_idx = node_name_to_idx.get(\"Churn Label\")\n",
    "    # Check if Churn Label exists and set its row to forbidden edges.\n",
    "    if churn_idx is not None:\n",
    "        constraint_matrix[churn_idx, :] = 0.0\n",
    "\n",
    "    # Set demographic variables as root nodes (no incoming edges)\n",
    "    # Iterate over tier 0 features to block incoming edges.\n",
    "    for feature in tiers[0]:\n",
    "        if feature in node_name_to_idx:\n",
    "            feature_idx = node_name_to_idx[feature]\n",
    "            constraint_matrix[:, feature_idx] = 0.0\n",
    "\n",
    "    # Prevent edges within Tier 1\n",
    "    # Iterate over tier 0 pairs to block edges between distinct features.\n",
    "    for src in tiers[0]:\n",
    "        for dst in tiers[0]:\n",
    "            if (\n",
    "                src != dst and\n",
    "                src in node_name_to_idx and\n",
    "                dst in node_name_to_idx\n",
    "            ):\n",
    "                constraint_matrix[\n",
    "                    node_name_to_idx[src],\n",
    "                    node_name_to_idx[dst]\n",
    "                ] = 0.0\n",
    "\n",
    "    # Allow edges only from Tier N to Tier N+1\n",
    "    # Iterate over tier pairs to allow edges only to the immediate next tier.\n",
    "    for src_tier_idx, src_tier in enumerate(tiers[:-1]):\n",
    "        dst_tier = tiers[src_tier_idx + 1]\n",
    "\n",
    "        # Allow edges from current tier to next tier.\n",
    "        for src in src_tier:\n",
    "            for dst in dst_tier:\n",
    "                if (\n",
    "                    src in node_name_to_idx and\n",
    "                    dst in node_name_to_idx\n",
    "                ):\n",
    "                    constraint_matrix[\n",
    "                        node_name_to_idx[src],\n",
    "                        node_name_to_idx[dst]\n",
    "                    ] = np.nan\n",
    "\n",
    "        # Block edges to non-adjacent tiers.\n",
    "        for other_tier_idx, other_tier in enumerate(tiers):\n",
    "            if other_tier_idx != src_tier_idx + 1:\n",
    "                for src in src_tier:\n",
    "                    for dst in other_tier:\n",
    "                        if (\n",
    "                            src in node_name_to_idx and\n",
    "                            dst in node_name_to_idx\n",
    "                        ):\n",
    "                            constraint_matrix[\n",
    "                                node_name_to_idx[src],\n",
    "                                node_name_to_idx[dst]\n",
    "                            ] = 0.0\n",
    "\n",
    "    # Apply specific constraints\n",
    "    # Check for forbidden constraints and apply them.\n",
    "    if specific_constraints:\n",
    "        for src, dst in specific_constraints.get(\"forbidden\", []):\n",
    "            if (\n",
    "                src in node_name_to_idx and\n",
    "                dst in node_name_to_idx\n",
    "            ):\n",
    "                constraint_matrix[\n",
    "                    node_name_to_idx[src],\n",
    "                    node_name_to_idx[dst]\n",
    "                ] = 0.0\n",
    "\n",
    "        # Check for allowed constraints and apply them.\n",
    "        for src, dst in specific_constraints.get(\"allowed\", []):\n",
    "            if (\n",
    "                src in node_name_to_idx and\n",
    "                dst in node_name_to_idx\n",
    "            ):\n",
    "                constraint_matrix[\n",
    "                    node_name_to_idx[src],\n",
    "                    node_name_to_idx[dst]\n",
    "                ] = np.nan\n",
    "\n",
    "    # Log the shape of the created constraint matrix.\n",
    "    logger.info(\n",
    "        \"Constraint matrix created with shape: %s\",\n",
    "        constraint_matrix.shape\n",
    "    )\n",
    "\n",
    "    return constraint_matrix, node_name_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Define the Constraints Validator Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This diagnostic function evaluates a directed acyclic graph (DAG) or adjacency matrix against domain-imposed constraints. It detects violations such as improper directional edges from or into constrained variables, ensuring the integrity of the causal graph with respect to prior knowledge and structural assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "### Purpose and Context\n",
    "\n",
    "The constraint validation function is a post-processing utility that evaluates whether a directed acyclic graph (DAG) or adjacency matrix adheres to predefined structural constraints. It is primarily designed to assess the logical consistency of inferred causal graphs against prior knowledge, including tier-based acyclicity, variable roles (e.g., sinks or roots), and domain-specific prohibitions.\n",
    "\n",
    "### Inputs and Parameters\n",
    "\n",
    "Let $G = (V, E)$ denote a directed graph representing the learned causal structure. The inputs are:\n",
    "\n",
    "- $G$: Either a `networkx.DiGraph` or a real-valued adjacency matrix $A \\in [0,1]^{d \\times d}$.\n",
    "- $\\phi: \\mathcal{N} \\rightarrow \\{1, \\dots, d\\}$: Mapping from node names to indices.\n",
    "- $\\mathcal{T} = [\\mathcal{T}_1, \\dots, \\mathcal{T}_K]$: Tiered structure over $\\mathcal{N}$.\n",
    "- $\\tau$: Threshold for edge inclusion (default $\\tau = 0.5$).\n",
    "\n",
    "### Algorithmic Procedure\n",
    "\n",
    "1. **Graph Construction**:\n",
    "   - If $G$ is an adjacency matrix $A$, construct a directed graph:\n",
    "     $$\n",
    "     (i, j) \\in E \\iff A_{ij} > \\tau\n",
    "     $$\n",
    "\n",
    "2. **Sink Node Constraint (Terminal Label)**:\n",
    "   - Identify index $i^*$ of \"Churn Label\".\n",
    "   - If $\\exists j$ such that $(i^*, j) \\in E$, record violation:\n",
    "     $$\n",
    "     \\text{\"Churn Label has outgoing edges\"}\n",
    "     $$\n",
    "\n",
    "3. **Root Node Constraints (Tier 1)**:\n",
    "   - For each $v_i \\in \\mathcal{T}_1$, if $\\exists j$ such that $(j, i) \\in E$, record:\n",
    "     $$\n",
    "     \\text{\"$v_i$ has incoming edges\"}\n",
    "     $$\n",
    "\n",
    "4. **Intra-Tier Constraints**:\n",
    "   - For $v_i, v_j \\in \\mathcal{T}_1$, $i \\neq j$, if $(i, j) \\in E$, record:\n",
    "     $$\n",
    "     \\text{\"T1â†›T1 edge: $v_i \\to v_j$\"}\n",
    "     $$\n",
    "\n",
    "5. **Domain-Specific Forbidden Edges**:\n",
    "   - Identify the index of \"Gender\", denoted $g$.\n",
    "   - For all $v_k \\in \\mathcal{T}_3 \\cup \\mathcal{T}_4$ (e.g., service and billing), if $(g, k) \\in E$, record:\n",
    "     $$\n",
    "     \\text{\"Genderâ†’$v_k$ edge exists\"}\n",
    "     $$\n",
    "\n",
    "6. **Logging and Return**:\n",
    "   - If no violations are detected, log success.\n",
    "   - Otherwise, log and return the list of violations.\n",
    "\n",
    "### Theoretical Justification\n",
    "\n",
    "This validation method operationalizes domain constraints through syntactic graph queries, facilitating systematic assessment of learned graph structures. Tier-based constraints approximate temporal or functional hierarchies, while node-specific rules reflect expert knowledge or known exogenous/endogenous roles. By enforcing these checks post hoc, the method supports both hard and soft constraint-based learning paradigms.\n",
    "\n",
    "### Outcomes and Limitations\n",
    "\n",
    "The output is a list of textual descriptions of all constraint violations. This approach provides immediate diagnostic feedback but does not quantify the severity or probabilistic deviation of violations. It is most suitable for hard constraint evaluation and assumes accurate mapping and tier definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to validate constraints on a DAG or adjacency matrix.\n",
    "def validate_constraints(\n",
    "    dag,\n",
    "    node_name_to_idx,\n",
    "    tiers,\n",
    "    threshold=0.5\n",
    "):\n",
    "\n",
    "    violations = []\n",
    "    num_nodes = len(node_name_to_idx)\n",
    "    churn_idx = node_name_to_idx.get(\"Churn Label\")\n",
    "\n",
    "    # Convert adjacency matrix to DAG if needed.\n",
    "    if isinstance(dag, np.ndarray):\n",
    "        G = nx.DiGraph()\n",
    "        G.add_nodes_from(range(num_nodes))\n",
    "\n",
    "        # Add edges to the graph based on threshold.\n",
    "        for i in range(num_nodes):\n",
    "            for j in range(num_nodes):\n",
    "                if dag[i, j] > threshold:\n",
    "                    G.add_edge(i, j)\n",
    "    else:\n",
    "        G = dag\n",
    "\n",
    "    # Check if Churn Label has outgoing edges.\n",
    "    if (\n",
    "        churn_idx is not None and\n",
    "        any(G.has_edge(churn_idx, j) for j in range(num_nodes))\n",
    "    ):\n",
    "        violations.append(\"Churn Label has outgoing edges\")\n",
    "\n",
    "    # Check if Tier 1 variables have incoming edges.\n",
    "    for var in tiers[0]:\n",
    "        var_idx = node_name_to_idx.get(var)\n",
    "\n",
    "        if (\n",
    "            var_idx is not None and\n",
    "            any(G.has_edge(j, var_idx) for j in range(num_nodes))\n",
    "        ):\n",
    "            violations.append(f\"{var} has incoming edges\")\n",
    "\n",
    "    # Check for edges within Tier 1.\n",
    "    for src in tiers[0]:\n",
    "        src_idx = node_name_to_idx.get(src)\n",
    "\n",
    "        for dst in tiers[0]:\n",
    "            dst_idx = node_name_to_idx.get(dst)\n",
    "\n",
    "            if (\n",
    "                src != dst and\n",
    "                src_idx is not None and\n",
    "                dst_idx is not None and\n",
    "                G.has_edge(src_idx, dst_idx)\n",
    "            ):\n",
    "                violations.append(f\"T1â†›T1 edge: {src}â†’{dst}\")\n",
    "\n",
    "    # Check for forbidden Gender â†’ Service/Billing edges.\n",
    "    gender_idx = node_name_to_idx.get(\"Gender\")\n",
    "\n",
    "    if gender_idx is not None:\n",
    "        for dst in tiers[2] + tiers[3]:\n",
    "            dst_idx = node_name_to_idx.get(dst)\n",
    "\n",
    "            if (\n",
    "                dst_idx is not None and\n",
    "                G.has_edge(gender_idx, dst_idx)\n",
    "            ):\n",
    "                violations.append(f\"Genderâ†’{dst} edge exists\")\n",
    "\n",
    "    # Log the validation result.\n",
    "    if not violations:\n",
    "        logger.info(\"âœ… All constraints validated successfully\")\n",
    "    else:\n",
    "        logger.warning(\n",
    "            \"âŒ Constraint violations detected: %s\",\n",
    "            violations\n",
    "        )\n",
    "\n",
    "    return violations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Define the Relation Saver Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function exports identified causal relationshipsâ€”either from probabilistic matrices or NetworkX graphsâ€”to a text file. It facilitates result documentation and interpretability by formatting edge weights and directions, ensuring results are both human-readable and persistently stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "p_Ht7zb3HHbg"
   },
   "outputs": [],
   "source": [
    "# Define a function to save causal relationships to a text file.\n",
    "def save_relations_to_text(\n",
    "    dag,\n",
    "    node_names,\n",
    "    filename,\n",
    "    threshold=0.2\n",
    "):\n",
    "    \"\"\"\n",
    "    Save causal relationships to a text file and print to console with improved formatting.\n",
    "\n",
    "    Args:\n",
    "        dag: NetworkX DiGraph or np.ndarray (adjacency/probability matrix).\n",
    "        node_names (list): List of node names.\n",
    "        filename (str): Output text file name.\n",
    "        threshold (float): Probability/weight threshold for matrix-based DAGs.\n",
    "    \"\"\"\n",
    "    # Handle potential errors.\n",
    "    try:\n",
    "        # Initialize list to hold relationship data.\n",
    "        relations = []\n",
    "\n",
    "        print(\"\\n=== Causal Relationships ===\")\n",
    "\n",
    "        # Check if the DAG is a NumPy array.\n",
    "        if isinstance(dag, np.ndarray):\n",
    "            # Iterate over matrix rows.\n",
    "            for i in range(dag.shape[0]):\n",
    "                # Iterate over matrix columns.\n",
    "                for j in range(dag.shape[1]):\n",
    "                    # Add relationship if weight exceeds threshold.\n",
    "                    if dag[i, j] > threshold:\n",
    "                        relations.append({\n",
    "                            \"source\": node_names[i],\n",
    "                            \"destination\": node_names[j],\n",
    "                            \"weight\": float(dag[i, j])\n",
    "                        })\n",
    "\n",
    "                        print(\n",
    "                            f\"{node_names[i]} -> {node_names[j]} (weight: {dag[i, j]:.3f})\"\n",
    "                        )\n",
    "                        print(\"---\")\n",
    "\n",
    "        # Handle case where DAG is a graph object.\n",
    "        else:\n",
    "            # Iterate over edges in the graph.\n",
    "            for src, dst in dag.edges():\n",
    "                weight = dag[src][dst].get(\"weight\", 1.0)\n",
    "                relations.append({\n",
    "                    \"source\": node_names[src],\n",
    "                    \"destination\": node_names[dst],\n",
    "                    \"weight\": float(weight)\n",
    "                })\n",
    "\n",
    "                print(\n",
    "                    f\"{node_names[src]} -> {node_names[dst]} (weight: {weight:.3f})\"\n",
    "                )\n",
    "                print(\"---\")\n",
    "\n",
    "        # Convert relationships to DataFrame.\n",
    "        relations_df = pd.DataFrame(relations)\n",
    "\n",
    "        # Write relationships to file if any exist.\n",
    "        if not relations_df.empty:\n",
    "            # Open file context.\n",
    "            with open(filename, \"w\") as f:\n",
    "                f.write(\"Source -> Destination (Weight)\\n\")\n",
    "                f.write(\"=\" * 30 + \"\\n\")\n",
    "\n",
    "                # Iterate over DataFrame rows.\n",
    "                for _, row in relations_df.iterrows():\n",
    "                    f.write(\n",
    "                        f\"{row['source']:<25} -> {row['destination']:<25} ({row['weight']:.3f})\\n\"\n",
    "                    )\n",
    "                    f.write(\"-\" * 30 + \"\\n\")\n",
    "\n",
    "            logger.info(\n",
    "                \"Causal relationships saved to %s (%d relations)\",\n",
    "                filename,\n",
    "                len(relations_df)\n",
    "            )\n",
    "\n",
    "        # Handle case where no relationships are found.\n",
    "        else:\n",
    "            # Open file context.\n",
    "            with open(filename, \"w\") as f:\n",
    "                f.write(\"No causal relationships found.\\n\")\n",
    "\n",
    "            logger.warning(\n",
    "                \"No causal relationships to save for %s\",\n",
    "                filename\n",
    "            )\n",
    "\n",
    "    # Handle any exception that occurs during execution.\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            \"Failed to save relations to %s: %s\",\n",
    "            filename,\n",
    "            str(e)\n",
    "        )\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. Define the Causal Graph Visualizer Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A visualization utility that renders causal DAGs using uniform spatial distribution (circular layout) and annotated nodes. This function provides graphical representations of causal structures to assist in exploratory analysis and model verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "H49mDITAHHbg"
   },
   "outputs": [],
   "source": [
    "# Define function to visualize a causal graph.\n",
    "def visualize_causal_graph(\n",
    "    dag,\n",
    "    node_names,\n",
    "    filename=\"causal_graph.png\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize a causal graph with uniform edge lengths, focusing on nodes and relationships.\n",
    "    \n",
    "    Args:\n",
    "        dag: NetworkX DiGraph or np.ndarray (adjacency matrix).\n",
    "        node_names (list): List of node names.\n",
    "        filename (str): Output file name.\n",
    "    \n",
    "    Returns:\n",
    "        nx.DiGraph: Visualized graph.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if input is a NumPy array and convert to DiGraph if needed.\n",
    "    if isinstance(dag, np.ndarray):\n",
    "        G = nx.DiGraph(dag)\n",
    "    else:\n",
    "        G = dag\n",
    "\n",
    "    # Relabel nodes with names.\n",
    "    G = nx.relabel_nodes(\n",
    "        G,\n",
    "        dict(enumerate(node_names))\n",
    "    )\n",
    "\n",
    "    # Create a matplotlib figure with specified size and resolution.\n",
    "    plt.figure(\n",
    "        figsize=(12, 10),\n",
    "        dpi=300\n",
    "    )\n",
    "    # Use circular layout for uniform edge lengths and clear node placement.\n",
    "    pos = nx.circular_layout(G)\n",
    "\n",
    "    # Draw graph nodes.\n",
    "    nx.draw_networkx_nodes(\n",
    "        G,\n",
    "        pos,\n",
    "        node_size=1000,\n",
    "        node_color=\"lightblue\",\n",
    "        alpha=0.8\n",
    "    )\n",
    "\n",
    "    # Draw graph edges.\n",
    "    nx.draw_networkx_edges(\n",
    "        G,\n",
    "        pos,\n",
    "        width=1,\n",
    "        alpha=0.6,\n",
    "        arrowsize=15\n",
    "    )\n",
    "\n",
    "    # Draw graph labels.\n",
    "    nx.draw_networkx_labels(\n",
    "        G,\n",
    "        pos,\n",
    "        font_size=8,\n",
    "        font_weight=\"bold\"\n",
    "    )\n",
    "\n",
    "    # Set the plot title and turn off axes.\n",
    "    plt.title(f\"Causal Graph ({filename.split('.')[0]})\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Save the figure to file and close the plot.\n",
    "    plt.savefig(\n",
    "        filename,\n",
    "        format=\"png\",\n",
    "        bbox_inches=\"tight\"\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "    # Log the successful save operation.\n",
    "    logger.info(\"Causal graph saved as %s\", filename)\n",
    "\n",
    "    # Return the graph.\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6. Define Structure Learning Analyzer Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function extracts descriptive metrics from a learned DAG or adjacency matrix, including graph density, in/out-degree distributions, and most influential/affected nodes. These metrics enable a deeper understanding of the structural characteristics of the causal model and facilitate comparative evaluations across algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "9ynmA2TPHHbh"
   },
   "outputs": [],
   "source": [
    "# Define function to analyze the structure of a learned DAG.\n",
    "def analyze_structure_learning(\n",
    "    dag,\n",
    "    node_names,\n",
    "    threshold=0.5, # This threshold is primarily for when dag is a numpy array\n",
    "    output_dir=\"causal_discovery_output\", # Added for future use (e.g., plots)\n",
    "    algo_prefix=\"\" # Added for future use (e.g., unique plot filenames)\n",
    "):\n",
    "    \"\"\"\n",
    "    Analyze the structure of a learned DAG.\n",
    "\n",
    "    Args:\n",
    "        dag: NetworkX DiGraph or np.ndarray (adjacency/probability matrix).\n",
    "        node_names (list): List of node names.\n",
    "        threshold (float): Probability threshold for matrix-based DAGs.\n",
    "        output_dir (str): Directory to save potential output files.\n",
    "        algo_prefix (str): Prefix for algorithm specific outputs.\n",
    "\n",
    "    Returns:\n",
    "        dict: Structure learning metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    G_for_analysis = None\n",
    "    adj_matrix_for_analysis = None\n",
    "    num_nodes = len(node_names)\n",
    "\n",
    "    if isinstance(dag, np.ndarray):\n",
    "        # If input is a matrix, threshold it\n",
    "        adj_matrix_for_analysis = (dag > threshold).astype(int)\n",
    "        # Create a named graph from this matrix for consistent metric calculation\n",
    "        G_for_analysis = nx.DiGraph()\n",
    "        G_for_analysis.add_nodes_from(node_names) # Add nodes with their actual names\n",
    "        for r_idx in range(num_nodes):\n",
    "            for c_idx in range(num_nodes):\n",
    "                if adj_matrix_for_analysis[r_idx, c_idx] != 0:\n",
    "                    G_for_analysis.add_edge(node_names[r_idx], node_names[c_idx])\n",
    "    elif isinstance(dag, nx.DiGraph):\n",
    "        G_for_analysis = dag # Assume dag is the graph to be analyzed (e.g., G_viz_primary_named)\n",
    "        # Ensure the graph has string node names as expected if it came from visualize_causal_graph\n",
    "        # Convert G_for_analysis to adj_matrix using node_names as the nodelist\n",
    "        adj_matrix_for_analysis = nx.to_numpy_array(\n",
    "            G_for_analysis,\n",
    "            nodelist=node_names # Use the actual string node names for the nodelist\n",
    "        )\n",
    "    else:\n",
    "        logger.error(f\"analyze_structure_learning: Unsupported dag type: {type(dag)}\")\n",
    "        return {} # Return empty metrics on error\n",
    "\n",
    "    num_edges = G_for_analysis.number_of_edges()\n",
    "\n",
    "    graph_density = (\n",
    "        num_edges / (num_nodes * (num_nodes - 1))\n",
    "        if num_nodes > 1 else 0.0\n",
    "    )\n",
    "\n",
    "    # Calculate in-degree and out-degree directly from the named graph\n",
    "    in_degrees_dict = dict(G_for_analysis.in_degree())\n",
    "    out_degrees_dict = dict(G_for_analysis.out_degree())\n",
    "\n",
    "    avg_in_degree = sum(in_degrees_dict.values()) / num_nodes if num_nodes > 0 else 0.0\n",
    "    avg_out_degree = sum(out_degrees_dict.values()) / num_nodes if num_nodes > 0 else 0.0\n",
    "\n",
    "    most_influential = sorted(\n",
    "        out_degrees_dict.items(), # Already (node_name, degree)\n",
    "        key=lambda item: item[1],\n",
    "        reverse=True\n",
    "    )[:5]\n",
    "\n",
    "    most_affected = sorted(\n",
    "        in_degrees_dict.items(), # Already (node_name, degree)\n",
    "        key=lambda item: item[1],\n",
    "        reverse=True\n",
    "    )[:5]\n",
    "\n",
    "    metrics = {\n",
    "        \"num_edges\": int(num_edges),\n",
    "        \"graph_density\": graph_density,\n",
    "        \"avg_in_degree\": avg_in_degree,\n",
    "        \"avg_out_degree\": avg_out_degree,\n",
    "        \"most_influential\": most_influential,\n",
    "        \"most_affected\": most_affected,\n",
    "    }\n",
    "\n",
    "    # (Optional: Future addition for centrality plots as discussed before)\n",
    "    # if num_nodes > 0 and algo_prefix: # Check if algo_prefix is provided\n",
    "    #     try:\n",
    "    #         # ... (centrality calculation and plotting code would go here) ...\n",
    "    #         # Example: metrics['degree_centrality'] = nx.degree_centrality(G_for_analysis)\n",
    "    #         # ... (code to plot and save centrality bar charts using output_dir and algo_prefix) ...\n",
    "    #         pass \n",
    "    #     except Exception as e_metrics_plot:\n",
    "    #         logger.error(f\"Could not compute/plot centrality metrics for {algo_prefix}: {e_metrics_plot}\")\n",
    "\n",
    "    logger.info(\n",
    "        \"Structure Learning Metrics for %s: %s\",\n",
    "        algo_prefix.upper() if algo_prefix else \"Algorithm\",\n",
    "        {k: (f\"{v:.3f}\" if isinstance(v, float) else v) for k, v in metrics.items() if not isinstance(v, list)}\n",
    "    )\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7. Define the Causal Evaluator Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This evaluator quantifies the validity of discovered causal models by measuring constraint violations on test data. It handles data cleaning (e.g., missing values, constant columns) and supports tailored validation schemes (e.g., for LiNGAM's continuous-only structure), returning detailed diagnostic summaries per algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "57F6to0tHHbh"
   },
   "outputs": [],
   "source": [
    "# Define function to evaluate causal discovery algorithms on test data.\n",
    "def evaluate_causal_discovery(\n",
    "    results,\n",
    "    test_data,\n",
    "    tiers,\n",
    "    node_name_to_idx,\n",
    "    threshold=0.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate causal discovery algorithms on test data, focusing on constraint violations.\n",
    "\n",
    "    Args:\n",
    "        results (dict): Results from run_causal_discovery_pipeline.\n",
    "        test_data (pd.DataFrame): Test data.\n",
    "        tiers (list): List of tier lists.\n",
    "        node_name_to_idx (dict): Mapping of node names to indices.\n",
    "        threshold (float): Probability threshold for matrix-based DAGs.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Evaluation metrics for each algorithm.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Log the start of evaluation.\n",
    "    logger.info(\"Evaluating constraint violations on test data...\")\n",
    "    evaluation_metrics = {}\n",
    "\n",
    "    # Identify and remove constant columns in test data.\n",
    "    constant_cols = [\n",
    "        col for col in test_data.columns\n",
    "        if test_data[col].std() == 0\n",
    "    ]\n",
    "\n",
    "    # Handle constant columns if found.\n",
    "    if constant_cols:\n",
    "        logger.warning(\"Constant columns in test data: %s\", constant_cols)\n",
    "        test_data = test_data.drop(columns=constant_cols)\n",
    "\n",
    "    # Handle missing values in test data.\n",
    "    if test_data.isna().any().any():\n",
    "        logger.warning(\"NaNs in test data, filling with mean\")\n",
    "        test_data = test_data.fillna(test_data.mean())\n",
    "\n",
    "    # Iterate over results from each algorithm.\n",
    "    for algo_name, result in results.items():\n",
    "        # Skip evaluation if there was an error.\n",
    "        if \"error\" in result:\n",
    "            evaluation_metrics[algo_name] = {\n",
    "                \"constraint_violations\": \"N/A\",\n",
    "                \"violation_details\": \"N/A\"\n",
    "            }\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Extract DAG and adjacency matrix.\n",
    "            dag = result[\"dag\"]\n",
    "            adj_matrix = result[\"adj_matrix\"]\n",
    "\n",
    "            # Set up node index mapping for LiNGAM if applicable.\n",
    "            algo_node_name_to_idx = node_name_to_idx\n",
    "\n",
    "            # Filter for continuous features if algorithm is LiNGAM.\n",
    "            if algo_name == \"LiNGAM\":\n",
    "                continuous_features = [\n",
    "                    f for f in test_data.columns\n",
    "                    if f in [\n",
    "                        \"Age\",\n",
    "                        \"Number of Dependents\",\n",
    "                        \"Number of Referrals\",\n",
    "                        \"Tenure in Months\",\n",
    "                        \"Total Revenue\"\n",
    "                    ]\n",
    "                ]\n",
    "\n",
    "                algo_node_name_to_idx = {\n",
    "                    name: i for i, name in enumerate(continuous_features)\n",
    "                }\n",
    "\n",
    "                # Define tier structure for LiNGAM.\n",
    "                lingam_tiers = [\n",
    "                    [f for f in tiers[0] if f in continuous_features],\n",
    "                    [f for f in tiers[1] if f in continuous_features],\n",
    "                    [f for f in tiers[3] if f in continuous_features]\n",
    "                ]\n",
    "            else:\n",
    "                lingam_tiers = tiers\n",
    "\n",
    "            # Validate causal constraints.\n",
    "            violations = validate_constraints(\n",
    "                dag if algo_name != \"LiNGAM\" else adj_matrix,\n",
    "                algo_node_name_to_idx,\n",
    "                lingam_tiers,\n",
    "                threshold\n",
    "            )\n",
    "\n",
    "            # Record evaluation metrics.\n",
    "            evaluation_metrics[algo_name] = {\n",
    "                \"constraint_violations\": len(violations),\n",
    "                \"violation_details\": violations if violations else \"None\"\n",
    "            }\n",
    "\n",
    "            # Log the number of constraint violations.\n",
    "            logger.info(\n",
    "                \"%s: %d constraint violations on test data: %s\",\n",
    "                algo_name,\n",
    "                len(violations),\n",
    "                violations if violations else \"None\"\n",
    "            )\n",
    "\n",
    "        # Handle potential errors during evaluation.\n",
    "        except Exception as e:\n",
    "            logger.error(\"Evaluation failed for %s: %s\", algo_name, str(e))\n",
    "            evaluation_metrics[algo_name] = {\n",
    "                \"constraint_violations\": \"N/A\",\n",
    "                \"violation_details\": \"N/A\"\n",
    "            }\n",
    "\n",
    "    # Create summary DataFrame of evaluation metrics.\n",
    "    summary = pd.DataFrame(evaluation_metrics).T\n",
    "\n",
    "    # Log the final summary.\n",
    "    logger.info(\"Constraint violation summary:\\n%s\", summary)\n",
    "\n",
    "    # Return the summary DataFrame.\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7v9Z5zxHHbh"
   },
   "source": [
    "# 5. Define the Causal Discovery and Inference Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section implements a class-based framework for encapsulating causal discovery algorithms. Each subclass of the CausalDiscoveryAlgorithm base class adheres to a unified fit() interface, allowing for standardized training, evaluation, and output handling across multiple algorithmic strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Define the Causal Discovery Base Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An abstract base class defining the required interface (fit() method) for all causal discovery algorithm implementations. This abstraction ensures structural uniformity and enforces contract adherence for algorithm-specific subclasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an abstract base class for causal discovery algorithms.\n",
    "class CausalDiscoveryAlgorithm(ABC):\n",
    "    \"\"\"Base class for causal discovery algorithms.\"\"\"\n",
    "\n",
    "    # Define an abstract method to fit the causal discovery algorithm.\n",
    "    @abstractmethod\n",
    "    def fit(\n",
    "        self,\n",
    "        data,\n",
    "        constraint_matrix,\n",
    "        node_names,\n",
    "        node_name_to_idx,\n",
    "        tiers,\n",
    "        output_dir\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Fit the causal discovery algorithm.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): Input data.\n",
    "            constraint_matrix (np.ndarray): Constraint matrix.\n",
    "            node_names (list): List of node names.\n",
    "            node_name_to_idx (dict): Mapping of node names to indices.\n",
    "            tiers (list): List of tier lists.\n",
    "\n",
    "        Returns:\n",
    "            dict: Results including DAG, adjacency matrix, metrics, and violations.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0Ghg6xRHHbh"
   },
   "source": [
    "# 5.2. Define the DECI Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implements the DECI (Differentiable Equilibrium-based Causal Inference) method. This complex probabilistic model fits a DAG using deep learning-based techniques and acyclicity constraints. The implementation includes data preparation, constraint integration, model training, graph extraction, evaluation, and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "### Purpose and Context\n",
    "\n",
    "The DECI (Differentiable Equivariant Causal Inference) algorithm is designed to infer the causal structure of a set of observed variables from data by learning the probabilistic adjacency relations that best explain the observed joint distribution. It utilizes a variational inference framework to approximate the posterior distribution over directed acyclic graphs (DAGs), allowing the recovery of causal relationships under both observational and interventional settings. DECI operates under the structural causal model (SCM) formalism, representing causal mechanisms as functions with additive noise.\n",
    "\n",
    "### Inputs and Parameters\n",
    "\n",
    "Let $X \\in \\mathbb{R}^{n \\times d}$ denote the observational dataset with $n$ samples and $d$ variables. The following inputs and parameters define the algorithm:\n",
    "\n",
    "- $X$: Input dataset, where each column corresponds to a variable.\n",
    "- $\\mathcal{C} \\in \\{0,1\\}^{d \\times d}$: Binary constraint matrix indicating known causal restrictions.\n",
    "- $\\mathcal{N}$: A list of variable names.\n",
    "- $\\phi: \\mathcal{N} \\rightarrow \\{1, \\dots, d\\}$: Mapping from variable names to indices.\n",
    "- $\\mathcal{T}$: Tiered structure of variables specifying acyclicity constraints.\n",
    "- $\\lambda$: Prior sparsity coefficient, penalizing edge density in the DAG.\n",
    "- $\\rho_0$: Initial value for augmented Lagrangian penalty.\n",
    "- $\\alpha_0$: Initial Lagrangian multiplier.\n",
    "- $\\text{AugLagLRConfig}$: Hyperparameters for the augmented Lagrangian optimization.\n",
    "\n",
    "### Algorithmic Procedure\n",
    "\n",
    "1. **Variable and Data Initialization**:\n",
    "   - Parse variable metadata to extract expected variables $\\mathcal{V} = \\{v_1, \\dots, v_d\\}$.\n",
    "   - Validate that $\\text{columns}(X) = \\mathcal{V}$.\n",
    "   - Normalize $X$ and construct a data module for mini-batch training.\n",
    "\n",
    "2. **Model Setup**:\n",
    "   - Define a DECI module consisting of:\n",
    "     - A generative model with Gaussian noise: $X = f(PA(X)) + \\varepsilon$, $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$.\n",
    "     - An adjacency distribution with parameters $\\theta_{\\text{exist}}$ and $\\theta_{\\text{orient}}$.\n",
    "   - Set prior sparsity with $\\lambda$ and initialize augmented Lagrangian parameters $(\\rho_0, \\alpha_0)$.\n",
    "\n",
    "3. **Variational Inference and Optimization**:\n",
    "   - Optimize the evidence lower bound (ELBO) using a doubly-stochastic gradient descent scheme.\n",
    "   - Update parameters to maximize:\n",
    "     $$\n",
    "     \\mathbb{E}_{q(A)}[\\log p(X \\mid A, \\theta)] - \\text{KL}(q(A) \\| p(A))\n",
    "     $$\n",
    "     subject to tiered acyclicity constraints and sparsity prior.\n",
    "\n",
    "4. **Adjacency Probability Estimation**:\n",
    "   - Let $\\text{logits}_{\\text{exist}}, \\text{logits}_{\\text{orient}}$ denote logits for edge existence and orientation.\n",
    "   - Construct a skew-symmetric matrix:\n",
    "     $$\n",
    "     \\Theta = \\text{fill}_{\\text{upper}}(\\text{logits}_{\\text{orient}}) - \\text{fill}_{\\text{lower}}(\\text{logits}_{\\text{orient}})\n",
    "     $$\n",
    "   - Compute logit-based score matrix:\n",
    "     $$\n",
    "     S_{ij} = -\\log\\left(\\exp(-\\text{logits}_{\\text{exist}}) + \\exp(\\Theta_{ij}) + \\exp(\\Theta_{ij} - \\text{logits}_{\\text{exist}})\\right)\n",
    "     $$\n",
    "   - Apply sigmoid transformation:\n",
    "     $$\n",
    "     P_{ij} = \\sigma(S_{ij}) = \\frac{1}{1 + \\exp(-S_{ij})}\n",
    "     $$\n",
    "\n",
    "5. **Constraint Enforcement and Graph Extraction**:\n",
    "   - Enforce structural constraints: $P = P \\odot \\text{mask}(\\mathcal{C})$.\n",
    "   - Identify constraint violations using $\\mathcal{T}$ and $\\phi$.\n",
    "   - Extract DAG $G = (V, E)$ such that $(i, j) \\in E$ iff $P_{ij} > 0.5$.\n",
    "\n",
    "6. **Visualization and Output**:\n",
    "   - Generate a graph visualization of the inferred DAG.\n",
    "   - Export adjacency probabilities and causal metrics to the output directory.\n",
    "\n",
    "### Theoretical Justification\n",
    "\n",
    "DECI leverages the principles of variational inference, specifically amortized inference over a DAG space, using continuous relaxation and equivariant parametrization of the adjacency distribution. The optimization employs augmented Lagrangian methods to handle acyclicity constraints, ensuring that the inferred graph adheres to a DAG structure while respecting prior structural knowledge.\n",
    "\n",
    "### Outcomes and Limitations\n",
    "\n",
    "The output includes the learned DAG $G$, adjacency matrix $P \\in [0,1]^{d \\times d}$ representing edge probabilities, a set of structural violations, and evaluation metrics such as precision, recall, and SHD (Structural Hamming Distance). The algorithm scales with $O(d^2)$ in terms of parameter estimation, with performance sensitive to initialization and tier definitions. The continuous relaxation may introduce approximation errors in the DAG structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "TCXr3cjeHHbh"
   },
   "outputs": [],
   "source": [
    "class DECIAlgorithm(CausalDiscoveryAlgorithm):\n",
    "    def fit(\n",
    "        self,\n",
    "        data,\n",
    "        constraint_matrix,\n",
    "        node_names,\n",
    "        node_name_to_idx,\n",
    "        tiers,\n",
    "        output_dir\n",
    "    ):\n",
    "        logger.info(\"Running DECI algorithm...\")\n",
    "        try:\n",
    "            # Load variable type specifications for DECI\n",
    "            # Ensure VARIABLES_PATH is correctly defined (e.g., \"data/variables.json\")\n",
    "            with fsspec.open(VARIABLES_PATH, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "                variables_json = json.load(f)\n",
    "                if \"variables\" not in variables_json:\n",
    "                    raise KeyError(\"Key 'variables' not found in variables.json.\")\n",
    "                variables = variables_json[\"variables\"]\n",
    "\n",
    "            # Validate data columns against variable specifications\n",
    "            expected_columns = [var[\"name\"] for var in variables]\n",
    "            if set(expected_columns) != set(data.columns):\n",
    "                missing = set(expected_columns) - set(data.columns)\n",
    "                extra = set(data.columns) - set(expected_columns)\n",
    "                logger.error(f\"DECI data columns mismatch. Missing: {missing}, Extra: {extra}\")\n",
    "                raise ValueError(\"Data columns do not match variables.json.\")\n",
    "\n",
    "            # Prepare PyTorch Lightning DataModule for DECI\n",
    "            data_module = BasicDECIDataModule(\n",
    "                data,\n",
    "                variables=[Variable.from_dict(d) for d in variables],\n",
    "                batch_size=128,\n",
    "                normalize=True\n",
    "            )\n",
    "\n",
    "            # Initialize DECI Lightning Module with specified hyperparameters\n",
    "            lightning_module = DECIModule(\n",
    "                noise_dist=ContinuousNoiseDist.GAUSSIAN,\n",
    "                prior_sparsity_lambda=100.0,\n",
    "                init_rho=30.0,\n",
    "                init_alpha=0.20,\n",
    "                auglag_config=AugLagLRConfig(\n",
    "                    max_inner_steps=1500,\n",
    "                    max_outer_steps=8,\n",
    "                    lr_init_dict={\n",
    "                        \"icgnn\": 0.00076,\n",
    "                        \"vardist\": 0.0098,\n",
    "                        \"functional_relationships\": 3e-4,\n",
    "                        \"noise_dist\": 0.0070\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "            # Apply domain knowledge constraints to the DECI module\n",
    "            lightning_module.constraint_matrix = torch.tensor(constraint_matrix, dtype=torch.float32)\n",
    "\n",
    "            # Configure and run PyTorch Lightning Trainer\n",
    "            trainer = pl.Trainer(\n",
    "                accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "                devices=1,\n",
    "                max_epochs=10,\n",
    "                callbacks=[],  # Empty callbacks as progress bar is disabled\n",
    "                enable_checkpointing=False,\n",
    "                enable_progress_bar=False, # Disables training progress bar for cleaner logs\n",
    "                logger=False # Disables default Pytorch Lightning logger\n",
    "            )\n",
    "            trainer.fit(lightning_module, datamodule=data_module)\n",
    "\n",
    "            # Save the trained DECI model\n",
    "            model_save_path = os.path.join(output_dir, \"deci_model.pt\")\n",
    "            torch.save(lightning_module.sem_module, model_save_path)\n",
    "            logger.info(f\"DECI model saved to {model_save_path}\")\n",
    "\n",
    "            # Extract logits for edge existence and orientation\n",
    "            logits_exist = lightning_module.sem_module.adjacency_module.adjacency_distribution.logits_exist\n",
    "            logits_orient = lightning_module.sem_module.adjacency_module.adjacency_distribution.logits_orient\n",
    "\n",
    "            # Helper to reconstruct matrix from vector of triangular elements\n",
    "            def fill_triangular(vec, upper=False):\n",
    "                d = int((-1 + np.sqrt(1 + 8 * len(vec))) / 2) + 1 # Number of nodes\n",
    "                mat = vec.new_zeros(d, d)\n",
    "                if upper:\n",
    "                    mat[np.triu_indices(d, k=1)] = vec\n",
    "                else:\n",
    "                    mat[np.tril_indices(d, k=-1)] = vec\n",
    "                return mat\n",
    "\n",
    "            # Compute probability matrix from learned logits\n",
    "            neg_theta = fill_triangular(logits_orient, upper=True) - fill_triangular(logits_orient, upper=False)\n",
    "            logits_matrix = -torch.logsumexp(\n",
    "                torch.stack([-logits_exist, neg_theta, neg_theta - logits_exist], dim=-1),\n",
    "                dim=-1\n",
    "            )\n",
    "            prob_matrix = 1 / (1 + np.exp(-logits_matrix.cpu().detach().numpy()))\n",
    "            prob_matrix = prob_matrix * np.isnan(constraint_matrix) # Enforce hard constraints\n",
    "\n",
    "            # --- 1. Heatmap of the DECI Probability Matrix ---\n",
    "            plt.figure(figsize=(14, 12), dpi=300)\n",
    "            sns.heatmap(prob_matrix, xticklabels=node_names, yticklabels=node_names,\n",
    "                        cmap=\"viridis\", annot=True, fmt=\".2f\", annot_kws={\"size\": 6})\n",
    "            plt.title(\"DECI - Estimated Probability Matrix\")\n",
    "            heatmap_filename = os.path.join(output_dir, \"deci_prob_matrix_heatmap.png\")\n",
    "            plt.savefig(heatmap_filename, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            logger.info(f\"DECI probability matrix heatmap saved as {heatmap_filename}\")\n",
    "\n",
    "            # --- 2. Generate Causal Graphs at Different Additional Thresholds ---\n",
    "            thresholds_to_explore_deci = [0.3, 0.6, 0.7, 0.8]\n",
    "            logger.info(f\"DECI: Generating graphs for varying thresholds: {thresholds_to_explore_deci}\")\n",
    "\n",
    "            for thresh_val in thresholds_to_explore_deci:\n",
    "                adj_matrix_loop = (prob_matrix > thresh_val).astype(int) # Binarize\n",
    "                G_loop_for_viz = nx.DiGraph(adj_matrix_loop)\n",
    "                \n",
    "                loop_graph_filename = os.path.join(output_dir, f\"deci_graph_thresh_{thresh_val:.2f}.png\")\n",
    "                visualize_causal_graph(G_loop_for_viz, node_names, loop_graph_filename)\n",
    "                logger.info(f\"DECI graph for threshold {thresh_val:.2f} saved as {loop_graph_filename}\")\n",
    "\n",
    "                loop_relations_filename = os.path.join(output_dir, f\"deci_relations_thresh_{thresh_val:.2f}.txt\")\n",
    "                save_relations_to_text(adj_matrix_loop, node_names, loop_relations_filename, threshold=0.01)\n",
    "\n",
    "            # --- Primary graph generation and analysis (using a standard 0.5 threshold) ---\n",
    "            primary_deci_threshold = 0.5\n",
    "            adj_matrix_primary = (prob_matrix > primary_deci_threshold).astype(int)\n",
    "            \n",
    "            violations = validate_constraints(\n",
    "                adj_matrix_primary, \n",
    "                node_name_to_idx,\n",
    "                tiers,\n",
    "                threshold=0.0 # adj_matrix_primary is already 0/1\n",
    "            )\n",
    "\n",
    "            G_primary_int_indexed = nx.DiGraph(adj_matrix_primary)\n",
    "            G_viz_primary_named = visualize_causal_graph(\n",
    "                G_primary_int_indexed, \n",
    "                node_names,\n",
    "                os.path.join(output_dir, \"deci_graph_primary.png\")\n",
    "            )\n",
    "\n",
    "            save_relations_to_text(\n",
    "                prob_matrix, # Use original probability matrix for relation text (uses its own threshold)\n",
    "                node_names,\n",
    "                os.path.join(output_dir, \"deci_relations_primary.txt\"),\n",
    "                threshold=0.2 # Default threshold in save_relations_to_text\n",
    "            )\n",
    "\n",
    "            metrics = analyze_structure_learning(\n",
    "                G_viz_primary_named, \n",
    "                node_names,\n",
    "                threshold=0.0, # Graph is already binarized\n",
    "                output_dir=output_dir,\n",
    "                algo_prefix=\"deci\"\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"dag\": G_viz_primary_named,\n",
    "                \"adj_matrix\": prob_matrix, # Full probability matrix from DECI\n",
    "                \"metrics\": metrics,\n",
    "                \"violations\": violations\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"DECI failed: %s\", str(e))\n",
    "            import traceback\n",
    "            logger.error(traceback.format_exc()) # Provides detailed traceback for debugging\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bX5Ddfg2HHbh"
   },
   "source": [
    "# 5.3. Define the LiNGAM Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implements the DirectLiNGAM (Linear Non-Gaussian Acyclic Model) algorithm tailored for continuous variables. It constructs a tier-aware constraint matrix and identifies linear causal relationships, offering precise control over permissible edge directions and robust validation for structural assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "### Purpose and Context\n",
    "\n",
    "The Linear Non-Gaussian Acyclic Model (LiNGAM) algorithm is designed to recover the causal structure among a set of continuous variables under the assumption of linear relationships and non-Gaussian noise. Unlike traditional structural equation modeling approaches that often require prior knowledge of the causal ordering, LiNGAM exploits non-Gaussianity to identify a unique directed acyclic graph (DAG) structure without such assumptions. The DirectLiNGAM variant leverages ICA-based principles and prior knowledge constraints to optimize the causal discovery process.\n",
    "\n",
    "### Inputs and Parameters\n",
    "\n",
    "Let $X \\in \\mathbb{R}^{n \\times d}$ be a data matrix of $n$ observations over $d$ continuous variables. The algorithm requires the following inputs:\n",
    "\n",
    "- $X$: Filtered data matrix of continuous-valued features.\n",
    "- $\\mathcal{C} \\in \\{-1, 0\\}^{d \\times d}$: Prior knowledge matrix where:\n",
    "  - $-1$ indicates an unknown causal relation,\n",
    "  - $0$ forbids a causal relation.\n",
    "- $\\mathcal{T}$: A tiered structure on variables indicating causal ordering constraints.\n",
    "- $\\phi: \\mathcal{N} \\rightarrow \\{1, \\dots, d\\}$: Mapping from feature names to indices.\n",
    "- $\\mathcal{N}$: A list of continuous variable names.\n",
    "\n",
    "### Algorithmic Procedure\n",
    "\n",
    "1. **Feature Filtering**:\n",
    "   - Select a subset $\\mathcal{F} \\subseteq \\mathcal{N}$ of continuous features suitable for linear causal modeling.\n",
    "   - Remove constant-valued features to ensure identifiability.\n",
    "\n",
    "2. **Constraint Matrix Construction**:\n",
    "   - Initialize $\\mathcal{C}$ as a $d \\times d$ matrix with all entries set to $-1$ (unknown).\n",
    "   - Enforce domain-specific constraints:\n",
    "     - For instance, enforce $C_{i,:} = 0$ for variables in tier 0 or fixed exogenous variables.\n",
    "     - Disallow edges that violate the temporal or tiered structure by setting $C_{ij} = 0$.\n",
    "     - Maintain acyclicity and remove self-causal loops.\n",
    "\n",
    "3. **Model Fitting**:\n",
    "   - Apply the DirectLiNGAM algorithm using the constraint matrix $\\mathcal{C}$.\n",
    "   - DirectLiNGAM assumes the data generation model:\n",
    "     $$\n",
    "     X = B X + \\varepsilon\n",
    "     $$\n",
    "     where $B$ is a strictly lower-triangular matrix representing causal coefficients and $\\varepsilon$ are independent non-Gaussian error terms.\n",
    "   - The model estimates $B$ such that:\n",
    "     - $B_{ij} \\neq 0 \\implies$ variable $j$ is a direct cause of variable $i$.\n",
    "     - $B$ respects the ordering induced by non-Gaussianity and enforced constraints.\n",
    "\n",
    "4. **Adjacency Matrix Extraction**:\n",
    "   - Construct the adjacency matrix $A \\in \\{0,1\\}^{d \\times d}$ where $A_{ij} = 1$ iff $B_{ij} \\neq 0$.\n",
    "\n",
    "5. **Constraint Validation**:\n",
    "   - Validate $A$ against the tiered structure $\\mathcal{T}$ to identify violations of assumed causal directions.\n",
    "\n",
    "6. **Graph Construction and Visualization**:\n",
    "   - Generate a directed graph $G = (V, E)$ with $V = \\mathcal{F}$ and $(i, j) \\in E$ iff $A_{ij} = 1$.\n",
    "   - Visualize and save the graph and adjacency matrix to disk for interpretability and further analysis.\n",
    "\n",
    "### Theoretical Justification\n",
    "\n",
    "LiNGAM rests on the identifiability of linear non-Gaussian acyclic models. Under the assumption of linear structural equations and statistically independent, non-Gaussian noise, the causal ordering and structure are identifiable from observational dataâ€”a result not guaranteed in Gaussian settings. The DirectLiNGAM algorithm employs ICA-based estimation techniques and exploits second-order statistics to determine the causal ordering, then estimates the structural coefficients via regression.\n",
    "\n",
    "### Outcomes and Limitations\n",
    "\n",
    "The algorithm outputs a DAG $G$, an adjacency matrix $A$, a list of tier violations, and structural quality metrics (e.g., SHD, precision, recall). Its computational complexity is dominated by ICA and regression steps, typically scaling as $O(d^3)$ for $d$ variables. Limitations include its restriction to linear causal relationships and its reliance on non-Gaussian noise for identifiability. It also does not handle latent confounders or feedback loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "qHaCqPEFHHbh"
   },
   "outputs": [],
   "source": [
    "class LiNGAMAlgorithm(CausalDiscoveryAlgorithm):\n",
    "    def fit(\n",
    "        self,\n",
    "        data,\n",
    "        constraint_matrix, # Main constraint matrix; LiNGAM will use a filtered version\n",
    "        node_names,        # Full list of node names\n",
    "        node_name_to_idx,  # Mapping for full list\n",
    "        tiers,             # Tiers for full list\n",
    "        output_dir         # Algorithm-specific sub-directory, e.g., \"causal_discovery_output/LiNGAM\"\n",
    "    ):\n",
    "        logger.info(\"Running LiNGAM algorithm...\")\n",
    "        try:\n",
    "            # --- LiNGAM Specific Data Preparation ---\n",
    "            # LiNGAM operates on continuous (numeric) features only.\n",
    "            # Define the list of numeric features intended for LiNGAM (e.g., from data exploration phase).\n",
    "            # Example: numeric_features_for_lingam = [\"Age\", \"Number of Dependents\", ..., \"Total Revenue\"]\n",
    "            # Here, we'll use a predefined list and filter by columns present in the current dataset.\n",
    "            potential_lingam_features = [\"Age\", \"Number of Dependents\", \"Number of Referrals\", \"Tenure in Months\", \"Total Revenue\"]\n",
    "            current_numeric_features = [nf for nf in potential_lingam_features if nf in node_names]\n",
    "\n",
    "            if not current_numeric_features:\n",
    "                logger.error(\"LiNGAM: No pre-defined numeric features found in the dataset.\")\n",
    "                return {\"dag\": nx.DiGraph(), \"adj_matrix\": np.array([]), \"metrics\": {}, \"violations\": [\"No numeric data\"], \"error\": \"No numeric data\"}\n",
    "\n",
    "            lingam_data = data[current_numeric_features].copy()\n",
    "\n",
    "            # Remove constant columns from LiNGAM's input data as they provide no variance\n",
    "            constant_cols = [col for col in lingam_data.columns if lingam_data[col].std() == 0]\n",
    "            if constant_cols:\n",
    "                logger.warning(f\"LiNGAM: Removing constant columns: {constant_cols}\")\n",
    "                lingam_data = lingam_data.drop(columns=constant_cols)\n",
    "                current_numeric_features = [f for f in current_numeric_features if f not in constant_cols]\n",
    "\n",
    "            # Ensure sufficient features remain after filtering\n",
    "            if lingam_data.empty or lingam_data.shape[1] < 2:\n",
    "                logger.error(\"LiNGAM: Not enough valid features to run after preprocessing.\")\n",
    "                return {\"dag\": nx.DiGraph(), \"adj_matrix\": np.array([]), \"metrics\": {}, \"violations\": [\"Insufficient features\"], \"error\": \"Insufficient features\"}\n",
    "\n",
    "            # Handle NaNs by mean imputation for LiNGAM\n",
    "            if lingam_data.isna().any().any():\n",
    "                logger.warning(\"LiNGAM: Data contains NaNs. Filling with column means.\")\n",
    "                lingam_data = lingam_data.fillna(lingam_data.mean())\n",
    "\n",
    "            # --- LiNGAM Specific Constraint Matrix Setup ---\n",
    "            # Map selected continuous feature names to their new indices for LiNGAM\n",
    "            lingam_node_to_idx_map = {name: i for i, name in enumerate(current_numeric_features)}\n",
    "            \n",
    "            # Initialize LiNGAM's prior_knowledge matrix (-1: unknown, 0: forbidden, 1: required)\n",
    "            lingam_prior_knowledge = np.full(\n",
    "                (len(current_numeric_features), len(current_numeric_features)), -1, dtype=np.int32\n",
    "            )\n",
    "\n",
    "            # Adapt general tiers to the subset of features used by LiNGAM\n",
    "            lingam_tiers_filtered = []\n",
    "            for tier_group in tiers:\n",
    "                filtered_tier_group = [f for f in tier_group if f in current_numeric_features]\n",
    "                if filtered_tier_group:\n",
    "                    lingam_tiers_filtered.append(filtered_tier_group)\n",
    "            \n",
    "            # Apply tier-based constraints to LiNGAM's prior_knowledge matrix\n",
    "            if lingam_tiers_filtered and lingam_tiers_filtered[0]: # Tier 0 (e.g., demographics)\n",
    "                for feature_in_tier0 in lingam_tiers_filtered[0]:\n",
    "                    if feature_in_tier0 in lingam_node_to_idx_map:\n",
    "                        idx = lingam_node_to_idx_map[feature_in_tier0]\n",
    "                        lingam_prior_knowledge[:, idx] = 0  # No incoming edges to Tier 0 features\n",
    "                        for other_feature_in_tier0 in lingam_tiers_filtered[0]: # No edges within Tier 0\n",
    "                            if feature_in_tier0 != other_feature_in_tier0 and other_feature_in_tier0 in lingam_node_to_idx_map:\n",
    "                                other_idx = lingam_node_to_idx_map[other_feature_in_tier0]\n",
    "                                lingam_prior_knowledge[idx, other_idx] = 0\n",
    "            \n",
    "            # Example: If \"Total Revenue\" is a known sink among continuous variables for LiNGAM\n",
    "            if \"Total Revenue\" in lingam_node_to_idx_map:\n",
    "                tr_idx = lingam_node_to_idx_map[\"Total Revenue\"]\n",
    "                lingam_prior_knowledge[tr_idx, :] = 0 # No outgoing edges from Total Revenue\n",
    "\n",
    "            # Fit the DirectLiNGAM model\n",
    "            model = lingam.DirectLiNGAM(prior_knowledge=lingam_prior_knowledge)\n",
    "            model.fit(lingam_data)\n",
    "            \n",
    "            # Extract the adjacency matrix containing causal coefficients\n",
    "            adj_matrix_coeffs = model.adjacency_matrix_\n",
    "\n",
    "            # --- 1. Heatmap of LiNGAM Coefficients ---\n",
    "            heatmap_fig_size = (max(8, len(current_numeric_features) + 2), max(6, len(current_numeric_features)))\n",
    "            plt.figure(figsize=heatmap_fig_size, dpi=300)\n",
    "            sns.heatmap(adj_matrix_coeffs, xticklabels=current_numeric_features, yticklabels=current_numeric_features,\n",
    "                        cmap=\"vlag\", center=0, annot=True, fmt=\".2f\", annot_kws={\"size\": 8})\n",
    "            plt.title(\"LiNGAM - Estimated Adjacency Matrix (Causal Coefficients)\")\n",
    "            heatmap_filename = os.path.join(output_dir, \"lingam_adj_matrix_heatmap.png\")\n",
    "            plt.savefig(heatmap_filename, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            logger.info(f\"LiNGAM coefficient matrix heatmap saved as {heatmap_filename}\")\n",
    "\n",
    "            # --- 2. Generate Causal Graphs at Different Coefficient Thresholds ---\n",
    "            thresholds_to_explore_lingam = [0.1, 0.3, 0.5, 0.7] # Thresholds for absolute coefficient values\n",
    "            logger.info(f\"LiNGAM: Generating graphs for varying thresholds on absolute coefficients: {thresholds_to_explore_lingam}\")\n",
    "\n",
    "            for thresh_val in thresholds_to_explore_lingam:\n",
    "                # Threshold based on absolute coefficient strength\n",
    "                adj_matrix_loop = np.where(np.abs(adj_matrix_coeffs) > thresh_val, adj_matrix_coeffs, 0)\n",
    "                G_loop_for_viz = nx.DiGraph(adj_matrix_loop) # Integer-indexed for visualization function\n",
    "                \n",
    "                loop_graph_filename = os.path.join(output_dir, f\"lingam_graph_thresh_{thresh_val:.2f}.png\")\n",
    "                visualize_causal_graph(G_loop_for_viz, current_numeric_features, loop_graph_filename)\n",
    "                logger.info(f\"LiNGAM graph for threshold {thresh_val:.2f} saved as {loop_graph_filename}\")\n",
    "\n",
    "                loop_relations_filename = os.path.join(output_dir, f\"lingam_relations_thresh_{thresh_val:.2f}.txt\")\n",
    "                save_relations_to_text(adj_matrix_loop, current_numeric_features, loop_relations_filename, threshold=0.001) # adj_matrix_loop is already 0/1 effectively\n",
    "\n",
    "            # --- Primary Graph Generation & Analysis (based on non-zero LiNGAM coefficients) ---\n",
    "            # For LiNGAM, an edge exists if its corresponding coefficient is non-zero.\n",
    "            violations = validate_constraints(\n",
    "                adj_matrix_coeffs,        # Validate using the raw coefficient matrix\n",
    "                lingam_node_to_idx_map,   # LiNGAM-specific node to index map\n",
    "                lingam_tiers_filtered,    # LiNGAM-specific tiers\n",
    "                threshold=1e-9            # Consider any non-zero coefficient as an edge\n",
    "            )\n",
    "\n",
    "            # Create the primary graph structure (integer-indexed)\n",
    "            G_primary_int_indexed = nx.DiGraph()\n",
    "            G_primary_int_indexed.add_nodes_from(range(len(current_numeric_features)))\n",
    "            for i in range(len(current_numeric_features)):\n",
    "                for j in range(len(current_numeric_features)):\n",
    "                    if adj_matrix_coeffs[i, j] != 0:\n",
    "                        G_primary_int_indexed.add_edge(i, j, weight=adj_matrix_coeffs[i, j])\n",
    "            \n",
    "            # Visualize and get the named primary graph\n",
    "            G_viz_primary_named = visualize_causal_graph(\n",
    "                G_primary_int_indexed,\n",
    "                current_numeric_features,\n",
    "                os.path.join(output_dir, \"lingam_graph_primary.png\")\n",
    "            )\n",
    "\n",
    "            # Save primary relations based on actual coefficients\n",
    "            save_relations_to_text(\n",
    "                adj_matrix_coeffs, \n",
    "                current_numeric_features,\n",
    "                os.path.join(output_dir, \"lingam_relations_primary.txt\"),\n",
    "                threshold=0.001 # Save any non-negligible coefficient\n",
    "            )\n",
    "            \n",
    "            # Analyze the structure of the primary graph\n",
    "            metrics = analyze_structure_learning(\n",
    "                G_viz_primary_named, \n",
    "                current_numeric_features,\n",
    "                threshold=0.0, # Graph structure is already determined by non-zero check\n",
    "                output_dir=output_dir,\n",
    "                algo_prefix=\"lingam\"\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"dag\": G_viz_primary_named,       # The named primary graph\n",
    "                \"adj_matrix\": adj_matrix_coeffs,  # The raw coefficient matrix from LiNGAM\n",
    "                \"metrics\": metrics,\n",
    "                \"violations\": violations\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"LiNGAM failed: %s\", str(e))\n",
    "            import traceback\n",
    "            logger.error(traceback.format_exc()) # Detailed traceback for debugging\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_H3WEKFCHHbh"
   },
   "source": [
    "# 5.4. Define the PC-GIN Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A constraint-aware extension of the PC algorithm using the GIN (Generalized Independence) test. This variant handles both categorical and numerical data through encoding and residual-based independence testing. It builds skeletons, applies orientation rules, and outputs DAGs respecting prior constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "### Purpose and Context\n",
    "\n",
    "The PC-GIN (Peter-Clark with Generalized Independence with Noise) algorithm is a constraint-based method for causal discovery that extends the classical PC algorithm by integrating a regression-based residual independence testâ€”termed the Generalized Independence with Noise (GIN) test. This test evaluates conditional independence through residual correlations after regressing on conditioning sets, enabling causal structure discovery from mixed-type data, including encoded categorical features.\n",
    "\n",
    "### Inputs and Parameters\n",
    "\n",
    "Let $X \\in \\mathbb{R}^{n \\times d}$ denote the dataset of $n$ observations over $d$ variables, where each column may represent either continuous or categorical variables (appropriately encoded). The following inputs are utilized:\n",
    "\n",
    "- $X$: Input data matrix with preprocessed and encoded features.\n",
    "- $\\mathcal{C} \\in \\{-1, 0\\}^{d \\times d}$: Constraint matrix, where:\n",
    "  - $-1$ denotes unknown or unconstrained relationships,\n",
    "  - $0$ forbids a causal edge from variable $i$ to $j$.\n",
    "- $\\alpha$: Significance level for statistical independence testing (default $\\alpha = 0.05$).\n",
    "- $\\mathcal{T}$: Tier structure defining partial ordering constraints among variables.\n",
    "- $\\phi: \\mathcal{N} \\rightarrow \\{1, \\dots, d\\}$: Mapping from variable names to indices.\n",
    "- $\\mathcal{N}$: List of variable names.\n",
    "\n",
    "### Algorithmic Procedure\n",
    "\n",
    "1. **Data Encoding**:\n",
    "   - Categorical variables are encoded using integer encoding.\n",
    "   - The dataset $X$ is transformed accordingly.\n",
    "\n",
    "2. **GIN Conditional Independence Test**:\n",
    "   - For variables $X_i$, $X_j$ conditioned on a set $Z$, fit:\n",
    "     - $X_i = \\beta_{Z \\rightarrow i} Z + \\varepsilon_i$\n",
    "     - $X_j = \\beta_{Z \\rightarrow j} Z + \\varepsilon_j$\n",
    "   - Evaluate the correlation $\\rho(\\varepsilon_i, \\varepsilon_j)$.\n",
    "   - Accept $X_i \\perp X_j \\mid Z$ if the $p$-value from Pearson correlation exceeds $\\alpha$.\n",
    "\n",
    "3. **Skeleton Construction**:\n",
    "   - Initialize a complete undirected graph with edges $(i, j)$ iff $\\mathcal{C}_{ij} = -1$ or $\\mathcal{C}_{ji} = -1$.\n",
    "   - Iteratively remove edges based on conditional independence tests using conditioning sets of increasing size $d = 0, 1, 2, \\dots$:\n",
    "     $$\n",
    "     \\text{If } p > \\alpha \\Rightarrow \\text{remove edge } (i, j)\n",
    "     $$\n",
    "   - Maintain a record of separating sets $\\text{Sep}(i, j)$.\n",
    "\n",
    "4. **Initial DAG Construction**:\n",
    "   - For each remaining undirected edge $(i, j)$:\n",
    "     - If $\\mathcal{C}_{ij} = -1$ and $\\mathcal{C}_{ji} = 0$, orient $i \\to j$.\n",
    "     - If $\\mathcal{C}_{ji} = -1$ and $\\mathcal{C}_{ij} = 0$, orient $j \\to i$.\n",
    "     - If both directions are unconstrained, include both $i \\to j$ and $j \\to i$ provisionally.\n",
    "\n",
    "5. **V-Structure Orientation**:\n",
    "   - For triplets $(i, j, k)$:\n",
    "     - If $i$ and $k$ are not adjacent, and both have edges to $j$:\n",
    "       - If $j \\notin \\text{Sep}(i, k)$, orient as $i \\to j \\leftarrow k$.\n",
    "\n",
    "6. **Conflict Resolution**:\n",
    "   - Eliminate contradictory edges using constraints $\\mathcal{C}$.\n",
    "   - If both $i \\to j$ and $j \\to i$ exist, retain only the edge consistent with $\\mathcal{C}$ or remove arbitrarily if both directions are allowed.\n",
    "\n",
    "### Theoretical Justification\n",
    "\n",
    "PC-GIN integrates the classical PC algorithm, which is asymptotically correct under the causal Markov and faithfulness assumptions, with a residual-based conditional independence test. The GIN test approximates conditional independence by comparing residuals after linear regressions, thereby accommodating some nonlinear and non-Gaussian features in a linear testing framework. The algorithm exploits sparsity in the conditional independence graph to limit the size of conditioning sets, reducing computational burden.\n",
    "\n",
    "### Outcomes and Limitations\n",
    "\n",
    "The output includes a DAG $G$, an adjacency matrix $A \\in \\{0, 1\\}^{d \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "FN0P9JnsHHbh"
   },
   "outputs": [],
   "source": [
    "# Define the PCGINAlgorithm class.\n",
    "class PCGINAlgorithm(CausalDiscoveryAlgorithm):\n",
    "\n",
    "    # Define the fit method for the algorithm.\n",
    "    def fit(\n",
    "        self,\n",
    "        data,\n",
    "        constraint_matrix,\n",
    "        node_names,\n",
    "        node_name_to_idx,\n",
    "        tiers,\n",
    "        output_dir\n",
    "    ):\n",
    "        logger.info(\"Running PC-GIN algorithm...\")\n",
    "\n",
    "        # Handle potential errors.\n",
    "        try:\n",
    "            # Encode categorical columns.\n",
    "            categorical_cols = [\n",
    "                col for col in data.columns if col in [\n",
    "                    \"Gender\",\n",
    "                    \"Internet Type\",\n",
    "                    \"Offer\",\n",
    "                    \"Payment Method\"\n",
    "                ]\n",
    "            ]\n",
    "\n",
    "            encoded_data = data.copy()\n",
    "\n",
    "            # Encode each categorical column.\n",
    "            for col in categorical_cols:\n",
    "                le = LabelEncoder()\n",
    "                encoded_data[col] = le.fit_transform(\n",
    "                    encoded_data[col].astype(str)\n",
    "                )\n",
    "\n",
    "            # Define the GIN conditional independence test function.\n",
    "            def gin_test(X, Y, Z=None, alpha=0.05):\n",
    "                n = len(X)\n",
    "\n",
    "                # Check independence without conditioning variables.\n",
    "                if Z is None or Z.shape[1] == 0:\n",
    "                    corr, p_value = stats.pearsonr(X, Y)\n",
    "                    return p_value\n",
    "\n",
    "                # Fit regression model for X given Z.\n",
    "                model_x = LinearRegression().fit(\n",
    "                    Z,\n",
    "                    X\n",
    "                )\n",
    "                residuals_x = X - model_x.predict(Z)\n",
    "\n",
    "                # Fit regression model for Y given Z.\n",
    "                model_y = LinearRegression().fit(\n",
    "                    Z,\n",
    "                    Y\n",
    "                )\n",
    "                residuals_y = Y - model_y.predict(Z)\n",
    "\n",
    "                # Compute correlation of residuals.\n",
    "                corr, p_value = stats.pearsonr(\n",
    "                    residuals_x,\n",
    "                    residuals_y\n",
    "                )\n",
    "\n",
    "                return p_value\n",
    "\n",
    "            # Define the PC-GIN algorithm function.\n",
    "            def pc_gin(data, constraint_matrix, alpha=0.01):\n",
    "                n = data.shape[1]\n",
    "\n",
    "                # Initialize the skeleton graph.\n",
    "                skeleton = nx.Graph()\n",
    "                skeleton.add_nodes_from(range(n))\n",
    "\n",
    "                separating_sets = {}\n",
    "\n",
    "                # Add initial edges based on constraint matrix.\n",
    "                for i in range(n):\n",
    "                    for j in range(i + 1, n):\n",
    "                        if (\n",
    "                            np.isnan(constraint_matrix[i, j])\n",
    "                            or np.isnan(constraint_matrix[j, i])\n",
    "                        ):\n",
    "                            skeleton.add_edge(i, j)\n",
    "\n",
    "                # Remove edges based on conditional independence tests.\n",
    "                for d in range(n):\n",
    "                    edges = list(skeleton.edges())\n",
    "\n",
    "                    for i, j in edges:\n",
    "                        if not skeleton.has_edge(i, j):\n",
    "                            continue\n",
    "\n",
    "                        adj_i = set(skeleton.neighbors(i)) - {j}\n",
    "\n",
    "                        if len(adj_i) >= d:\n",
    "                            for subset in itertools.combinations(adj_i, d):\n",
    "                                subset_list = list(subset)\n",
    "\n",
    "                                conditioning_set = (\n",
    "                                    data[:, subset_list]\n",
    "                                    if subset_list else None\n",
    "                                )\n",
    "\n",
    "                                p_val = gin_test(\n",
    "                                    data[:, i],\n",
    "                                    data[:, j],\n",
    "                                    conditioning_set.reshape(data.shape[0], -1)\n",
    "                                    if conditioning_set is not None else None,\n",
    "                                    alpha=alpha\n",
    "                                )\n",
    "\n",
    "                                if p_val > alpha:\n",
    "                                    skeleton.remove_edge(i, j)\n",
    "                                    separating_sets[(i, j)] = subset\n",
    "                                    separating_sets[(j, i)] = subset\n",
    "                                    break\n",
    "\n",
    "                # Create a directed acyclic graph (DAG) from the skeleton.\n",
    "                dag = nx.DiGraph()\n",
    "                dag.add_nodes_from(range(n))\n",
    "\n",
    "                for i, j in skeleton.edges():\n",
    "                    if (\n",
    "                        np.isnan(constraint_matrix[i, j])\n",
    "                        and not np.isnan(constraint_matrix[j, i])\n",
    "                    ):\n",
    "                        dag.add_edge(i, j)\n",
    "\n",
    "                    elif (\n",
    "                        np.isnan(constraint_matrix[j, i])\n",
    "                        and not np.isnan(constraint_matrix[i, j])\n",
    "                    ):\n",
    "                        dag.add_edge(j, i)\n",
    "\n",
    "                    else:\n",
    "                        dag.add_edge(i, j)\n",
    "                        dag.add_edge(j, i)\n",
    "\n",
    "                # Orient edges based on separating sets.\n",
    "                for i in range(n):\n",
    "                    for j in range(n):\n",
    "                        if i == j or not dag.has_edge(i, j):\n",
    "                            continue\n",
    "\n",
    "                        for k in range(n):\n",
    "                            if k == i or k == j:\n",
    "                                continue\n",
    "\n",
    "                            if dag.has_edge(k, j) and not skeleton.has_edge(i, k):\n",
    "                                if (\n",
    "                                    (i, k) in separating_sets\n",
    "                                    and j not in separating_sets[(i, k)]\n",
    "                                ) or (\n",
    "                                    (k, i) in separating_sets\n",
    "                                    and j not in separating_sets[(k, i)]\n",
    "                                ):\n",
    "                                    if dag.has_edge(j, i):\n",
    "                                        dag.remove_edge(j, i)\n",
    "\n",
    "                                    if dag.has_edge(j, k):\n",
    "                                        dag.remove_edge(j, k)\n",
    "\n",
    "                # Remove conflicting edges based on constraint matrix.\n",
    "                for i, j in list(dag.edges()):\n",
    "                    if dag.has_edge(j, i):\n",
    "                        if (\n",
    "                            np.isnan(constraint_matrix[i, j])\n",
    "                            and not np.isnan(constraint_matrix[j, i])\n",
    "                        ):\n",
    "                            dag.remove_edge(j, i)\n",
    "\n",
    "                        elif (\n",
    "                            np.isnan(constraint_matrix[j, i])\n",
    "                            and not np.isnan(constraint_matrix[i, j])\n",
    "                        ):\n",
    "                            dag.remove_edge(i, j)\n",
    "\n",
    "                        else:\n",
    "                            dag.remove_edge(j, i)\n",
    "\n",
    "                return dag\n",
    "\n",
    "            # Run the PC-GIN algorithm.\n",
    "            dag = pc_gin(\n",
    "                encoded_data.values,\n",
    "                constraint_matrix,\n",
    "                alpha=0.05\n",
    "            )\n",
    "\n",
    "            # Create adjacency matrix from DAG.\n",
    "            adj_matrix = nx.to_numpy_array(\n",
    "                dag,\n",
    "                nodelist=range(len(node_names))\n",
    "            )\n",
    "\n",
    "            # Validate constraints.\n",
    "            violations = validate_constraints(\n",
    "                dag,\n",
    "                node_name_to_idx,\n",
    "                tiers\n",
    "            )\n",
    "\n",
    "            # Visualize and save results.\n",
    "            G_viz = visualize_causal_graph(\n",
    "                dag,\n",
    "                node_names,\n",
    "                os.path.join(\n",
    "                    output_dir,\n",
    "                    \"pcgin_graph.png\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "            save_relations_to_text(\n",
    "                dag,\n",
    "                node_names,\n",
    "                os.path.join(\n",
    "                    output_dir,\n",
    "                    \"pcgin_relations.txt\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Analyze structure.\n",
    "            metrics = analyze_structure_learning(\n",
    "                dag,\n",
    "                node_names\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"dag\": dag,\n",
    "                \"adj_matrix\": adj_matrix,\n",
    "                \"metrics\": metrics,\n",
    "                \"violations\": violations\n",
    "            }\n",
    "\n",
    "        # Log and raise exception if an error occurs.\n",
    "        except Exception as e:\n",
    "            logger.error(\n",
    "                \"PC-GIN failed: %s\",\n",
    "                str(e)\n",
    "            )\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSJ4bvi9HHbi"
   },
   "source": [
    "# 5.5. Define the NOTEARS Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrates NOTEARS (Non-combinatorial Optimization via Trace Exponential and Augmented lagRangian for Structure learning) with soft constraint enforcement. The implementation focuses on constrained matrix optimization to produce acyclic structures, followed by graph conversion, evaluation, and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "### Purpose and Context\n",
    "\n",
    "The NOTEARS (Non-combinatorial Optimization via Trace Exponential and Augmented lagRangian for Structure learning) algorithm is a continuous optimization approach to learning a directed acyclic graph (DAG) from observational data. It reformulates the combinatorial structure learning problem into a smooth constrained optimization problem, enabling the use of gradient-based solvers. The key innovation lies in enforcing the acyclicity constraint via a differentiable function, allowing efficient optimization over weighted adjacency matrices.\n",
    "\n",
    "### Inputs and Parameters\n",
    "\n",
    "Given a standardized data matrix $X \\in \\mathbb{R}^{n \\times d}$ with $n$ samples and $d$ variables, the algorithm utilizes the following:\n",
    "\n",
    "- $X$: Standardized input data matrix.\n",
    "- $\\mathcal{C} \\in \\{0, 1, \\text{NaN}\\}^{d \\times d}$: Constraint matrix where:\n",
    "  - $0$ forbids an edge,\n",
    "  - $\\text{NaN}$ allows edge inference.\n",
    "- $\\lambda_1$: Regularization parameter for L1 penalty.\n",
    "- $h_{\\text{tol}}$: Tolerance for acyclicity constraint.\n",
    "- $\\rho_{\\max}$: Maximum penalty for augmented Lagrangian.\n",
    "- $w_{\\text{thresh}}$: Threshold for edge weight post-processing.\n",
    "\n",
    "### Algorithmic Procedure\n",
    "\n",
    "1. **Data Standardization**:\n",
    "   - Normalize each column of $X$ to zero mean and unit variance:\n",
    "     $$\n",
    "     X_{\\text{standardized}} = \\frac{X - \\mu}{\\sigma}\n",
    "     $$\n",
    "\n",
    "2. **Acyclicity Constraint Function**:\n",
    "   - For weight matrix $W \\in \\mathbb{R}^{d \\times d}$, define:\n",
    "     $$\n",
    "     h(W) = \\text{tr} \\left( \\exp(W \\circ W / d) \\right) - d\n",
    "     $$\n",
    "     where $\\circ$ denotes the Hadamard product. This function satisfies $h(W) = 0$ iff $W$ corresponds to a DAG.\n",
    "\n",
    "3. **Objective Function**:\n",
    "   - Define the loss as:\n",
    "     $$\n",
    "     \\mathcal{L}(W) = \\frac{1}{2n} \\| X - XW \\|_F^2 + \\lambda_1 \\| W \\|_1\n",
    "     $$\n",
    "     subject to $h(W) = 0$ and $W_{ij} = 0$ if $\\mathcal{C}_{ij} = 0$.\n",
    "\n",
    "4. **Optimization via Augmented Lagrangian**:\n",
    "   - Solve:\n",
    "     $$\n",
    "     \\min_W \\mathcal{L}(W) + \\frac{\\rho}{2} h(W)^2 + \\alpha h(W)\n",
    "     $$\n",
    "     using L-BFGS-B method with updates to penalty parameter $\\rho$ and multiplier $\\alpha$:\n",
    "     - If $|h(W^{(t)})| > \\epsilon$, increase $\\rho$.\n",
    "     - Update $\\alpha \\leftarrow \\alpha + \\rho h(W^{(t)})$.\n",
    "\n",
    "5. **Post-Processing**:\n",
    "   - Threshold weights to induce sparsity:\n",
    "     $$\n",
    "     W_{ij} = 0 \\text{ if } |W_{ij}| < w_{\\text{thresh}}\n",
    "     $$\n",
    "   - Construct graph $G = (V, E)$ with edges from non-zero entries in $W$.\n",
    "\n",
    "6. **Cycle Removal**:\n",
    "   - Iteratively remove the weakest edge in detected cycles to ensure acyclicity:\n",
    "     - Identify a cycle $C \\subset G$.\n",
    "     - Remove edge $(u, v) \\in C$ with minimal $|W_{uv}|$.\n",
    "\n",
    "### Theoretical Justification\n",
    "\n",
    "NOTEARS is grounded in a novel acyclicity characterization based on matrix exponential properties. By expressing the DAG constraint as a smooth function, the method enables direct optimization over real-valued adjacency matrices without resorting to discrete search. The convergence of the augmented Lagrangian method ensures satisfaction of the acyclicity constraint under mild assumptions. L1 regularization induces sparsity, promoting interpretability of the resulting causal graph.\n",
    "\n",
    "### Outcomes and Limitations\n",
    "\n",
    "The algorithm outputs a DAG $G$, adjacency matrix $W \\in \\mathbb{R}^{d \\times d}$, tier violation list, and structural metrics (e.g., SHD, precision, recall). Its computational complexity is dominated by matrix operations and L-BFGS optimization, typically $O(d^3)$ per iteration. NOTEARS assumes linear relationships and may be sensitive to noise or model misspecification. Additionally, soft constraint enforcement may yield approximate DAGs requiring post-hoc cycle corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "hGVXvjyOHHbi"
   },
   "outputs": [],
   "source": [
    "# Ensure these are imported at the top of your notebook:\n",
    "# import matplotlib.pyplot as plt # Already in cell execution_count: 149\n",
    "# import seaborn as sns # You've added this at the start of the class definition\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import networkx as nx\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from causalnex.structure import notears # Already imported with the class\n",
    "# from causalnex.structure.structuremodel import StructureModel # Already imported with the class\n",
    "# Assuming CausalDiscoveryAlgorithm, visualize_causal_graph, save_relations_to_text,\n",
    "# analyze_structure_learning, validate_constraints, and logger are defined elsewhere.\n",
    "\n",
    "class NOTEARSAlgorithm(CausalDiscoveryAlgorithm):\n",
    "    def fit(self, data, constraint_matrix, node_names, node_name_to_idx, tiers, output_dir):\n",
    "        logger.info(\"Running NOTEARS algorithm using CausalNex...\")\n",
    "        try:\n",
    "            # Prepare DataFrame for NOTEARS\n",
    "            if isinstance(data, pd.DataFrame):\n",
    "                df = data.copy()\n",
    "            else:\n",
    "                df = pd.DataFrame(data, columns=node_names)\n",
    "\n",
    "            # Standardize data column-wise, handling zero-variance columns\n",
    "            scaler = StandardScaler()\n",
    "            df_standardized = pd.DataFrame(index=df.index, columns=df.columns, dtype=float)\n",
    "\n",
    "            for col in df.columns:\n",
    "                if df[col].std() == 0:\n",
    "                    logger.warning(\n",
    "                        f\"NOTEARS: Column '{col}' has zero std. deviation. Standardized values set to 0.\"\n",
    "                    )\n",
    "                    df_standardized[col] = 0.0\n",
    "                else:\n",
    "                    df_standardized[col] = scaler.fit_transform(df[[col]]).flatten()\n",
    "            \n",
    "            # Final check to ensure all data is finite for CausalNex\n",
    "            if not np.all(np.isfinite(df_standardized.values)):\n",
    "                logger.warning(\n",
    "                    \"NOTEARS: Non-finite values in standardized data. Replacing with 0.\"\n",
    "                )\n",
    "                df_standardized.replace([np.inf, -np.inf, np.nan], 0, inplace=True)\n",
    "\n",
    "            # Define NOTEARS hyperparameters\n",
    "            max_iter = 200\n",
    "            h_tol = 1e-8\n",
    "            w_threshold = 0.2  # Primary threshold for CausalNex's NOTEARS edge pruning\n",
    "\n",
    "            # Prepare tabu edges (forbidden edges) from the constraint matrix\n",
    "            tabu_edges = []\n",
    "            if constraint_matrix is not None:\n",
    "                for i in range(constraint_matrix.shape[0]):\n",
    "                    for j in range(constraint_matrix.shape[1]):\n",
    "                        if constraint_matrix[i, j] == 0.0: # 0.0 indicates a forbidden edge\n",
    "                            tabu_edges.append((node_names[i], node_names[j]))\n",
    "\n",
    "            logger.info(f\"NOTEARS: Running with {len(tabu_edges)} tabu edges...\")\n",
    "            \n",
    "            # Learn causal structure using CausalNex's NOTEARS implementation\n",
    "            structure_model = notears.from_pandas(\n",
    "                df_standardized,\n",
    "                max_iter=max_iter,\n",
    "                h_tol=h_tol,\n",
    "                w_threshold=w_threshold, # CausalNex uses this to prune edges in the returned model\n",
    "                tabu_edges=tabu_edges if tabu_edges else None\n",
    "            )\n",
    "            \n",
    "            # Extract the weighted adjacency matrix from the learned CausalNex model\n",
    "            # This reflects edges and weights after CausalNex's internal thresholding.\n",
    "            initial_weighted_adj_matrix_df = nx.to_pandas_adjacency(\n",
    "                structure_model, nodelist=node_names, weight=\"weight\"\n",
    "            )\n",
    "            initial_weighted_adj_matrix = initial_weighted_adj_matrix_df.fillna(0).values\n",
    "\n",
    "            # --- 1. Heatmap of the Learned Weighted Adjacency Matrix ---\n",
    "            plt.figure(figsize=(14, 12), dpi=300)\n",
    "            sns.heatmap(initial_weighted_adj_matrix, xticklabels=node_names, yticklabels=node_names, \n",
    "                        cmap=\"viridis\", annot=True, fmt=\".2f\", annot_kws={\"size\": 6})\n",
    "            plt.title(\"NOTEARS - Learned Weighted Adjacency Matrix (after CausalNex thresholding)\")\n",
    "            heatmap_filename = os.path.join(output_dir, \"notears_adj_matrix_heatmap.png\")\n",
    "            plt.savefig(heatmap_filename, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            logger.info(f\"NOTEARS adjacency matrix heatmap saved as {heatmap_filename}\")\n",
    "\n",
    "            # --- 2. Generate Causal Graphs at Different Additional Thresholds ---\n",
    "            # These thresholds are applied on top of the 'initial_weighted_adj_matrix'.\n",
    "            thresholds_to_explore = [0.05, 0.15, 0.25, 0.3] # Customize as needed\n",
    "            logger.info(f\"NOTEARS: Generating graphs for varying additional thresholds: {thresholds_to_explore}\")\n",
    "\n",
    "            for thresh_val in thresholds_to_explore:\n",
    "                # Apply current additional threshold to the (absolute) weights\n",
    "                adj_matrix_loop = np.where(\n",
    "                    np.abs(initial_weighted_adj_matrix) > thresh_val, \n",
    "                    initial_weighted_adj_matrix, \n",
    "                    0\n",
    "                )\n",
    "                G_loop_for_viz = nx.DiGraph(adj_matrix_loop) # Integer-indexed graph\n",
    "                \n",
    "                loop_graph_filename = os.path.join(output_dir, f\"notears_graph_additional_thresh_{thresh_val:.2f}.png\")\n",
    "                visualize_causal_graph(G_loop_for_viz, node_names, loop_graph_filename)\n",
    "                logger.info(f\"NOTEARS graph for additional threshold {thresh_val:.2f} saved as {loop_graph_filename}\")\n",
    "\n",
    "                loop_relations_filename = os.path.join(output_dir, f\"notears_relations_additional_thresh_{thresh_val:.2f}.txt\")\n",
    "                save_relations_to_text(adj_matrix_loop, node_names, loop_relations_filename, threshold=0.01)\n",
    "\n",
    "            # --- Primary Graph Generation & Analysis (using the main w_threshold) ---\n",
    "            # This ensures the primary output is based on the main CausalNex threshold consistently.\n",
    "            adj_matrix_est = np.where(\n",
    "                np.abs(initial_weighted_adj_matrix) > w_threshold, \n",
    "                initial_weighted_adj_matrix, \n",
    "                0\n",
    "            )\n",
    "            \n",
    "            violations = validate_constraints(\n",
    "                adj_matrix_est, \n",
    "                node_name_to_idx, \n",
    "                tiers, \n",
    "                threshold=w_threshold / 2 # Threshold for validate_constraints (if it expects probabilities)\n",
    "            )\n",
    "            \n",
    "            G_primary_int_indexed = nx.DiGraph(adj_matrix_est) # Integer-indexed graph\n",
    "            \n",
    "            # Visualize and save the primary graph (visualize_causal_graph returns the named graph)\n",
    "            G_viz_primary_named = visualize_causal_graph(\n",
    "                G_primary_int_indexed, \n",
    "                node_names, \n",
    "                os.path.join(output_dir, \"notears_graph_primary.png\")\n",
    "            )\n",
    "            \n",
    "            # Save primary relations (save_relations_to_text uses its internal threshold)\n",
    "            save_relations_to_text(\n",
    "                adj_matrix_est, \n",
    "                node_names, \n",
    "                os.path.join(output_dir, \"notears_relations_primary.txt\"), \n",
    "                threshold=w_threshold / 2 \n",
    "            )\n",
    "            \n",
    "            # Analyze the structure of the primary graph\n",
    "            metrics = analyze_structure_learning(\n",
    "                G_viz_primary_named, # Pass the named graph\n",
    "                node_names, \n",
    "                threshold=0.0, # Graph is already binarized by w_threshold\n",
    "                output_dir=output_dir,\n",
    "                algo_prefix=\"notears\"\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"NOTEARS completed. Found {np.sum(adj_matrix_est != 0)} edges for primary graph.\")\n",
    "            \n",
    "            return {\n",
    "                \"dag\": G_viz_primary_named,          # The primary named graph\n",
    "                \"adj_matrix\": adj_matrix_est,        # The primary thresholded adjacency matrix\n",
    "                \"structure_model\": structure_model,  # Original CausalNex model\n",
    "                \"metrics\": metrics,\n",
    "                \"violations\": violations\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"NOTEARS (CausalNex) failed: %s\", str(e))\n",
    "            import traceback\n",
    "            logger.error(traceback.format_exc()) # Detailed traceback for debugging\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFgcACXZHHbi"
   },
   "source": [
    "# 5.6. Define the GRaSP Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implements the GRaSP (Gradient-based Regularized Structure learning with Penalties) algorithm using a custom acyclicity-constrained optimization scheme. The method identifies sparse DAGs by minimizing loss and penalty terms over standardized data, respecting constraint masks throughout optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "### Purpose and Context\n",
    "\n",
    "The GRaSP (Gradient-based Residual and Structure Penalization) algorithm is a score-based causal discovery method that extends NOTEARS by embedding structure- and residual-based penalties within a constrained optimization framework. It is specifically designed to enforce acyclicity while accommodating domain-specific constraints. Like NOTEARS, GRaSP leverages a continuous optimization formulation but differentiates itself by its flexible modularity and explicit incorporation of regularized loss terms and augmented Lagrangian penalties.\n",
    "\n",
    "### Inputs and Parameters\n",
    "\n",
    "Let $X \\in \\mathbb{R}^{n \\times d}$ represent a dataset of $n$ samples across $d$ standardized variables. The algorithm is parameterized as follows:\n",
    "\n",
    "- $X$: Standardized input data matrix.\n",
    "- $\\mathcal{C} \\in \\{0, 1, \\text{NaN}\\}^{d \\times d}$: Constraint matrix, where:\n",
    "  - $0$ indicates forbidden edges,\n",
    "  - $\\text{NaN}$ allows free optimization.\n",
    "- $\\lambda_1$: L1 regularization parameter enforcing sparsity.\n",
    "- $h_{\\text{tol}}$: Tolerance threshold for enforcing acyclicity.\n",
    "- $\\rho_{\\max}$: Maximum penalty for the augmented Lagrangian term.\n",
    "- $w_{\\text{thresh}}$: Threshold applied to final edge weights.\n",
    "- $\\mathcal{T}$: Tier structure for constraint validation.\n",
    "- $\\phi: \\mathcal{N} \\rightarrow \\{1, \\dots, d\\}$: Node-to-index map.\n",
    "\n",
    "### Algorithmic Procedure\n",
    "\n",
    "1. **Data Standardization**:\n",
    "   - For each variable $x_j$, compute:\n",
    "     $$\n",
    "     x_j' = \\frac{x_j - \\mu_j}{\\sigma_j}, \\quad \\text{if } \\sigma_j > 0\n",
    "     $$\n",
    "     or $x_j' = 0$ if $\\sigma_j = 0$.\n",
    "\n",
    "2. **Acyclicity Characterization**:\n",
    "   - Define the smooth acyclicity constraint:\n",
    "     $$\n",
    "     h(W) = \\text{tr}\\left(\\exp(W \\circ W / d)\\right) - d\n",
    "     $$\n",
    "     where $W \\in \\mathbb{R}^{d \\times d}$ is the weighted adjacency matrix.\n",
    "\n",
    "3. **Penalized Loss Function**:\n",
    "   - The optimization objective combines squared loss, L1 regularization, and acyclicity terms:\n",
    "     $$\n",
    "     \\mathcal{L}(W) = \\frac{1}{2n} \\|X - XW\\|_F^2 + \\lambda_1 \\|W\\|_1 + \\frac{\\rho}{2} h(W)^2 + \\alpha h(W)\n",
    "     $$\n",
    "   - Masking is applied such that $W_{ij} = 0$ where $\\mathcal{C}_{ij} = 0$.\n",
    "\n",
    "4. **Gradient Computation**:\n",
    "   - Gradients are computed with respect to all penalty terms, including:\n",
    "     - Residual loss: $\\nabla \\|X - XW\\|_F^2$\n",
    "     - L1 regularization: $\\lambda_1 \\cdot \\text{sign}(W)$\n",
    "     - Acyclicity: $(\\rho h(W) + \\alpha) \\cdot \\nabla h(W)$\n",
    "\n",
    "5. **Augmented Lagrangian Optimization**:\n",
    "   - Use L-BFGS-B to iteratively minimize $\\mathcal{L}(W)$.\n",
    "   - Update multipliers:\n",
    "     $$\n",
    "     \\rho \\leftarrow \\min(\\rho \\cdot 10, \\rho_{\\max}), \\quad \\alpha \\leftarrow \\alpha + \\rho \\cdot h(W)\n",
    "     $$\n",
    "     until $|h(W)| \\leq h_{\\text{tol}}$.\n",
    "\n",
    "6. **Post-Processing**:\n",
    "   - Threshold small weights: $W_{ij} = 0$ if $|W_{ij}| < w_{\\text{thresh}}$.\n",
    "   - Construct graph $G = (V, E)$ from non-zero $W_{ij}$.\n",
    "\n",
    "7. **Cycle Removal**:\n",
    "   - Ensure $G$ is a DAG by iteratively:\n",
    "     - Detecting cycles,\n",
    "     - Removing the edge with the smallest absolute weight within the cycle.\n",
    "\n",
    "### Theoretical Justification\n",
    "\n",
    "GRaSP maintains the theoretical foundation of continuous DAG learning by enforcing a differentiable acyclicity constraint. The use of augmented Lagrangian multipliers allows for a flexible enforcement of structural validity. The combination of L1 sparsity, Frobenius norm loss, and cycle penalization ensures interpretable and data-consistent graph recovery. The convexity of the loss with respect to linear models, along with the smoothness of $h(W)$, permits convergence under standard optimization assumptions.\n",
    "\n",
    "### Outcomes and Limitations\n",
    "\n",
    "The algorithm outputs a DAG $G$, adjacency matrix $W \\in \\mathbb{R}^{d \\times d}$, constraint violations, and structure learning metrics (e.g., SHD, precision, recall). Its complexity is primarily governed by L-BFGS iterations and matrix exponentials, typically $O(d^3)$ per iteration. The main limitations include sensitivity to regularization tuning, assumption of linearity in structural equations, and approximate enforcement of acyclicity that may necessitate post hoc corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "MsQzIkVYHHbi"
   },
   "outputs": [],
   "source": [
    "# Define the GRaSPAlgorithm class.\n",
    "class GRaSPAlgorithm(CausalDiscoveryAlgorithm):\n",
    "\n",
    "    # Define the fit method for the GRaSPAlgorithm class.\n",
    "    def fit(\n",
    "        self,\n",
    "        data,\n",
    "        constraint_matrix,\n",
    "        node_names,\n",
    "        node_name_to_idx,\n",
    "        tiers,\n",
    "        output_dir\n",
    "    ):\n",
    "        # Log the start of the GRaSP algorithm.\n",
    "        logger.info(\"Running GRaSP algorithm...\")\n",
    "\n",
    "        # Handle potential errors.\n",
    "        try:\n",
    "            # Extract values from the data.\n",
    "            X = data.values\n",
    "\n",
    "            # Compute standard deviations and means.\n",
    "            stds = np.std(X, axis=0)\n",
    "            means = np.mean(X, axis=0)\n",
    "\n",
    "            # Standardize the data.\n",
    "            X_standardized = np.where(\n",
    "                stds != 0,\n",
    "                (X - means) / stds,\n",
    "                0\n",
    "            )\n",
    "\n",
    "            # Define the GRaSP algorithm with constraints.\n",
    "            def grasp_with_constraints(\n",
    "                X,\n",
    "                constraint_matrix,\n",
    "                lambda1=0.01,\n",
    "                max_iter=200,\n",
    "                h_tol=1e-8,\n",
    "                rho_max=1e+16,\n",
    "                w_threshold=0.1\n",
    "            ):\n",
    "                # Initialize dimensions and mask.\n",
    "                n, d = X.shape\n",
    "                mask = 1.0 - np.isnan(constraint_matrix).astype(float)\n",
    "\n",
    "                # Define the acyclicity function.\n",
    "                def _h(w):\n",
    "                    W = w.reshape((d, d))\n",
    "                    M = np.eye(d) + W * W / d\n",
    "                    return np.trace(slin.expm(M)) - d\n",
    "\n",
    "                # Define the loss function with penalties.\n",
    "                def _func(w, rho, alpha):\n",
    "                    W = w.reshape((d, d))\n",
    "                    W = W * (1.0 - mask)\n",
    "                    R = X - X @ W\n",
    "\n",
    "                    loss = 0.5 / n * np.sum(R * R)\n",
    "                    l1_penalty = lambda1 * np.sum(np.abs(W))\n",
    "                    h_val = _h(w)\n",
    "\n",
    "                    return (\n",
    "                        loss\n",
    "                        + l1_penalty\n",
    "                        + 0.5 * rho * h_val ** 2\n",
    "                        + alpha * h_val\n",
    "                    )\n",
    "\n",
    "                # Define the gradient of the loss function.\n",
    "                def _grad(w, rho, alpha):\n",
    "                    W = w.reshape((d, d))\n",
    "                    W = W * (1.0 - mask)\n",
    "                    R = X - X @ W\n",
    "\n",
    "                    G_loss = -1.0 / n * X.T @ R\n",
    "                    G_l1 = lambda1 * np.sign(W)\n",
    "\n",
    "                    h_val = _h(w)\n",
    "                    h_gradient = _h_grad(w).reshape((d, d))\n",
    "                    G_acyclicity = (rho * h_val + alpha) * h_gradient\n",
    "\n",
    "                    G = (G_loss + G_l1 + G_acyclicity) * (1.0 - mask)\n",
    "                    return G.flatten()\n",
    "\n",
    "                # Define the gradient of the acyclicity function.\n",
    "                def _h_grad(w):\n",
    "                    W = w.reshape((d, d))\n",
    "                    M = np.eye(d) + W * W / d\n",
    "                    E = slin.expm(M)\n",
    "\n",
    "                    G = E.T * (2 * W / d)\n",
    "                    G = G * (1.0 - mask)\n",
    "\n",
    "                    return G.flatten()\n",
    "\n",
    "                # Initialize optimization parameters.\n",
    "                w_est = np.zeros(d * d)\n",
    "                rho, alpha, h = 1.0, 0.0, np.inf\n",
    "\n",
    "                # Run optimization loop.\n",
    "                for _ in range(max_iter):\n",
    "                    w_new = sopt.minimize(\n",
    "                        lambda w: _func(w, rho, alpha),\n",
    "                        w_est,\n",
    "                        method=\"L-BFGS-B\",\n",
    "                        jac=lambda w: _grad(w, rho, alpha),\n",
    "                        options={\n",
    "                            \"ftol\": 1e-6,\n",
    "                            \"gtol\": 1e-6\n",
    "                        }\n",
    "                    ).x\n",
    "\n",
    "                    h_new = _h(w_new)\n",
    "\n",
    "                    # Break if tolerance or maximum rho is reached.\n",
    "                    if abs(h_new) <= h_tol or rho >= rho_max:\n",
    "                        break\n",
    "\n",
    "                    # Adjust rho based on acyclicity.\n",
    "                    if abs(h_new) > 0.25 * abs(h):\n",
    "                        rho *= 10\n",
    "\n",
    "                    alpha += rho * h_new\n",
    "                    w_est, h = w_new, h_new\n",
    "\n",
    "                # Reshape and threshold the estimated weights.\n",
    "                W_est = w_est.reshape((d, d))\n",
    "                W_est = W_est * (1.0 - mask)\n",
    "                W_est[np.abs(W_est) < w_threshold] = 0\n",
    "\n",
    "                # Create a directed graph from the weight matrix.\n",
    "                G = nx.DiGraph(W_est)\n",
    "\n",
    "                # Remove cycles from the graph.\n",
    "                while not nx.is_directed_acyclic_graph(G):\n",
    "                    # Handle potential cycles in the graph.\n",
    "                    try:\n",
    "                        cycle = nx.find_cycle(G)\n",
    "                        min_weight = float(\"inf\")\n",
    "                        min_edge = None\n",
    "\n",
    "                        # Identify the edge with minimum weight.\n",
    "                        for u, v in cycle:\n",
    "                            if abs(W_est[u, v]) < min_weight:\n",
    "                                min_weight = abs(W_est[u, v])\n",
    "                                min_edge = (u, v)\n",
    "\n",
    "                        # Remove the weakest edge in the cycle.\n",
    "                        if min_edge:\n",
    "                            G.remove_edge(*min_edge)\n",
    "                            W_est[min_edge[0], min_edge[1]] = 0\n",
    "\n",
    "                    except nx.NetworkXNoCycle:\n",
    "                        break\n",
    "\n",
    "                return W_est\n",
    "\n",
    "            # Run GRaSP with constraints.\n",
    "            adj_matrix = grasp_with_constraints(\n",
    "                X_standardized,\n",
    "                constraint_matrix\n",
    "            )\n",
    "\n",
    "            # Validate constraints.\n",
    "            violations = validate_constraints(\n",
    "                adj_matrix,\n",
    "                node_name_to_idx,\n",
    "                tiers\n",
    "            )\n",
    "\n",
    "            # Create a directed graph for visualization.\n",
    "            G = nx.DiGraph(adj_matrix)\n",
    "\n",
    "            # Visualize and save results.\n",
    "            G_viz = visualize_causal_graph(\n",
    "                G,\n",
    "                node_names,\n",
    "                os.path.join(output_dir, \"grasp_graph.png\")\n",
    "            )\n",
    "\n",
    "            save_relations_to_text(\n",
    "                adj_matrix,\n",
    "                node_names,\n",
    "                os.path.join(output_dir, \"grasp_relations.txt\")\n",
    "            )\n",
    "\n",
    "            # Analyze structure.\n",
    "            metrics = analyze_structure_learning(\n",
    "                adj_matrix,\n",
    "                node_names\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"dag\": G,\n",
    "                \"adj_matrix\": adj_matrix,\n",
    "                \"metrics\": metrics,\n",
    "                \"violations\": violations\n",
    "            }\n",
    "\n",
    "        # Log and raise exceptions.\n",
    "        except Exception as e:\n",
    "            logger.error(\"GRaSP failed: %s\", str(e))\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytR2PRKTHHbi"
   },
   "source": [
    "# 6. Prepare the Causal Discovery Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section defines the high-level execution pipeline that orchestrates causal discovery across multiple algorithms. It initializes constraints, prepares the modeling environment, invokes each algorithm's fit() method, captures and logs outputs, and compiles a comparative summary of structural metrics and constraint adherence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Define the Causal Discovery Pipeline Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The central orchestration function that drives the end-to-end pipeline. It applies preprocessing, initializes the constraint matrix, executes all registered causal discovery algorithms, handles errors, and compiles a results dictionary and summary table, enabling comprehensive comparative analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "cNYptQh2HHbi"
   },
   "outputs": [],
   "source": [
    "def run_causal_discovery_pipeline(\n",
    "    train_data,\n",
    "    tiers,\n",
    "    specific_constraints=None,\n",
    "    output_dir=\"causal_discovery_output\"  # This will be the main parent directory\n",
    "):\n",
    "    \"\"\"\n",
    "    Run the causal discovery pipeline for all algorithms.\n",
    "    Each algorithm's output will be saved in a dedicated sub-folder.\n",
    "\n",
    "    Args:\n",
    "        train_data (pd.DataFrame): Training data.\n",
    "        tiers (list): List of tier lists.\n",
    "        specific_constraints (dict): Additional constraints.\n",
    "        output_dir (str): Parent directory to save output files.\n",
    "\n",
    "    Returns:\n",
    "        dict: Results for each algorithm.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting causal discovery pipeline...\")\n",
    "\n",
    "    # Create the main output directory if it doesn't exist\n",
    "    main_parent_output_dir = output_dir \n",
    "    os.makedirs(main_parent_output_dir, exist_ok=True)\n",
    "\n",
    "    node_names = list(train_data.columns)\n",
    "    constraint_matrix, node_name_to_idx = create_constraint_matrix(\n",
    "        node_names,\n",
    "        tiers,\n",
    "        specific_constraints\n",
    "    )\n",
    "\n",
    "    algorithms = {\n",
    "        \"DECI\": DECIAlgorithm(),\n",
    "        \"LiNGAM\": LiNGAMAlgorithm(),\n",
    "        \"NOTEARS\": NOTEARSAlgorithm(),\n",
    "        # \"PC-GIN\": PCGINAlgorithm(), # Uncomment if you want to run these\n",
    "        # \"GRaSP\": GRaSPAlgorithm()  # Uncomment if you want to run these\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for algo_name, algo in algorithms.items():\n",
    "        try:\n",
    "            logger.info(\"Executing %s...\", algo_name)\n",
    "            \n",
    "            # --- Create a specific sub-directory for the current algorithm ---\n",
    "            algo_specific_output_dir = os.path.join(main_parent_output_dir, algo_name)\n",
    "            os.makedirs(algo_specific_output_dir, exist_ok=True)\n",
    "            # --- End of sub-directory creation ---\n",
    "\n",
    "            # This print statement might become redundant if you silence save_relations_to_text\n",
    "            # print(f\"\\n{algo_name} Causal Relationships:\") # You can keep or remove this\n",
    "\n",
    "            result = algo.fit(\n",
    "                train_data,\n",
    "                constraint_matrix,\n",
    "                node_names,\n",
    "                node_name_to_idx,\n",
    "                tiers,\n",
    "                algo_specific_output_dir # Pass the algorithm-specific directory\n",
    "            )\n",
    "            results[algo_name] = result\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"%s failed: %s\", algo_name, str(e))\n",
    "            results[algo_name] = {\"error\": str(e)}\n",
    "\n",
    "    summary = pd.DataFrame({\n",
    "        algo_name: {\n",
    "            \"num_edges\": result[\"metrics\"][\"num_edges\"] if \"metrics\" in result and result[\"metrics\"] else \"N/A\",\n",
    "            \"graph_density\": result[\"metrics\"][\"graph_density\"] if \"metrics\" in result and result[\"metrics\"] else \"N/A\",\n",
    "            \"violations\": len(result[\"violations\"]) if \"violations\" in result else \"N/A\"\n",
    "        } for algo_name, result in results.items()\n",
    "    }).T\n",
    "\n",
    "    logger.info(\"Summary of results:\\n%s\", summary)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **D. Let the Machine Learn the Causal Graphs and Infer Them**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section transitions from defining preparatory functions to applying them for causal structure discovery. It initiates the actual learning of causal graphs from data by first categorizing features into semantically meaningful tiers, imposing domain-informed constraints, and then running the complete causal discovery pipeline. The output includes both learned causal graphs and a validation of these graphs against the constraints using unseen test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Define the Tiers and Constraints, them Learn the Causal Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This subsection lays the foundation for constraint-aware causal discovery by explicitly defining the hierarchical tiers of variables and specifying inter-variable restrictions. These configurations guide the structure learning process, ensuring the inferred graphs are both statistically plausible and domain-compliant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Define Tiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tier structure segments variables into five conceptual layers: Demographic, Customer, Service, Billing, and Outcome. These layers encode a temporal or logical flow of causality, where earlier tiers can influence later ones but not vice versa. This hierarchy is used to constrain edge directionality in causal discovery, ensuring consistency with prior knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data tiers with specific constraints for categorization.\n",
    "tiers = [\n",
    "    [\n",
    "        \"Gender\",\n",
    "        \"Age\",\n",
    "        \"Senior Citizen\",\n",
    "        \"Married\",\n",
    "        \"Number of Dependents\"\n",
    "    ],  # Tier 1: Demographic\n",
    "\n",
    "    [\n",
    "        \"Number of Referrals\",\n",
    "        \"Tenure in Months\",\n",
    "        \"Offer\"\n",
    "    ],  # Tier 2: Customer\n",
    "\n",
    "    [\n",
    "        \"Phone Service\",\n",
    "        \"Multiple Lines\",\n",
    "        \"Internet Type\",\n",
    "        \"Unlimited Data\",\n",
    "        \"Online Security\",\n",
    "        \"Online Backup\",\n",
    "        \"Device Protection Plan\",\n",
    "        \"Premium Tech Support\",\n",
    "        \"Streaming TV\",\n",
    "        \"Streaming Movies\",\n",
    "        \"Streaming Music\"\n",
    "    ],  # Tier 3: Service\n",
    "\n",
    "    [\n",
    "        \"Total Revenue\",\n",
    "        \"Paperless Billing\",\n",
    "        \"Payment Method\"\n",
    "    ],  # Tier 4: Billing\n",
    "\n",
    "    [\n",
    "        \"Churn Label\"\n",
    "    ]  # Tier 5: Outcome\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Define Specific Contsraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section outlines manually imposed constraints on individual variable relationships. Specifically, it disallows causal edges from \"Gender\" to all Service and Billing tier variables, and from \"Internet Type\" to selected streaming and security-related services. These constraints help eliminate known spurious relationships, enhancing the reliability of the learned causal structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define specific feature-pair constraints including forbidden relationships.\n",
    "specific_constraints = {\n",
    "    \"forbidden\": [\n",
    "        (\"Gender\", dst)\n",
    "        # Disallow connections from \"Gender\" to all Tier 3 and Tier 4 features.\n",
    "        for dst in tiers[2] + tiers[3]\n",
    "    ] + [\n",
    "        (\"Internet Type\", dst)\n",
    "        # Disallow connections from \"Internet Type\" to specified streaming and security services.\n",
    "        for dst in [\n",
    "            \"Unlimited Data\",\n",
    "            \"Online Security\",\n",
    "            \"Online Backup\",\n",
    "            \"Device Protection Plan\",\n",
    "            \"Premium Tech Support\",\n",
    "            \"Streaming TV\",\n",
    "            \"Streaming Movies\",\n",
    "            \"Streaming Music\"\n",
    "        ]\n",
    "    ],\n",
    "    \"allowed\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pspBm_tHHHbi"
   },
   "source": [
    "## 7.3. Discover the Causality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step in this block executes the complete causal discovery pipeline across multiple algorithms using the defined tiers and constraints. It evaluates the discovered graphs on test data, measuring adherence to the constraints. This step yields a comparative assessment of each algorithmâ€™s effectiveness in learning valid and interpretable causal structures from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "teQbf1VgHHbj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting causal discovery pipeline...\n",
      "INFO:__main__:Constraint matrix created with shape: (23, 23)\n",
      "INFO:__main__:Executing DECI...\n",
      "INFO:__main__:Running DECI algorithm...\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name        | Type                  | Params\n",
      "------------------------------------------------------\n",
      "0 | auglag_loss | AugLagLossCalculator  | 0     \n",
      "1 | sem_module  | SEMDistributionModule | 36.7 K\n",
      "------------------------------------------------------\n",
      "36.2 K    Trainable params\n",
      "529       Non-trainable params\n",
      "36.7 K    Total params\n",
      "0.147     Total estimated model params size (MB)\n",
      "c:\\Users\\aafz1\\miniconda3\\envs\\causal_env_shahriyar\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\aafz1\\miniconda3\\envs\\causal_env_shahriyar\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:212: You called `self.log('num_edges', ...)` in your `training_step` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'num_edges': ...})` instead.\n",
      "c:\\Users\\aafz1\\miniconda3\\envs\\causal_env_shahriyar\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:212: You called `self.log('num_lr_updates', ...)` in your `on_train_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'num_lr_updates': ...})` instead.\n",
      "c:\\Users\\aafz1\\miniconda3\\envs\\causal_env_shahriyar\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:212: You called `self.log('outer_opt_counter', ...)` in your `on_train_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'outer_opt_counter': ...})` instead.\n",
      "c:\\Users\\aafz1\\miniconda3\\envs\\causal_env_shahriyar\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:212: You called `self.log('step_counter', ...)` in your `on_train_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'step_counter': ...})` instead.\n",
      "c:\\Users\\aafz1\\miniconda3\\envs\\causal_env_shahriyar\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:212: You called `self.log('outer_below_penalty_tol', ...)` in your `on_train_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'outer_below_penalty_tol': ...})` instead.\n",
      "c:\\Users\\aafz1\\miniconda3\\envs\\causal_env_shahriyar\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:212: You called `self.log('outer_max_rho', ...)` in your `on_train_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'outer_max_rho': ...})` instead.\n",
      "c:\\Users\\aafz1\\miniconda3\\envs\\causal_env_shahriyar\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:212: You called `self.log('last_best_step', ...)` in your `on_train_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'last_best_step': ...})` instead.\n",
      "c:\\Users\\aafz1\\miniconda3\\envs\\causal_env_shahriyar\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:212: You called `self.log('last_lr_update_step', ...)` in your `on_train_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'last_lr_update_step': ...})` instead.\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "INFO:__main__:DECI model saved to causal_discovery_output\\DECI\\deci_model.pt\n",
      "INFO:__main__:DECI probability matrix heatmap saved as causal_discovery_output\\DECI\\deci_prob_matrix_heatmap.png\n",
      "INFO:__main__:DECI: Generating graphs for varying thresholds: [0.3, 0.6, 0.7, 0.8]\n",
      "INFO:__main__:Causal graph saved as causal_discovery_output\\DECI\\deci_graph_thresh_0.30.png\n",
      "INFO:__main__:DECI graph for threshold 0.30 saved as causal_discovery_output\\DECI\\deci_graph_thresh_0.30.png\n",
      "INFO:__main__:Causal relationships saved to causal_discovery_output\\DECI\\deci_relations_thresh_0.30.txt (24 relations)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Causal Relationships ===\n",
      "Married -> Number of Referrals (weight: 1.000)\n",
      "---\n",
      "Married -> Tenure in Months (weight: 1.000)\n",
      "---\n",
      "Number of Dependents -> Number of Referrals (weight: 1.000)\n",
      "---\n",
      "Tenure in Months -> Multiple Lines (weight: 1.000)\n",
      "---\n",
      "Tenure in Months -> Online Security (weight: 1.000)\n",
      "---\n",
      "Tenure in Months -> Online Backup (weight: 1.000)\n",
      "---\n",
      "Tenure in Months -> Device Protection Plan (weight: 1.000)\n",
      "---\n",
      "Tenure in Months -> Premium Tech Support (weight: 1.000)\n",
      "---\n",
      "Tenure in Months -> Streaming TV (weight: 1.000)\n",
      "---\n",
      "Tenure in Months -> Streaming Movies (weight: 1.000)\n",
      "---\n",
      "Tenure in Months -> Streaming Music (weight: 1.000)\n",
      "---\n",
      "Multiple Lines -> Total Revenue (weight: 1.000)\n",
      "---\n",
      "Internet Type -> Total Revenue (weight: 1.000)\n",
      "---\n",
      "Internet Type -> Paperless Billing (weight: 1.000)\n",
      "---\n",
      "Unlimited Data -> Paperless Billing (weight: 1.000)\n",
      "---\n",
      "Online Security -> Total Revenue (weight: 1.000)\n",
      "---\n",
      "Online Backup -> Total Revenue (weight: 1.000)\n",
      "---\n",
      "Device Protection Plan -> Total Revenue (weight: 1.000)\n",
      "---\n",
      "Premium Tech Support -> Total Revenue (weight: 1.000)\n",
      "---\n",
      "Streaming TV -> Total Revenue (weight: 1.000)\n",
      "---\n",
      "Streaming Movies -> Total Revenue (weight: 1.000)\n",
      "---\n",
      "Streaming Music -> Total Revenue (weight: 1.000)\n",
      "---\n",
      "Total Revenue -> Churn Label (weight: 1.000)\n",
      "---\n",
      "Paperless Billing -> Churn Label (weight: 1.000)\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Causal graph saved as causal_discovery_output\\DECI\\deci_graph_thresh_0.60.png\n",
      "INFO:__main__:DECI graph for threshold 0.60 saved as causal_discovery_output\\DECI\\deci_graph_thresh_0.60.png\n",
      "INFO:__main__:Causal relationships saved to causal_discovery_output\\DECI\\deci_relations_thresh_0.60.txt (12 relations)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Causal Relationships ===\n",
      "Married -> Number of Referrals (weight: 1.000)\n",
      "---\n",
      "Married -> Tenure in Months (weight: 1.000)\n",
      "---\n",
      "Tenure in Months -> Multiple Lines (weight: 1.000)\n",
      "---\n",
      "Tenure in Months -> Online Security (weight: 1.000)\n",
      "---\n",
      "Tenure in Months -> Online Backup (weight: 1.000)\n",
      "---\n",
      "Tenure in Months -> Device Protection Plan (weight: 1.000)\n",
      "---\n",
      "Tenure in Months -> Premium Tech Support (weight: 1.000)\n",
      "---\n",
      "Tenure in Months -> Streaming Movies (weight: 1.000)\n",
      "---\n",
      "Multiple Lines -> Total Revenue (weight: 1.000)\n",
      "---\n",
      "Internet Type -> Paperless Billing (weight: 1.000)\n",
      "---\n",
      "Online Backup -> Total Revenue (weight: 1.000)\n",
      "---\n",
      "Device Protection Plan -> Total Revenue (weight: 1.000)\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Causal graph saved as causal_discovery_output\\DECI\\deci_graph_thresh_0.70.png\n",
      "INFO:__main__:DECI graph for threshold 0.70 saved as causal_discovery_output\\DECI\\deci_graph_thresh_0.70.png\n",
      "INFO:__main__:Causal relationships saved to causal_discovery_output\\DECI\\deci_relations_thresh_0.70.txt (6 relations)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Causal Relationships ===\n",
      "Married -> Number of Referrals (weight: 1.000)\n",
      "---\n",
      "Married -> Tenure in Months (weight: 1.000)\n",
      "---\n",
      "Tenure in Months -> Multiple Lines (weight: 1.000)\n",
      "---\n",
      "Tenure in Months -> Online Backup (weight: 1.000)\n",
      "---\n",
      "Tenure in Months -> Device Protection Plan (weight: 1.000)\n",
      "---\n",
      "Tenure in Months -> Streaming Movies (weight: 1.000)\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Causal graph saved as causal_discovery_output\\DECI\\deci_graph_thresh_0.80.png\n",
      "INFO:__main__:DECI graph for threshold 0.80 saved as causal_discovery_output\\DECI\\deci_graph_thresh_0.80.png\n",
      "WARNING:__main__:No causal relationships to save for causal_discovery_output\\DECI\\deci_relations_thresh_0.80.txt\n",
      "INFO:__main__:âœ… All constraints validated successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Causal Relationships ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Causal graph saved as causal_discovery_output\\DECI\\deci_graph_primary.png\n",
      "INFO:__main__:Causal relationships saved to causal_discovery_output\\DECI\\deci_relations_primary.txt (26 relations)\n",
      "INFO:__main__:Structure Learning Metrics for DECI: {'num_edges': 18, 'graph_density': '0.036', 'avg_in_degree': '0.783', 'avg_out_degree': '0.783'}\n",
      "INFO:__main__:Executing LiNGAM...\n",
      "INFO:__main__:Running LiNGAM algorithm...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Causal Relationships ===\n",
      "Married -> Number of Referrals (weight: 0.726)\n",
      "---\n",
      "Married -> Tenure in Months (weight: 0.704)\n",
      "---\n",
      "Number of Dependents -> Number of Referrals (weight: 0.441)\n",
      "---\n",
      "Tenure in Months -> Multiple Lines (weight: 0.709)\n",
      "---\n",
      "Tenure in Months -> Online Security (weight: 0.647)\n",
      "---\n",
      "Tenure in Months -> Online Backup (weight: 0.728)\n",
      "---\n",
      "Tenure in Months -> Device Protection Plan (weight: 0.751)\n",
      "---\n",
      "Tenure in Months -> Premium Tech Support (weight: 0.648)\n",
      "---\n",
      "Tenure in Months -> Streaming TV (weight: 0.576)\n",
      "---\n",
      "Tenure in Months -> Streaming Movies (weight: 0.719)\n",
      "---\n",
      "Tenure in Months -> Streaming Music (weight: 0.545)\n",
      "---\n",
      "Offer -> Online Security (weight: 0.202)\n",
      "---\n",
      "Multiple Lines -> Total Revenue (weight: 0.668)\n",
      "---\n",
      "Internet Type -> Total Revenue (weight: 0.321)\n",
      "---\n",
      "Internet Type -> Paperless Billing (weight: 0.683)\n",
      "---\n",
      "Unlimited Data -> Paperless Billing (weight: 0.316)\n",
      "---\n",
      "Online Security -> Total Revenue (weight: 0.538)\n",
      "---\n",
      "Online Backup -> Total Revenue (weight: 0.678)\n",
      "---\n",
      "Device Protection Plan -> Total Revenue (weight: 0.653)\n",
      "---\n",
      "Premium Tech Support -> Total Revenue (weight: 0.541)\n",
      "---\n",
      "Streaming TV -> Total Revenue (weight: 0.499)\n",
      "---\n",
      "Streaming TV -> Paperless Billing (weight: 0.290)\n",
      "---\n",
      "Streaming Movies -> Total Revenue (weight: 0.523)\n",
      "---\n",
      "Streaming Music -> Total Revenue (weight: 0.325)\n",
      "---\n",
      "Total Revenue -> Churn Label (weight: 0.569)\n",
      "---\n",
      "Paperless Billing -> Churn Label (weight: 0.447)\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:LiNGAM coefficient matrix heatmap saved as causal_discovery_output\\LiNGAM\\lingam_adj_matrix_heatmap.png\n",
      "INFO:__main__:LiNGAM: Generating graphs for varying thresholds on absolute coefficients: [0.1, 0.3, 0.5, 0.7]\n",
      "INFO:__main__:Causal graph saved as causal_discovery_output\\LiNGAM\\lingam_graph_thresh_0.10.png\n",
      "INFO:__main__:LiNGAM graph for threshold 0.10 saved as causal_discovery_output\\LiNGAM\\lingam_graph_thresh_0.10.png\n",
      "INFO:__main__:Causal relationships saved to causal_discovery_output\\LiNGAM\\lingam_relations_thresh_0.10.txt (5 relations)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Causal Relationships ===\n",
      "Age -> Total Revenue (weight: 0.150)\n",
      "---\n",
      "Number of Dependents -> Number of Referrals (weight: 0.281)\n",
      "---\n",
      "Number of Dependents -> Tenure in Months (weight: 0.170)\n",
      "---\n",
      "Number of Referrals -> Tenure in Months (weight: 0.337)\n",
      "---\n",
      "Tenure in Months -> Total Revenue (weight: 0.850)\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Causal graph saved as causal_discovery_output\\LiNGAM\\lingam_graph_thresh_0.30.png\n",
      "INFO:__main__:LiNGAM graph for threshold 0.30 saved as causal_discovery_output\\LiNGAM\\lingam_graph_thresh_0.30.png\n",
      "INFO:__main__:Causal relationships saved to causal_discovery_output\\LiNGAM\\lingam_relations_thresh_0.30.txt (2 relations)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Causal Relationships ===\n",
      "Number of Referrals -> Tenure in Months (weight: 0.337)\n",
      "---\n",
      "Tenure in Months -> Total Revenue (weight: 0.850)\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Causal graph saved as causal_discovery_output\\LiNGAM\\lingam_graph_thresh_0.50.png\n",
      "INFO:__main__:LiNGAM graph for threshold 0.50 saved as causal_discovery_output\\LiNGAM\\lingam_graph_thresh_0.50.png\n",
      "INFO:__main__:Causal relationships saved to causal_discovery_output\\LiNGAM\\lingam_relations_thresh_0.50.txt (1 relations)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Causal Relationships ===\n",
      "Tenure in Months -> Total Revenue (weight: 0.850)\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Causal graph saved as causal_discovery_output\\LiNGAM\\lingam_graph_thresh_0.70.png\n",
      "INFO:__main__:LiNGAM graph for threshold 0.70 saved as causal_discovery_output\\LiNGAM\\lingam_graph_thresh_0.70.png\n",
      "INFO:__main__:Causal relationships saved to causal_discovery_output\\LiNGAM\\lingam_relations_thresh_0.70.txt (1 relations)\n",
      "INFO:__main__:âœ… All constraints validated successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Causal Relationships ===\n",
      "Tenure in Months -> Total Revenue (weight: 0.850)\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Causal graph saved as causal_discovery_output\\LiNGAM\\lingam_graph_primary.png\n",
      "INFO:__main__:Causal relationships saved to causal_discovery_output\\LiNGAM\\lingam_relations_primary.txt (5 relations)\n",
      "INFO:__main__:Structure Learning Metrics for LINGAM: {'num_edges': 7, 'graph_density': '0.350', 'avg_in_degree': '1.400', 'avg_out_degree': '1.400'}\n",
      "INFO:__main__:Executing NOTEARS...\n",
      "INFO:__main__:Running NOTEARS algorithm using CausalNex...\n",
      "WARNING:__main__:NOTEARS: Non-finite values in standardized data. Replacing with 0.\n",
      "INFO:__main__:NOTEARS: Running with 445 tabu edges...\n",
      "INFO:root:Learning structure using 'NOTEARS' optimisation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Causal Relationships ===\n",
      "Age -> Total Revenue (weight: 0.150)\n",
      "---\n",
      "Number of Dependents -> Number of Referrals (weight: 0.281)\n",
      "---\n",
      "Number of Dependents -> Tenure in Months (weight: 0.170)\n",
      "---\n",
      "Number of Referrals -> Tenure in Months (weight: 0.337)\n",
      "---\n",
      "Tenure in Months -> Total Revenue (weight: 0.850)\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:NOTEARS adjacency matrix heatmap saved as causal_discovery_output\\NOTEARS\\notears_adj_matrix_heatmap.png\n",
      "INFO:__main__:NOTEARS: Generating graphs for varying additional thresholds: [0.05, 0.15, 0.25, 0.3]\n",
      "INFO:__main__:Causal graph saved as causal_discovery_output\\NOTEARS\\notears_graph_additional_thresh_0.05.png\n",
      "INFO:__main__:NOTEARS graph for additional threshold 0.05 saved as causal_discovery_output\\NOTEARS\\notears_graph_additional_thresh_0.05.png\n",
      "INFO:__main__:Causal relationships saved to causal_discovery_output\\NOTEARS\\notears_relations_additional_thresh_0.05.txt (13 relations)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Causal Relationships ===\n",
      "Married -> Number of Referrals (weight: 0.681)\n",
      "---\n",
      "Married -> Tenure in Months (weight: 0.393)\n",
      "---\n",
      "Tenure in Months -> Multiple Lines (weight: 0.319)\n",
      "---\n",
      "Tenure in Months -> Online Security (weight: 0.298)\n",
      "---\n",
      "Tenure in Months -> Online Backup (weight: 0.356)\n",
      "---\n",
      "Tenure in Months -> Device Protection Plan (weight: 0.356)\n",
      "---\n",
      "Tenure in Months -> Premium Tech Support (weight: 0.302)\n",
      "---\n",
      "Tenure in Months -> Streaming TV (weight: 0.289)\n",
      "---\n",
      "Tenure in Months -> Streaming Movies (weight: 0.302)\n",
      "---\n",
      "Tenure in Months -> Streaming Music (weight: 0.252)\n",
      "---\n",
      "Multiple Lines -> Total Revenue (weight: 0.227)\n",
      "---\n",
      "Online Security -> Total Revenue (weight: 0.202)\n",
      "---\n",
      "Online Backup -> Total Revenue (weight: 0.236)\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Causal graph saved as causal_discovery_output\\NOTEARS\\notears_graph_additional_thresh_0.15.png\n",
      "INFO:__main__:NOTEARS graph for additional threshold 0.15 saved as causal_discovery_output\\NOTEARS\\notears_graph_additional_thresh_0.15.png\n",
      "INFO:__main__:Causal relationships saved to causal_discovery_output\\NOTEARS\\notears_relations_additional_thresh_0.15.txt (13 relations)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Causal Relationships ===\n",
      "Married -> Number of Referrals (weight: 0.681)\n",
      "---\n",
      "Married -> Tenure in Months (weight: 0.393)\n",
      "---\n",
      "Tenure in Months -> Multiple Lines (weight: 0.319)\n",
      "---\n",
      "Tenure in Months -> Online Security (weight: 0.298)\n",
      "---\n",
      "Tenure in Months -> Online Backup (weight: 0.356)\n",
      "---\n",
      "Tenure in Months -> Device Protection Plan (weight: 0.356)\n",
      "---\n",
      "Tenure in Months -> Premium Tech Support (weight: 0.302)\n",
      "---\n",
      "Tenure in Months -> Streaming TV (weight: 0.289)\n",
      "---\n",
      "Tenure in Months -> Streaming Movies (weight: 0.302)\n",
      "---\n",
      "Tenure in Months -> Streaming Music (weight: 0.252)\n",
      "---\n",
      "Multiple Lines -> Total Revenue (weight: 0.227)\n",
      "---\n",
      "Online Security -> Total Revenue (weight: 0.202)\n",
      "---\n",
      "Online Backup -> Total Revenue (weight: 0.236)\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Causal graph saved as causal_discovery_output\\NOTEARS\\notears_graph_additional_thresh_0.25.png\n",
      "INFO:__main__:NOTEARS graph for additional threshold 0.25 saved as causal_discovery_output\\NOTEARS\\notears_graph_additional_thresh_0.25.png\n",
      "INFO:__main__:Causal relationships saved to causal_discovery_output\\NOTEARS\\notears_relations_additional_thresh_0.25.txt (10 relations)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Causal Relationships ===\n",
      "Married -> Number of Referrals (weight: 0.681)\n",
      "---\n",
      "Married -> Tenure in Months (weight: 0.393)\n",
      "---\n",
      "Tenure in Months -> Multiple Lines (weight: 0.319)\n",
      "---\n",
      "Tenure in Months -> Online Security (weight: 0.298)\n",
      "---\n",
      "Tenure in Months -> Online Backup (weight: 0.356)\n",
      "---\n",
      "Tenure in Months -> Device Protection Plan (weight: 0.356)\n",
      "---\n",
      "Tenure in Months -> Premium Tech Support (weight: 0.302)\n",
      "---\n",
      "Tenure in Months -> Streaming TV (weight: 0.289)\n",
      "---\n",
      "Tenure in Months -> Streaming Movies (weight: 0.302)\n",
      "---\n",
      "Tenure in Months -> Streaming Music (weight: 0.252)\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Causal graph saved as causal_discovery_output\\NOTEARS\\notears_graph_additional_thresh_0.30.png\n",
      "INFO:__main__:NOTEARS graph for additional threshold 0.30 saved as causal_discovery_output\\NOTEARS\\notears_graph_additional_thresh_0.30.png\n",
      "INFO:__main__:Causal relationships saved to causal_discovery_output\\NOTEARS\\notears_relations_additional_thresh_0.30.txt (7 relations)\n",
      "INFO:__main__:âœ… All constraints validated successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Causal Relationships ===\n",
      "Married -> Number of Referrals (weight: 0.681)\n",
      "---\n",
      "Married -> Tenure in Months (weight: 0.393)\n",
      "---\n",
      "Tenure in Months -> Multiple Lines (weight: 0.319)\n",
      "---\n",
      "Tenure in Months -> Online Backup (weight: 0.356)\n",
      "---\n",
      "Tenure in Months -> Device Protection Plan (weight: 0.356)\n",
      "---\n",
      "Tenure in Months -> Premium Tech Support (weight: 0.302)\n",
      "---\n",
      "Tenure in Months -> Streaming Movies (weight: 0.302)\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Causal graph saved as causal_discovery_output\\NOTEARS\\notears_graph_primary.png\n",
      "INFO:__main__:Causal relationships saved to causal_discovery_output\\NOTEARS\\notears_relations_primary.txt (13 relations)\n",
      "INFO:__main__:Structure Learning Metrics for NOTEARS: {'num_edges': 14, 'graph_density': '0.028', 'avg_in_degree': '0.609', 'avg_out_degree': '0.609'}\n",
      "INFO:__main__:NOTEARS completed. Found 14 edges for primary graph.\n",
      "INFO:__main__:Summary of results:\n",
      "         num_edges  graph_density  violations\n",
      "DECI          18.0           0.04         0.0\n",
      "LiNGAM         7.0           0.35         0.0\n",
      "NOTEARS       14.0           0.03         0.0\n",
      "INFO:__main__:Constraint matrix created with shape: (23, 23)\n",
      "INFO:__main__:Evaluating constraint violations on test data...\n",
      "WARNING:__main__:NaNs in test data, filling with mean\n",
      "INFO:__main__:âœ… All constraints validated successfully\n",
      "INFO:__main__:DECI: 0 constraint violations on test data: None\n",
      "INFO:__main__:âœ… All constraints validated successfully\n",
      "INFO:__main__:LiNGAM: 0 constraint violations on test data: None\n",
      "INFO:__main__:âœ… All constraints validated successfully\n",
      "INFO:__main__:NOTEARS: 0 constraint violations on test data: None\n",
      "INFO:__main__:Constraint violation summary:\n",
      "        constraint_violations violation_details\n",
      "DECI                        0              None\n",
      "LiNGAM                      0              None\n",
      "NOTEARS                     0              None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Causal Relationships ===\n",
      "Married -> Number of Referrals (weight: 0.681)\n",
      "---\n",
      "Married -> Tenure in Months (weight: 0.393)\n",
      "---\n",
      "Tenure in Months -> Multiple Lines (weight: 0.319)\n",
      "---\n",
      "Tenure in Months -> Online Security (weight: 0.298)\n",
      "---\n",
      "Tenure in Months -> Online Backup (weight: 0.356)\n",
      "---\n",
      "Tenure in Months -> Device Protection Plan (weight: 0.356)\n",
      "---\n",
      "Tenure in Months -> Premium Tech Support (weight: 0.302)\n",
      "---\n",
      "Tenure in Months -> Streaming TV (weight: 0.289)\n",
      "---\n",
      "Tenure in Months -> Streaming Movies (weight: 0.302)\n",
      "---\n",
      "Tenure in Months -> Streaming Music (weight: 0.252)\n",
      "---\n",
      "Multiple Lines -> Total Revenue (weight: 0.227)\n",
      "---\n",
      "Online Security -> Total Revenue (weight: 0.202)\n",
      "---\n",
      "Online Backup -> Total Revenue (weight: 0.236)\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Run the causal discovery pipeline.\n",
    "results = run_causal_discovery_pipeline(train_data, tiers, specific_constraints, output_dir=\"causal_discovery_output\")\n",
    "\n",
    "# Evaluate constraint violations on test data.\n",
    "node_names = list(train_data.columns)\n",
    "_, node_name_to_idx = create_constraint_matrix(node_names, tiers, specific_constraints)\n",
    "evaluation_summary = evaluate_causal_discovery(results, test_data, tiers, node_name_to_idx)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "causal_env_shahriyar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
