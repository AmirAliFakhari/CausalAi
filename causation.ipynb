{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZu59Z3oHHbN"
   },
   "source": [
    "<b><p style=\"font-size: 40px;\">Causal Discovery and Inference in Customer Churn</p></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yY7aFtJuHHbQ"
   },
   "source": [
    "<b><p style=\"font-size: 35px;\">I. First Phase: Prepare the Data</p></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5W8DuAsLHHbR"
   },
   "source": [
    "---\n",
    "# 1. Import the Relevant Packages and Configuire Them if Needed\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twH5vEvXHHbR"
   },
   "source": [
    "## 1.1. Import the Libraries and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6JLXa5s9HHbS"
   },
   "outputs": [],
   "source": [
    "# Import core Python utilities for iteration, serialization, logging, OS operations, warnings, and abstract base classes.\n",
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "from abc import ABC, abstractmethod\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZZnw05AaHHbT"
   },
   "outputs": [],
   "source": [
    "# Import filesystem, plotting, graph, array, data handling, ML frameworks, and utility libraries.\n",
    "import fsspec\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.linalg as slin\n",
    "import scipy.optimize as sopt\n",
    "from scipy import stats\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensordict import TensorDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XTOQ0G7bHHbT"
   },
   "outputs": [],
   "source": [
    "# Import Causica and LiNGAM modules for causal inference, SEMs, and optimization routines.\n",
    "import causica.distributions as _continuous_noise\n",
    "from causica.distributions import ContinuousNoiseDist\n",
    "from causica.datasets.causica_dataset_format import CAUSICA_DATASETS_PATH, Variable\n",
    "from causica.lightning.data_modules.basic_data_module import BasicDECIDataModule\n",
    "from causica.lightning.modules.deci_module import DECIModule\n",
    "from causica.sem.sem_distribution import SEMDistributionModule\n",
    "from causica.sem.structural_equation_model import ite\n",
    "from causica.training.auglag import AugLagLRConfig\n",
    "import lingam\n",
    "from lingam.utils import make_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bbc5icmXHHbU"
   },
   "source": [
    "## 1.2. Configure the Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "76Tf09iJHHbU"
   },
   "outputs": [],
   "source": [
    "# Configure NumPy, Pandas, and Matplotlib display settings for clear and consistent outputs.\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.precision\", 2)\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pd2Yin-iHHbV"
   },
   "source": [
    "## 1.3. Setup the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2WleNj6tHHbV"
   },
   "outputs": [],
   "source": [
    "# Enable PyTorch MPS fallback for macOS GPU support.\n",
    "PYTORCH_ENABLE_MPS_FALLBACK = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "sGGXYuE2HHbV"
   },
   "outputs": [],
   "source": [
    "# Determine if running in test mode and set file paths for data and variables.\n",
    "test_run = bool(os.getenv(\"TEST_RUN\", False))\n",
    "DATA_PATH = \"data/dataset.csv\"\n",
    "VARIABLES_PATH = \"data/variables.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Y1cuBO43HHbV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set global random seed for reproducible experiments across NumPy, PyTorch, and PyTorch Lightning.\n",
    "SEED = 100\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pl.seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8wErVcpHHbV"
   },
   "source": [
    "## 1.4. Setup the Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zAcW9nauHHbV"
   },
   "outputs": [],
   "source": [
    "# Initialize application-wide logging to file and console at INFO level.\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"causal_discovery.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3iexM24KHHbW"
   },
   "source": [
    "---\n",
    "# 2. Read and Show the Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KX5s5N6-HHbW"
   },
   "source": [
    "## 2.1. Read the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "H3dhIK6_HHbW"
   },
   "outputs": [],
   "source": [
    "# Load the dataset xslsx file into a Pandas DataFrame.\n",
    "data = pd.read_csv(\n",
    "    DATA_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcQZBRXEHHbW"
   },
   "source": [
    "## 2.2. Display the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "sKPokcEQHHbW"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_8afce\">\n",
       "  <caption><b>IBM Telco Customer Churn Dataset (First 5 Rows)</b></caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_8afce_level0_col0\" class=\"col_heading level0 col0\" >Customer ID</th>\n",
       "      <th id=\"T_8afce_level0_col1\" class=\"col_heading level0 col1\" >Gender</th>\n",
       "      <th id=\"T_8afce_level0_col2\" class=\"col_heading level0 col2\" >Age</th>\n",
       "      <th id=\"T_8afce_level0_col3\" class=\"col_heading level0 col3\" >Under 30</th>\n",
       "      <th id=\"T_8afce_level0_col4\" class=\"col_heading level0 col4\" >Senior Citizen</th>\n",
       "      <th id=\"T_8afce_level0_col5\" class=\"col_heading level0 col5\" >Married</th>\n",
       "      <th id=\"T_8afce_level0_col6\" class=\"col_heading level0 col6\" >Dependents</th>\n",
       "      <th id=\"T_8afce_level0_col7\" class=\"col_heading level0 col7\" >Number of Dependents</th>\n",
       "      <th id=\"T_8afce_level0_col8\" class=\"col_heading level0 col8\" >Country</th>\n",
       "      <th id=\"T_8afce_level0_col9\" class=\"col_heading level0 col9\" >State</th>\n",
       "      <th id=\"T_8afce_level0_col10\" class=\"col_heading level0 col10\" >City</th>\n",
       "      <th id=\"T_8afce_level0_col11\" class=\"col_heading level0 col11\" >Zip Code</th>\n",
       "      <th id=\"T_8afce_level0_col12\" class=\"col_heading level0 col12\" >Latitude</th>\n",
       "      <th id=\"T_8afce_level0_col13\" class=\"col_heading level0 col13\" >Longitude</th>\n",
       "      <th id=\"T_8afce_level0_col14\" class=\"col_heading level0 col14\" >Population</th>\n",
       "      <th id=\"T_8afce_level0_col15\" class=\"col_heading level0 col15\" >Quarter</th>\n",
       "      <th id=\"T_8afce_level0_col16\" class=\"col_heading level0 col16\" >Referred a Friend</th>\n",
       "      <th id=\"T_8afce_level0_col17\" class=\"col_heading level0 col17\" >Number of Referrals</th>\n",
       "      <th id=\"T_8afce_level0_col18\" class=\"col_heading level0 col18\" >Tenure in Months</th>\n",
       "      <th id=\"T_8afce_level0_col19\" class=\"col_heading level0 col19\" >Offer</th>\n",
       "      <th id=\"T_8afce_level0_col20\" class=\"col_heading level0 col20\" >Phone Service</th>\n",
       "      <th id=\"T_8afce_level0_col21\" class=\"col_heading level0 col21\" >Avg Monthly Long Distance Charges</th>\n",
       "      <th id=\"T_8afce_level0_col22\" class=\"col_heading level0 col22\" >Multiple Lines</th>\n",
       "      <th id=\"T_8afce_level0_col23\" class=\"col_heading level0 col23\" >Internet Service</th>\n",
       "      <th id=\"T_8afce_level0_col24\" class=\"col_heading level0 col24\" >Internet Type</th>\n",
       "      <th id=\"T_8afce_level0_col25\" class=\"col_heading level0 col25\" >Avg Monthly GB Download</th>\n",
       "      <th id=\"T_8afce_level0_col26\" class=\"col_heading level0 col26\" >Online Security</th>\n",
       "      <th id=\"T_8afce_level0_col27\" class=\"col_heading level0 col27\" >Online Backup</th>\n",
       "      <th id=\"T_8afce_level0_col28\" class=\"col_heading level0 col28\" >Device Protection Plan</th>\n",
       "      <th id=\"T_8afce_level0_col29\" class=\"col_heading level0 col29\" >Premium Tech Support</th>\n",
       "      <th id=\"T_8afce_level0_col30\" class=\"col_heading level0 col30\" >Streaming TV</th>\n",
       "      <th id=\"T_8afce_level0_col31\" class=\"col_heading level0 col31\" >Streaming Movies</th>\n",
       "      <th id=\"T_8afce_level0_col32\" class=\"col_heading level0 col32\" >Streaming Music</th>\n",
       "      <th id=\"T_8afce_level0_col33\" class=\"col_heading level0 col33\" >Unlimited Data</th>\n",
       "      <th id=\"T_8afce_level0_col34\" class=\"col_heading level0 col34\" >Contract</th>\n",
       "      <th id=\"T_8afce_level0_col35\" class=\"col_heading level0 col35\" >Paperless Billing</th>\n",
       "      <th id=\"T_8afce_level0_col36\" class=\"col_heading level0 col36\" >Payment Method</th>\n",
       "      <th id=\"T_8afce_level0_col37\" class=\"col_heading level0 col37\" >Monthly Charge</th>\n",
       "      <th id=\"T_8afce_level0_col38\" class=\"col_heading level0 col38\" >Total Charges</th>\n",
       "      <th id=\"T_8afce_level0_col39\" class=\"col_heading level0 col39\" >Total Refunds</th>\n",
       "      <th id=\"T_8afce_level0_col40\" class=\"col_heading level0 col40\" >Total Extra Data Charges</th>\n",
       "      <th id=\"T_8afce_level0_col41\" class=\"col_heading level0 col41\" >Total Long Distance Charges</th>\n",
       "      <th id=\"T_8afce_level0_col42\" class=\"col_heading level0 col42\" >Total Revenue</th>\n",
       "      <th id=\"T_8afce_level0_col43\" class=\"col_heading level0 col43\" >Satisfaction Score</th>\n",
       "      <th id=\"T_8afce_level0_col44\" class=\"col_heading level0 col44\" >Customer Status</th>\n",
       "      <th id=\"T_8afce_level0_col45\" class=\"col_heading level0 col45\" >Churn Label</th>\n",
       "      <th id=\"T_8afce_level0_col46\" class=\"col_heading level0 col46\" >Churn Score</th>\n",
       "      <th id=\"T_8afce_level0_col47\" class=\"col_heading level0 col47\" >CLTV</th>\n",
       "      <th id=\"T_8afce_level0_col48\" class=\"col_heading level0 col48\" >Churn Category</th>\n",
       "      <th id=\"T_8afce_level0_col49\" class=\"col_heading level0 col49\" >Churn Reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_8afce_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_8afce_row0_col0\" class=\"data row0 col0\" >8779-QRDMV</td>\n",
       "      <td id=\"T_8afce_row0_col1\" class=\"data row0 col1\" >Male</td>\n",
       "      <td id=\"T_8afce_row0_col2\" class=\"data row0 col2\" >78</td>\n",
       "      <td id=\"T_8afce_row0_col3\" class=\"data row0 col3\" >No</td>\n",
       "      <td id=\"T_8afce_row0_col4\" class=\"data row0 col4\" >Yes</td>\n",
       "      <td id=\"T_8afce_row0_col5\" class=\"data row0 col5\" >No</td>\n",
       "      <td id=\"T_8afce_row0_col6\" class=\"data row0 col6\" >No</td>\n",
       "      <td id=\"T_8afce_row0_col7\" class=\"data row0 col7\" >0</td>\n",
       "      <td id=\"T_8afce_row0_col8\" class=\"data row0 col8\" >United States</td>\n",
       "      <td id=\"T_8afce_row0_col9\" class=\"data row0 col9\" >California</td>\n",
       "      <td id=\"T_8afce_row0_col10\" class=\"data row0 col10\" >Los Angeles</td>\n",
       "      <td id=\"T_8afce_row0_col11\" class=\"data row0 col11\" >90022</td>\n",
       "      <td id=\"T_8afce_row0_col12\" class=\"data row0 col12\" >34.023810</td>\n",
       "      <td id=\"T_8afce_row0_col13\" class=\"data row0 col13\" >-118.156582</td>\n",
       "      <td id=\"T_8afce_row0_col14\" class=\"data row0 col14\" >68701</td>\n",
       "      <td id=\"T_8afce_row0_col15\" class=\"data row0 col15\" >Q3</td>\n",
       "      <td id=\"T_8afce_row0_col16\" class=\"data row0 col16\" >No</td>\n",
       "      <td id=\"T_8afce_row0_col17\" class=\"data row0 col17\" >0</td>\n",
       "      <td id=\"T_8afce_row0_col18\" class=\"data row0 col18\" >1</td>\n",
       "      <td id=\"T_8afce_row0_col19\" class=\"data row0 col19\" >None</td>\n",
       "      <td id=\"T_8afce_row0_col20\" class=\"data row0 col20\" >No</td>\n",
       "      <td id=\"T_8afce_row0_col21\" class=\"data row0 col21\" >0.000000</td>\n",
       "      <td id=\"T_8afce_row0_col22\" class=\"data row0 col22\" >No</td>\n",
       "      <td id=\"T_8afce_row0_col23\" class=\"data row0 col23\" >Yes</td>\n",
       "      <td id=\"T_8afce_row0_col24\" class=\"data row0 col24\" >DSL</td>\n",
       "      <td id=\"T_8afce_row0_col25\" class=\"data row0 col25\" >8</td>\n",
       "      <td id=\"T_8afce_row0_col26\" class=\"data row0 col26\" >No</td>\n",
       "      <td id=\"T_8afce_row0_col27\" class=\"data row0 col27\" >No</td>\n",
       "      <td id=\"T_8afce_row0_col28\" class=\"data row0 col28\" >Yes</td>\n",
       "      <td id=\"T_8afce_row0_col29\" class=\"data row0 col29\" >No</td>\n",
       "      <td id=\"T_8afce_row0_col30\" class=\"data row0 col30\" >No</td>\n",
       "      <td id=\"T_8afce_row0_col31\" class=\"data row0 col31\" >Yes</td>\n",
       "      <td id=\"T_8afce_row0_col32\" class=\"data row0 col32\" >No</td>\n",
       "      <td id=\"T_8afce_row0_col33\" class=\"data row0 col33\" >No</td>\n",
       "      <td id=\"T_8afce_row0_col34\" class=\"data row0 col34\" >Month-to-Month</td>\n",
       "      <td id=\"T_8afce_row0_col35\" class=\"data row0 col35\" >Yes</td>\n",
       "      <td id=\"T_8afce_row0_col36\" class=\"data row0 col36\" >Bank Withdrawal</td>\n",
       "      <td id=\"T_8afce_row0_col37\" class=\"data row0 col37\" >39.650000</td>\n",
       "      <td id=\"T_8afce_row0_col38\" class=\"data row0 col38\" >39.650000</td>\n",
       "      <td id=\"T_8afce_row0_col39\" class=\"data row0 col39\" >0.000000</td>\n",
       "      <td id=\"T_8afce_row0_col40\" class=\"data row0 col40\" >20</td>\n",
       "      <td id=\"T_8afce_row0_col41\" class=\"data row0 col41\" >0.000000</td>\n",
       "      <td id=\"T_8afce_row0_col42\" class=\"data row0 col42\" >59.650000</td>\n",
       "      <td id=\"T_8afce_row0_col43\" class=\"data row0 col43\" >3</td>\n",
       "      <td id=\"T_8afce_row0_col44\" class=\"data row0 col44\" >Churned</td>\n",
       "      <td id=\"T_8afce_row0_col45\" class=\"data row0 col45\" >Yes</td>\n",
       "      <td id=\"T_8afce_row0_col46\" class=\"data row0 col46\" >91</td>\n",
       "      <td id=\"T_8afce_row0_col47\" class=\"data row0 col47\" >5433</td>\n",
       "      <td id=\"T_8afce_row0_col48\" class=\"data row0 col48\" >Competitor</td>\n",
       "      <td id=\"T_8afce_row0_col49\" class=\"data row0 col49\" >Competitor offered more data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8afce_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_8afce_row1_col0\" class=\"data row1 col0\" >7495-OOKFY</td>\n",
       "      <td id=\"T_8afce_row1_col1\" class=\"data row1 col1\" >Female</td>\n",
       "      <td id=\"T_8afce_row1_col2\" class=\"data row1 col2\" >74</td>\n",
       "      <td id=\"T_8afce_row1_col3\" class=\"data row1 col3\" >No</td>\n",
       "      <td id=\"T_8afce_row1_col4\" class=\"data row1 col4\" >Yes</td>\n",
       "      <td id=\"T_8afce_row1_col5\" class=\"data row1 col5\" >Yes</td>\n",
       "      <td id=\"T_8afce_row1_col6\" class=\"data row1 col6\" >Yes</td>\n",
       "      <td id=\"T_8afce_row1_col7\" class=\"data row1 col7\" >1</td>\n",
       "      <td id=\"T_8afce_row1_col8\" class=\"data row1 col8\" >United States</td>\n",
       "      <td id=\"T_8afce_row1_col9\" class=\"data row1 col9\" >California</td>\n",
       "      <td id=\"T_8afce_row1_col10\" class=\"data row1 col10\" >Los Angeles</td>\n",
       "      <td id=\"T_8afce_row1_col11\" class=\"data row1 col11\" >90063</td>\n",
       "      <td id=\"T_8afce_row1_col12\" class=\"data row1 col12\" >34.044271</td>\n",
       "      <td id=\"T_8afce_row1_col13\" class=\"data row1 col13\" >-118.185237</td>\n",
       "      <td id=\"T_8afce_row1_col14\" class=\"data row1 col14\" >55668</td>\n",
       "      <td id=\"T_8afce_row1_col15\" class=\"data row1 col15\" >Q3</td>\n",
       "      <td id=\"T_8afce_row1_col16\" class=\"data row1 col16\" >Yes</td>\n",
       "      <td id=\"T_8afce_row1_col17\" class=\"data row1 col17\" >1</td>\n",
       "      <td id=\"T_8afce_row1_col18\" class=\"data row1 col18\" >8</td>\n",
       "      <td id=\"T_8afce_row1_col19\" class=\"data row1 col19\" >Offer E</td>\n",
       "      <td id=\"T_8afce_row1_col20\" class=\"data row1 col20\" >Yes</td>\n",
       "      <td id=\"T_8afce_row1_col21\" class=\"data row1 col21\" >48.850000</td>\n",
       "      <td id=\"T_8afce_row1_col22\" class=\"data row1 col22\" >Yes</td>\n",
       "      <td id=\"T_8afce_row1_col23\" class=\"data row1 col23\" >Yes</td>\n",
       "      <td id=\"T_8afce_row1_col24\" class=\"data row1 col24\" >Fiber Optic</td>\n",
       "      <td id=\"T_8afce_row1_col25\" class=\"data row1 col25\" >17</td>\n",
       "      <td id=\"T_8afce_row1_col26\" class=\"data row1 col26\" >No</td>\n",
       "      <td id=\"T_8afce_row1_col27\" class=\"data row1 col27\" >Yes</td>\n",
       "      <td id=\"T_8afce_row1_col28\" class=\"data row1 col28\" >No</td>\n",
       "      <td id=\"T_8afce_row1_col29\" class=\"data row1 col29\" >No</td>\n",
       "      <td id=\"T_8afce_row1_col30\" class=\"data row1 col30\" >No</td>\n",
       "      <td id=\"T_8afce_row1_col31\" class=\"data row1 col31\" >No</td>\n",
       "      <td id=\"T_8afce_row1_col32\" class=\"data row1 col32\" >No</td>\n",
       "      <td id=\"T_8afce_row1_col33\" class=\"data row1 col33\" >Yes</td>\n",
       "      <td id=\"T_8afce_row1_col34\" class=\"data row1 col34\" >Month-to-Month</td>\n",
       "      <td id=\"T_8afce_row1_col35\" class=\"data row1 col35\" >Yes</td>\n",
       "      <td id=\"T_8afce_row1_col36\" class=\"data row1 col36\" >Credit Card</td>\n",
       "      <td id=\"T_8afce_row1_col37\" class=\"data row1 col37\" >80.650000</td>\n",
       "      <td id=\"T_8afce_row1_col38\" class=\"data row1 col38\" >633.300000</td>\n",
       "      <td id=\"T_8afce_row1_col39\" class=\"data row1 col39\" >0.000000</td>\n",
       "      <td id=\"T_8afce_row1_col40\" class=\"data row1 col40\" >0</td>\n",
       "      <td id=\"T_8afce_row1_col41\" class=\"data row1 col41\" >390.800000</td>\n",
       "      <td id=\"T_8afce_row1_col42\" class=\"data row1 col42\" >1024.100000</td>\n",
       "      <td id=\"T_8afce_row1_col43\" class=\"data row1 col43\" >3</td>\n",
       "      <td id=\"T_8afce_row1_col44\" class=\"data row1 col44\" >Churned</td>\n",
       "      <td id=\"T_8afce_row1_col45\" class=\"data row1 col45\" >Yes</td>\n",
       "      <td id=\"T_8afce_row1_col46\" class=\"data row1 col46\" >69</td>\n",
       "      <td id=\"T_8afce_row1_col47\" class=\"data row1 col47\" >5302</td>\n",
       "      <td id=\"T_8afce_row1_col48\" class=\"data row1 col48\" >Competitor</td>\n",
       "      <td id=\"T_8afce_row1_col49\" class=\"data row1 col49\" >Competitor made better offer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8afce_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_8afce_row2_col0\" class=\"data row2 col0\" >1658-BYGOY</td>\n",
       "      <td id=\"T_8afce_row2_col1\" class=\"data row2 col1\" >Male</td>\n",
       "      <td id=\"T_8afce_row2_col2\" class=\"data row2 col2\" >71</td>\n",
       "      <td id=\"T_8afce_row2_col3\" class=\"data row2 col3\" >No</td>\n",
       "      <td id=\"T_8afce_row2_col4\" class=\"data row2 col4\" >Yes</td>\n",
       "      <td id=\"T_8afce_row2_col5\" class=\"data row2 col5\" >No</td>\n",
       "      <td id=\"T_8afce_row2_col6\" class=\"data row2 col6\" >Yes</td>\n",
       "      <td id=\"T_8afce_row2_col7\" class=\"data row2 col7\" >3</td>\n",
       "      <td id=\"T_8afce_row2_col8\" class=\"data row2 col8\" >United States</td>\n",
       "      <td id=\"T_8afce_row2_col9\" class=\"data row2 col9\" >California</td>\n",
       "      <td id=\"T_8afce_row2_col10\" class=\"data row2 col10\" >Los Angeles</td>\n",
       "      <td id=\"T_8afce_row2_col11\" class=\"data row2 col11\" >90065</td>\n",
       "      <td id=\"T_8afce_row2_col12\" class=\"data row2 col12\" >34.108833</td>\n",
       "      <td id=\"T_8afce_row2_col13\" class=\"data row2 col13\" >-118.229715</td>\n",
       "      <td id=\"T_8afce_row2_col14\" class=\"data row2 col14\" >47534</td>\n",
       "      <td id=\"T_8afce_row2_col15\" class=\"data row2 col15\" >Q3</td>\n",
       "      <td id=\"T_8afce_row2_col16\" class=\"data row2 col16\" >No</td>\n",
       "      <td id=\"T_8afce_row2_col17\" class=\"data row2 col17\" >0</td>\n",
       "      <td id=\"T_8afce_row2_col18\" class=\"data row2 col18\" >18</td>\n",
       "      <td id=\"T_8afce_row2_col19\" class=\"data row2 col19\" >Offer D</td>\n",
       "      <td id=\"T_8afce_row2_col20\" class=\"data row2 col20\" >Yes</td>\n",
       "      <td id=\"T_8afce_row2_col21\" class=\"data row2 col21\" >11.330000</td>\n",
       "      <td id=\"T_8afce_row2_col22\" class=\"data row2 col22\" >Yes</td>\n",
       "      <td id=\"T_8afce_row2_col23\" class=\"data row2 col23\" >Yes</td>\n",
       "      <td id=\"T_8afce_row2_col24\" class=\"data row2 col24\" >Fiber Optic</td>\n",
       "      <td id=\"T_8afce_row2_col25\" class=\"data row2 col25\" >52</td>\n",
       "      <td id=\"T_8afce_row2_col26\" class=\"data row2 col26\" >No</td>\n",
       "      <td id=\"T_8afce_row2_col27\" class=\"data row2 col27\" >No</td>\n",
       "      <td id=\"T_8afce_row2_col28\" class=\"data row2 col28\" >No</td>\n",
       "      <td id=\"T_8afce_row2_col29\" class=\"data row2 col29\" >No</td>\n",
       "      <td id=\"T_8afce_row2_col30\" class=\"data row2 col30\" >Yes</td>\n",
       "      <td id=\"T_8afce_row2_col31\" class=\"data row2 col31\" >Yes</td>\n",
       "      <td id=\"T_8afce_row2_col32\" class=\"data row2 col32\" >Yes</td>\n",
       "      <td id=\"T_8afce_row2_col33\" class=\"data row2 col33\" >Yes</td>\n",
       "      <td id=\"T_8afce_row2_col34\" class=\"data row2 col34\" >Month-to-Month</td>\n",
       "      <td id=\"T_8afce_row2_col35\" class=\"data row2 col35\" >Yes</td>\n",
       "      <td id=\"T_8afce_row2_col36\" class=\"data row2 col36\" >Bank Withdrawal</td>\n",
       "      <td id=\"T_8afce_row2_col37\" class=\"data row2 col37\" >95.450000</td>\n",
       "      <td id=\"T_8afce_row2_col38\" class=\"data row2 col38\" >1752.550000</td>\n",
       "      <td id=\"T_8afce_row2_col39\" class=\"data row2 col39\" >45.610000</td>\n",
       "      <td id=\"T_8afce_row2_col40\" class=\"data row2 col40\" >0</td>\n",
       "      <td id=\"T_8afce_row2_col41\" class=\"data row2 col41\" >203.940000</td>\n",
       "      <td id=\"T_8afce_row2_col42\" class=\"data row2 col42\" >1910.880000</td>\n",
       "      <td id=\"T_8afce_row2_col43\" class=\"data row2 col43\" >2</td>\n",
       "      <td id=\"T_8afce_row2_col44\" class=\"data row2 col44\" >Churned</td>\n",
       "      <td id=\"T_8afce_row2_col45\" class=\"data row2 col45\" >Yes</td>\n",
       "      <td id=\"T_8afce_row2_col46\" class=\"data row2 col46\" >81</td>\n",
       "      <td id=\"T_8afce_row2_col47\" class=\"data row2 col47\" >3179</td>\n",
       "      <td id=\"T_8afce_row2_col48\" class=\"data row2 col48\" >Competitor</td>\n",
       "      <td id=\"T_8afce_row2_col49\" class=\"data row2 col49\" >Competitor made better offer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8afce_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_8afce_row3_col0\" class=\"data row3 col0\" >4598-XLKNJ</td>\n",
       "      <td id=\"T_8afce_row3_col1\" class=\"data row3 col1\" >Female</td>\n",
       "      <td id=\"T_8afce_row3_col2\" class=\"data row3 col2\" >78</td>\n",
       "      <td id=\"T_8afce_row3_col3\" class=\"data row3 col3\" >No</td>\n",
       "      <td id=\"T_8afce_row3_col4\" class=\"data row3 col4\" >Yes</td>\n",
       "      <td id=\"T_8afce_row3_col5\" class=\"data row3 col5\" >Yes</td>\n",
       "      <td id=\"T_8afce_row3_col6\" class=\"data row3 col6\" >Yes</td>\n",
       "      <td id=\"T_8afce_row3_col7\" class=\"data row3 col7\" >1</td>\n",
       "      <td id=\"T_8afce_row3_col8\" class=\"data row3 col8\" >United States</td>\n",
       "      <td id=\"T_8afce_row3_col9\" class=\"data row3 col9\" >California</td>\n",
       "      <td id=\"T_8afce_row3_col10\" class=\"data row3 col10\" >Inglewood</td>\n",
       "      <td id=\"T_8afce_row3_col11\" class=\"data row3 col11\" >90303</td>\n",
       "      <td id=\"T_8afce_row3_col12\" class=\"data row3 col12\" >33.936291</td>\n",
       "      <td id=\"T_8afce_row3_col13\" class=\"data row3 col13\" >-118.332639</td>\n",
       "      <td id=\"T_8afce_row3_col14\" class=\"data row3 col14\" >27778</td>\n",
       "      <td id=\"T_8afce_row3_col15\" class=\"data row3 col15\" >Q3</td>\n",
       "      <td id=\"T_8afce_row3_col16\" class=\"data row3 col16\" >Yes</td>\n",
       "      <td id=\"T_8afce_row3_col17\" class=\"data row3 col17\" >1</td>\n",
       "      <td id=\"T_8afce_row3_col18\" class=\"data row3 col18\" >25</td>\n",
       "      <td id=\"T_8afce_row3_col19\" class=\"data row3 col19\" >Offer C</td>\n",
       "      <td id=\"T_8afce_row3_col20\" class=\"data row3 col20\" >Yes</td>\n",
       "      <td id=\"T_8afce_row3_col21\" class=\"data row3 col21\" >19.760000</td>\n",
       "      <td id=\"T_8afce_row3_col22\" class=\"data row3 col22\" >No</td>\n",
       "      <td id=\"T_8afce_row3_col23\" class=\"data row3 col23\" >Yes</td>\n",
       "      <td id=\"T_8afce_row3_col24\" class=\"data row3 col24\" >Fiber Optic</td>\n",
       "      <td id=\"T_8afce_row3_col25\" class=\"data row3 col25\" >12</td>\n",
       "      <td id=\"T_8afce_row3_col26\" class=\"data row3 col26\" >No</td>\n",
       "      <td id=\"T_8afce_row3_col27\" class=\"data row3 col27\" >Yes</td>\n",
       "      <td id=\"T_8afce_row3_col28\" class=\"data row3 col28\" >Yes</td>\n",
       "      <td id=\"T_8afce_row3_col29\" class=\"data row3 col29\" >No</td>\n",
       "      <td id=\"T_8afce_row3_col30\" class=\"data row3 col30\" >Yes</td>\n",
       "      <td id=\"T_8afce_row3_col31\" class=\"data row3 col31\" >Yes</td>\n",
       "      <td id=\"T_8afce_row3_col32\" class=\"data row3 col32\" >No</td>\n",
       "      <td id=\"T_8afce_row3_col33\" class=\"data row3 col33\" >Yes</td>\n",
       "      <td id=\"T_8afce_row3_col34\" class=\"data row3 col34\" >Month-to-Month</td>\n",
       "      <td id=\"T_8afce_row3_col35\" class=\"data row3 col35\" >Yes</td>\n",
       "      <td id=\"T_8afce_row3_col36\" class=\"data row3 col36\" >Bank Withdrawal</td>\n",
       "      <td id=\"T_8afce_row3_col37\" class=\"data row3 col37\" >98.500000</td>\n",
       "      <td id=\"T_8afce_row3_col38\" class=\"data row3 col38\" >2514.500000</td>\n",
       "      <td id=\"T_8afce_row3_col39\" class=\"data row3 col39\" >13.430000</td>\n",
       "      <td id=\"T_8afce_row3_col40\" class=\"data row3 col40\" >0</td>\n",
       "      <td id=\"T_8afce_row3_col41\" class=\"data row3 col41\" >494.000000</td>\n",
       "      <td id=\"T_8afce_row3_col42\" class=\"data row3 col42\" >2995.070000</td>\n",
       "      <td id=\"T_8afce_row3_col43\" class=\"data row3 col43\" >2</td>\n",
       "      <td id=\"T_8afce_row3_col44\" class=\"data row3 col44\" >Churned</td>\n",
       "      <td id=\"T_8afce_row3_col45\" class=\"data row3 col45\" >Yes</td>\n",
       "      <td id=\"T_8afce_row3_col46\" class=\"data row3 col46\" >88</td>\n",
       "      <td id=\"T_8afce_row3_col47\" class=\"data row3 col47\" >5337</td>\n",
       "      <td id=\"T_8afce_row3_col48\" class=\"data row3 col48\" >Dissatisfaction</td>\n",
       "      <td id=\"T_8afce_row3_col49\" class=\"data row3 col49\" >Limited range of services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8afce_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_8afce_row4_col0\" class=\"data row4 col0\" >4846-WHAFZ</td>\n",
       "      <td id=\"T_8afce_row4_col1\" class=\"data row4 col1\" >Female</td>\n",
       "      <td id=\"T_8afce_row4_col2\" class=\"data row4 col2\" >80</td>\n",
       "      <td id=\"T_8afce_row4_col3\" class=\"data row4 col3\" >No</td>\n",
       "      <td id=\"T_8afce_row4_col4\" class=\"data row4 col4\" >Yes</td>\n",
       "      <td id=\"T_8afce_row4_col5\" class=\"data row4 col5\" >Yes</td>\n",
       "      <td id=\"T_8afce_row4_col6\" class=\"data row4 col6\" >Yes</td>\n",
       "      <td id=\"T_8afce_row4_col7\" class=\"data row4 col7\" >1</td>\n",
       "      <td id=\"T_8afce_row4_col8\" class=\"data row4 col8\" >United States</td>\n",
       "      <td id=\"T_8afce_row4_col9\" class=\"data row4 col9\" >California</td>\n",
       "      <td id=\"T_8afce_row4_col10\" class=\"data row4 col10\" >Whittier</td>\n",
       "      <td id=\"T_8afce_row4_col11\" class=\"data row4 col11\" >90602</td>\n",
       "      <td id=\"T_8afce_row4_col12\" class=\"data row4 col12\" >33.972119</td>\n",
       "      <td id=\"T_8afce_row4_col13\" class=\"data row4 col13\" >-118.020188</td>\n",
       "      <td id=\"T_8afce_row4_col14\" class=\"data row4 col14\" >26265</td>\n",
       "      <td id=\"T_8afce_row4_col15\" class=\"data row4 col15\" >Q3</td>\n",
       "      <td id=\"T_8afce_row4_col16\" class=\"data row4 col16\" >Yes</td>\n",
       "      <td id=\"T_8afce_row4_col17\" class=\"data row4 col17\" >1</td>\n",
       "      <td id=\"T_8afce_row4_col18\" class=\"data row4 col18\" >37</td>\n",
       "      <td id=\"T_8afce_row4_col19\" class=\"data row4 col19\" >Offer C</td>\n",
       "      <td id=\"T_8afce_row4_col20\" class=\"data row4 col20\" >Yes</td>\n",
       "      <td id=\"T_8afce_row4_col21\" class=\"data row4 col21\" >6.330000</td>\n",
       "      <td id=\"T_8afce_row4_col22\" class=\"data row4 col22\" >Yes</td>\n",
       "      <td id=\"T_8afce_row4_col23\" class=\"data row4 col23\" >Yes</td>\n",
       "      <td id=\"T_8afce_row4_col24\" class=\"data row4 col24\" >Fiber Optic</td>\n",
       "      <td id=\"T_8afce_row4_col25\" class=\"data row4 col25\" >14</td>\n",
       "      <td id=\"T_8afce_row4_col26\" class=\"data row4 col26\" >No</td>\n",
       "      <td id=\"T_8afce_row4_col27\" class=\"data row4 col27\" >No</td>\n",
       "      <td id=\"T_8afce_row4_col28\" class=\"data row4 col28\" >No</td>\n",
       "      <td id=\"T_8afce_row4_col29\" class=\"data row4 col29\" >No</td>\n",
       "      <td id=\"T_8afce_row4_col30\" class=\"data row4 col30\" >No</td>\n",
       "      <td id=\"T_8afce_row4_col31\" class=\"data row4 col31\" >No</td>\n",
       "      <td id=\"T_8afce_row4_col32\" class=\"data row4 col32\" >No</td>\n",
       "      <td id=\"T_8afce_row4_col33\" class=\"data row4 col33\" >Yes</td>\n",
       "      <td id=\"T_8afce_row4_col34\" class=\"data row4 col34\" >Month-to-Month</td>\n",
       "      <td id=\"T_8afce_row4_col35\" class=\"data row4 col35\" >Yes</td>\n",
       "      <td id=\"T_8afce_row4_col36\" class=\"data row4 col36\" >Bank Withdrawal</td>\n",
       "      <td id=\"T_8afce_row4_col37\" class=\"data row4 col37\" >76.500000</td>\n",
       "      <td id=\"T_8afce_row4_col38\" class=\"data row4 col38\" >2868.150000</td>\n",
       "      <td id=\"T_8afce_row4_col39\" class=\"data row4 col39\" >0.000000</td>\n",
       "      <td id=\"T_8afce_row4_col40\" class=\"data row4 col40\" >0</td>\n",
       "      <td id=\"T_8afce_row4_col41\" class=\"data row4 col41\" >234.210000</td>\n",
       "      <td id=\"T_8afce_row4_col42\" class=\"data row4 col42\" >3102.360000</td>\n",
       "      <td id=\"T_8afce_row4_col43\" class=\"data row4 col43\" >2</td>\n",
       "      <td id=\"T_8afce_row4_col44\" class=\"data row4 col44\" >Churned</td>\n",
       "      <td id=\"T_8afce_row4_col45\" class=\"data row4 col45\" >Yes</td>\n",
       "      <td id=\"T_8afce_row4_col46\" class=\"data row4 col46\" >67</td>\n",
       "      <td id=\"T_8afce_row4_col47\" class=\"data row4 col47\" >2793</td>\n",
       "      <td id=\"T_8afce_row4_col48\" class=\"data row4 col48\" >Price</td>\n",
       "      <td id=\"T_8afce_row4_col49\" class=\"data row4 col49\" >Extra data charges</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x22d96ac7be0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first 10 rows of the dataset.\n",
    "data.head(5).style.set_caption(\n",
    "    \"<b>IBM Telco Customer Churn Dataset (First 5 Rows)</b>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Voq0jRY5HHbW"
   },
   "source": [
    "---\n",
    "# 3. Clean and Prepare the Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QtnvQHqKHHbW"
   },
   "source": [
    "## 3.1. Rename and Reorder the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8mTCr3-uHHbW"
   },
   "outputs": [],
   "source": [
    "# Define the feature mapping for the dataset.\n",
    "feature_mapping = {\n",
    "    \"Customer Info\": [\n",
    "        \"Customer ID\", \"Gender\", \"Age\", \"Under 30\", \"Senior Citizen\", \"Married\", \"Dependents\", \"Number of Dependents\"\n",
    "    ],\n",
    "    \"Location Info\": [\n",
    "        \"Country\", \"State\", \"City\", \"Zip Code\", \"Latitude\", \"Longitude\", \"Population\"\n",
    "    ],\n",
    "    \"Referral & Tenure\": [\n",
    "        \"Quarter\", \"Referred a Friend\", \"Number of Referrals\", \"Tenure in Months\", \"Offer\"\n",
    "    ],\n",
    "    \"Services Signed Up\": [\n",
    "        \"Phone Service\", \"Multiple Lines\", \"Internet Service\", \"Internet Type\", \"Unlimited Data\"\n",
    "    ],\n",
    "    \"Internet Features\": [\n",
    "        \"Online Security\", \"Online Backup\", \"Device Protection Plan\", \"Premium Tech Support\",\n",
    "        \"Streaming TV\", \"Streaming Movies\", \"Streaming Music\"\n",
    "    ],\n",
    "    \"Billing & Payment\": [\n",
    "        \"Avg Monthly Long Distance Charges\", \"Avg Monthly GB Download\", \"Monthly Charge\",\n",
    "        \"Total Charges\", \"Total Refunds\", \"Total Extra Data Charges\",\n",
    "        \"Total Long Distance Charges\", \"Total Revenue\", \"Paperless Billing\", \"Payment Method\"\n",
    "    ],\n",
    "    \"Customer Scores\": [\n",
    "        \"Satisfaction Score\", \"CLTV\", \"Churn Score\"\n",
    "    ],\n",
    "    \"Churn Info\": [\n",
    "        \"Customer Status\", \"Churn Label\", \"Churn Category\", \"Churn Reason\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ZrbQG0MaHHbW"
   },
   "outputs": [],
   "source": [
    "# Flatten the feature_mapping into a single list.\n",
    "desired_order = [col for group in feature_mapping.values() for col in group]\n",
    "\n",
    "# Reorder the DataFrame.\n",
    "data = data[desired_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTFTkKfFHHbW"
   },
   "source": [
    "## 3.2. Remove the Unnecessary Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "4YmalLFDHHbW"
   },
   "outputs": [],
   "source": [
    "# Define the features to remove from the dataset.\n",
    "features_to_remove = [\n",
    "    # Identifiers & Redundant Demographics\n",
    "    \"Customer ID\",           # High cardinality identifier\n",
    "    \"Under 30\",              # Redundant (derivable from Age)\n",
    "    \"Dependents\",            # Redundant (derivable from Number of Dependents)\n",
    "\n",
    "    # Location Info (low variance or low utility)\n",
    "    \"Country\",               # Constant (all United States)\n",
    "    \"State\",                 # Constant (all California)\n",
    "    \"Zip Code\",              # Too granular\n",
    "    \"City\",                  # High cardinality, many unique values\n",
    "    \"Latitude\",              # Granular\n",
    "    \"Longitude\",             # Granular\n",
    "    \"Population\",            # Possibly low variation or correlated with city\n",
    "\n",
    "    # Referral\n",
    "    \"Referred a Friend\",     # Redundant (derivable from Number of Referrals)\n",
    "\n",
    "    # Subscription Redundancy\n",
    "    \"Internet Service\",      # Redundant (inferable from Internet Type)\n",
    "\n",
    "    # Derived or Leaky Features\n",
    "    \"Customer Status\",       # Leaks churn label\n",
    "    \"Churn Score\",           # Usually post-hoc score, potential leakage\n",
    "    \"Churn Category\",        # Sparse & derived from churn\n",
    "    \"Churn Reason\",          # Sparse & derived from churn\n",
    "    \"CLTV\",                  # Leaks churn label\n",
    "    \"Satisfaction Score\",    # Leaks churn label\n",
    "\n",
    "    # Time Feature\n",
    "    \"Quarter\",               # Possibly low relevance unless time modeling is intended\n",
    "\n",
    "    # Financial features removed in favor of only keeping Total Revenue\n",
    "    \"Avg Monthly Long Distance Charges\",    # Usage-level detail removed\n",
    "    \"Avg Monthly GB Download\",              # Usage-level detail removed\n",
    "    \"Monthly Charge\",                       # Snapshot charge removed\n",
    "    \"Total Charges\",                        # Cumulative but derived\n",
    "    \"Total Refunds\",                        # Post-hoc financial info\n",
    "    \"Total Extra Data Charges\",             # Specific fee detail removed\n",
    "    \"Total Long Distance Charges\"           # Specific usage-based revenue removed\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "qMu80bKkHHbX"
   },
   "outputs": [],
   "source": [
    "# Remove the specified columns from the DataFrame.\n",
    "data = data.drop(\n",
    "    columns=features_to_remove\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCKyd0WnHHbX"
   },
   "source": [
    "## 3.3. Distinguish the Categorical and Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WLj9_zOTHHbX"
   },
   "outputs": [],
   "source": [
    "# Define the categorical features in the dataset.\n",
    "categorical_features = [\n",
    "    \"Gender\",\n",
    "    \"Senior Citizen\",\n",
    "    \"Married\",\n",
    "    \"Offer\",\n",
    "    \"Phone Service\",\n",
    "    \"Multiple Lines\",\n",
    "    \"Internet Type\",\n",
    "    \"Unlimited Data\",\n",
    "    \"Online Security\",\n",
    "    \"Online Backup\",\n",
    "    \"Device Protection Plan\",\n",
    "    \"Premium Tech Support\",\n",
    "    \"Streaming TV\",\n",
    "    \"Streaming Movies\",\n",
    "    \"Streaming Music\",\n",
    "    \"Paperless Billing\",\n",
    "    \"Payment Method\",\n",
    "    \"Churn Label\"\n",
    "]\n",
    "\n",
    "# Define the numeric features in the dataset.\n",
    "numeric_features = [\n",
    "    \"Age\",\n",
    "    \"Number of Dependents\",\n",
    "    \"Number of Referrals\",\n",
    "    \"Tenure in Months\",\n",
    "    \"Total Revenue\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djG8R_Q4HHbX"
   },
   "source": [
    "## 3.4. Check the Categorical Features' Unique Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "_l1AgcPJHHbX"
   },
   "outputs": [],
   "source": [
    "# Define a vacant list to store the rows.\n",
    "rows = []\n",
    "\n",
    "# Iterate through the categorical features.\n",
    "for feature in categorical_features:\n",
    "\n",
    "    # Get the unique values and their counts.\n",
    "    value_counts = data[feature].value_counts()\n",
    "    first_row = True\n",
    "\n",
    "    # Iterate through the unique values.\n",
    "    for value, count in value_counts.items():\n",
    "\n",
    "        # Calculate the percentage.\n",
    "        percentage = str(round((count / len(data)) * 100, 2))\n",
    "        # Set the feature name.\n",
    "        feature_name = feature if first_row else \"\"\n",
    "        # Append the unique value, count, and percentage to the rows.\n",
    "        rows.append([\n",
    "            feature_name,\n",
    "            value,\n",
    "            count,\n",
    "            percentage\n",
    "        ])\n",
    "        # Set the first row to False.\n",
    "        first_row = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "3Nje11wEHHbX"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 8\u001b[0m\n\u001b[0;32m      2\u001b[0m unique_values_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[0;32m      3\u001b[0m     rows,\n\u001b[0;32m      4\u001b[0m     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnique Value\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPercentage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Round the percentage to 2 decimal places.\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m unique_values_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPercentage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43munique_values_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPercentage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mround\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Style the unique values dataFrame.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m unique_values_df \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     12\u001b[0m     unique_values_df\u001b[38;5;241m.\u001b[39mstyle\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;241m.\u001b[39mset_caption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<b>Unique Values in Categorical Features</b>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;241m.\u001b[39mhide(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\aafz1\\miniconda3\\envs\\project-env\\lib\\site-packages\\pandas\\core\\series.py:2602\u001b[0m, in \u001b[0;36mSeries.round\u001b[1;34m(self, decimals, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2570\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2571\u001b[0m \u001b[38;5;124;03mRound each value in a Series to the given number of decimals.\u001b[39;00m\n\u001b[0;32m   2572\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2599\u001b[0m \u001b[38;5;124;03mdtype: float64\u001b[39;00m\n\u001b[0;32m   2600\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2601\u001b[0m nv\u001b[38;5;241m.\u001b[39mvalidate_round(args, kwargs)\n\u001b[1;32m-> 2602\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mround\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecimals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2603\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(result, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   2604\u001b[0m     \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mround\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2605\u001b[0m )\n\u001b[0;32m   2607\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'float'"
     ]
    }
   ],
   "source": [
    "# Create a dataFrame of unique values.\n",
    "unique_values_df = pd.DataFrame(\n",
    "    rows,\n",
    "    columns=[\"Feature\", \"Unique Value\", \"Frequency\", \"Percentage\"]\n",
    ")\n",
    "\n",
    "# Round the percentage to 2 decimal places.\n",
    "unique_values_df[\"Percentage\"] = unique_values_df[\"Percentage\"].round(2)\n",
    "\n",
    "# Style the unique values dataFrame.\n",
    "unique_values_df = (\n",
    "    unique_values_df.style\n",
    "    .set_caption(\"<b>Unique Values in Categorical Features</b>\")\n",
    "    .hide(axis=\"index\")\n",
    ")\n",
    "\n",
    "# Display the unique values in the categorical features.\n",
    "unique_values_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIkLFaaFHHbX"
   },
   "source": [
    "## 3.5. Check the Numeric Features' Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bvXCgre6HHbX"
   },
   "outputs": [],
   "source": [
    "data[numeric_features].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkocy5TPHHbX"
   },
   "source": [
    "## 3.6. Implement IQR-Based Clipping of Numeric Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hB6B96KkHHbX"
   },
   "outputs": [],
   "source": [
    "# Handle outliers in numeric features.\n",
    "for feature in numeric_features:\n",
    "\n",
    "    q1, q3 = data[feature].quantile([0.25, 0.75])\n",
    "    iqr = q3 - q1\n",
    "    lower_bound, upper_bound = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "\n",
    "    data[feature] = data[feature].clip(lower=lower_bound, upper=upper_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NpygiKUHHbX"
   },
   "source": [
    "## 3.6. Check for Missing Values in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGmYfnhJHHbY"
   },
   "outputs": [],
   "source": [
    "# Find out the missing values percentage in the dataset.\n",
    "non_missing_percentage = data.notnull().mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLU8qea3HHbY"
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame from the non-missing percentage series.\n",
    "non_missing_df = pd.DataFrame(\n",
    "    non_missing_percentage,\n",
    "    columns=[\"Non-Missing Percentage\"]\n",
    ")\n",
    "\n",
    "# Change the index to a column named \"Feature\".\n",
    "non_missing_df = non_missing_df.reset_index().rename(\n",
    "    columns={\n",
    "        \"index\": \"Feature\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Increment the DataFrame index to start from 1.\n",
    "non_missing_df.index = non_missing_df.index + 1\n",
    "\n",
    "# Display the non-missing percentage table with two decimal places.\n",
    "non_missing_df.style.set_caption(\n",
    "    \"<b>Non-Missing Percentage of Features</b>\"\n",
    ").format(\n",
    "    {\n",
    "        \"Non-Missing Percentage\": \"{:.2f}\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBSDLhEEHHbe"
   },
   "source": [
    "## 3.7. Fill the Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPvJMXngHHbe"
   },
   "outputs": [],
   "source": [
    "# Fill missing values in the dataset.\n",
    "data[\"Offer\"] = data[\"Offer\"].fillna(\"No Offer\")\n",
    "data[\"Internet Type\"] = data[\"Internet Type\"].fillna(\"No Internet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Oj466CmHHbe"
   },
   "source": [
    "## 3.8. Fix Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fPI5FWvbHHbe"
   },
   "outputs": [],
   "source": [
    "# Pad the shorter list with empty strings.\n",
    "max_length = max(\n",
    "    len(categorical_features),\n",
    "    len(numeric_features)\n",
    ")\n",
    "\n",
    "categorical_features_feature = categorical_features.copy()\n",
    "numeric_features_feature = numeric_features.copy()\n",
    "\n",
    "categorical_features_feature += [\"\"] * (\n",
    "    max_length - len(categorical_features)\n",
    ")\n",
    "numeric_features_feature += [\"\"] * (\n",
    "    max_length - len(numeric_features)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R72bNI83HHbe"
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame to display feature categorization.\n",
    "feature_types_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Categorical Features\": categorical_features_feature,\n",
    "        \"Numeric Features\": numeric_features_feature\n",
    "    }\n",
    ")\n",
    "\n",
    "# Increment the DataFrame index to start from 1.\n",
    "feature_types_df.index = feature_types_df.index + 1\n",
    "\n",
    "# Display the feature categorization table.\n",
    "feature_types_df.style.set_caption(\n",
    "    \"<b>Categorization of Features by Type</b>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SJsqGw7HHbf"
   },
   "source": [
    "## 3.9. Check the Features Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BtadamrmHHbf"
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame to display the data types of categorical features.\n",
    "categorical_features_data_types = pd.DataFrame(\n",
    "    data[categorical_features].dtypes,\n",
    "    columns=[\"Categorical Features' Data Types\"]\n",
    ")\n",
    "\n",
    "# Display the data types table with a caption.\n",
    "categorical_features_data_types.style.set_caption(\n",
    "    \"<b>Categorical Features' Data Types</b>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bmAbqAo4HHbf"
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame to display the data types of numeric features.\n",
    "numeric_features_data_types = pd.DataFrame(\n",
    "    data[numeric_features].dtypes,\n",
    "    columns=[\"Numeric Features' Data Types\"]\n",
    ")\n",
    "\n",
    "# Display the data types table with a caption.\n",
    "numeric_features_data_types.style.set_caption(\n",
    "    \"Numeric Features' Data Types\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TC7P4LpEHHbf"
   },
   "source": [
    "## 3.10. Encode the Binary Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iL0kA9EzHHbf"
   },
   "outputs": [],
   "source": [
    "# Define a function to encode binary features.\n",
    "def encode_binary_features(datasets, features, mapping):\n",
    "    \"\"\"\n",
    "    Applies binary encoding to specified features across multiple datasets.\n",
    "\n",
    "    Args:\n",
    "        datasets (List[pd.DataFrame]): A list of DataFrames to be modified in-place.\n",
    "        features (List[str]): The names of binary categorical features to encode.\n",
    "        mapping (dict): A dictionary mapping string categories to binary values.\n",
    "    \"\"\"\n",
    "    for df in datasets:\n",
    "        for feature in features:\n",
    "            df[feature] = df[feature].astype(str).map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dVyb3UF-HHbf"
   },
   "outputs": [],
   "source": [
    "# Define binary categorical features to be encoded.\n",
    "binary_features = [\n",
    "    \"Gender\",\n",
    "    \"Senior Citizen\",\n",
    "    \"Married\",\n",
    "    \"Phone Service\",\n",
    "    \"Multiple Lines\",\n",
    "    \"Unlimited Data\",\n",
    "    \"Online Security\",\n",
    "    \"Online Backup\",\n",
    "    \"Device Protection Plan\",\n",
    "    \"Premium Tech Support\",\n",
    "    \"Streaming TV\",\n",
    "    \"Streaming Movies\",\n",
    "    \"Streaming Music\",\n",
    "    \"Paperless Billing\",\n",
    "    \"Churn Label\"\n",
    "]\n",
    "\n",
    "# Define mapping for binary categories.\n",
    "binary_mapping = {\n",
    "    \"Yes\": 1,\n",
    "    \"No\": 0,\n",
    "    \"Male\": 1,\n",
    "    \"Female\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHqG0iPFHHbf"
   },
   "outputs": [],
   "source": [
    "# Apply binary encoding to all datasets.\n",
    "encode_binary_features(\n",
    "    datasets=[data],\n",
    "    features=binary_features,\n",
    "    mapping=binary_mapping\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7ZCge1uHHbf"
   },
   "source": [
    "## 3.11. Encode the Ordinal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WOA9F4QFHHbf"
   },
   "outputs": [],
   "source": [
    "# Define a function to encode ordinal features.\n",
    "def encode_ordinal_features(datasets, mappings):\n",
    "    \"\"\"\n",
    "    Applies ordinal encoding to specified features across multiple datasets.\n",
    "\n",
    "    Args:\n",
    "        datasets (List[pd.DataFrame]): A list of DataFrames to be modified in-place.\n",
    "        mappings (dict): A dictionary where keys are feature names and values are mapping dicts.\n",
    "    \"\"\"\n",
    "    for df in datasets:\n",
    "        for feature, mapping in mappings.items():\n",
    "            df[feature] = df[feature].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SsRJfXCHHbf"
   },
   "outputs": [],
   "source": [
    "# Define the mappings for ordinal features.\n",
    "offer_mapping = {\n",
    "    \"No Offer\": 0,\n",
    "    \"Offer A\": 1,\n",
    "    \"Offer B\": 2,\n",
    "    \"Offer C\": 3,\n",
    "    \"Offer D\": 4,\n",
    "    \"Offer E\": 5\n",
    "}\n",
    "\n",
    "internet_type_mapping = {\n",
    "    \"No Internet\": 0,\n",
    "    \"DSL\": 1,\n",
    "    \"Cable\": 2,\n",
    "    \"Fiber Optic\": 3\n",
    "}\n",
    "\n",
    "payment_method_mapping = {\n",
    "    \"Mailed Check\": 1,\n",
    "    \"Bank Withdrawal\": 2,\n",
    "    \"Credit Card\": 3\n",
    "}\n",
    "\n",
    "# Create a dictionary of ordinal mappings.\n",
    "ordinal_mappings = {\n",
    "    \"Offer\": offer_mapping,\n",
    "    \"Internet Type\": internet_type_mapping,\n",
    "    \"Payment Method\": payment_method_mapping\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1VrFHsSHHbf"
   },
   "outputs": [],
   "source": [
    "# Apply ordinal encoding to all datasets.\n",
    "encode_ordinal_features(\n",
    "    datasets=[data],\n",
    "    mappings=ordinal_mappings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JMVMd3t4HHbf"
   },
   "source": [
    "## 3.12. Standard Scale the Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6JE5vbAMHHbg"
   },
   "outputs": [],
   "source": [
    "# Apply the scaler to normalize all numeric features in the dataset.\n",
    "scaler = StandardScaler()\n",
    "\n",
    "data[numeric_features] = scaler.fit_transform(data[numeric_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5jZE5psHHbg"
   },
   "source": [
    "<b><p style=\"font-size: 35px;\">II. Second Phase: Causal Discovery and Inference</p></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5be8wSS-HHbg"
   },
   "source": [
    "# 1. Define Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g8edpdRXHHbg"
   },
   "outputs": [],
   "source": [
    "def create_constraint_matrix(node_names, tiers, specific_constraints=None):\n",
    "    \"\"\"\n",
    "    Create a constraint matrix for causal discovery algorithms.\n",
    "\n",
    "    Args:\n",
    "        node_names (list): List of variable names.\n",
    "        tiers (list): List of tier lists (e.g., [demographic, customer, ...]).\n",
    "        specific_constraints (dict): Additional constraints (e.g., {\"forbidden\": [(src, dst), ...]}).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Constraint matrix (np.nan for allowed edges, 0.0 for forbidden).\n",
    "    \"\"\"\n",
    "    num_nodes = len(node_names)\n",
    "    node_name_to_idx = {name: i for i, name in enumerate(node_names)}\n",
    "    constraint_matrix = np.full((num_nodes, num_nodes), np.nan, dtype=np.float32)\n",
    "\n",
    "    # Set Churn Label as sink node (no outgoing edges)\n",
    "    churn_idx = node_name_to_idx.get(\"Churn Label\")\n",
    "    if churn_idx is not None:\n",
    "        constraint_matrix[churn_idx, :] = 0.0\n",
    "\n",
    "    # Set demographic variables as root nodes (no incoming edges)\n",
    "    for feature in tiers[0]:  # Tier 1: Demographic\n",
    "        if feature in node_name_to_idx:\n",
    "            feature_idx = node_name_to_idx[feature]\n",
    "            constraint_matrix[:, feature_idx] = 0.0\n",
    "\n",
    "    # Prevent edges within Tier 1\n",
    "    for src in tiers[0]:\n",
    "        for dst in tiers[0]:\n",
    "            if src != dst and src in node_name_to_idx and dst in node_name_to_idx:\n",
    "                constraint_matrix[node_name_to_idx[src], node_name_to_idx[dst]] = 0.0\n",
    "\n",
    "    # Allow edges only from Tier N to Tier N+1\n",
    "    for src_tier_idx, src_tier in enumerate(tiers[:-1]):\n",
    "        dst_tier = tiers[src_tier_idx + 1]\n",
    "        for src in src_tier:\n",
    "            for dst in dst_tier:\n",
    "                if src in node_name_to_idx and dst in node_name_to_idx:\n",
    "                    constraint_matrix[node_name_to_idx[src], node_name_to_idx[dst]] = np.nan\n",
    "        # Block edges to other tiers\n",
    "        for other_tier_idx, other_tier in enumerate(tiers):\n",
    "            if other_tier_idx != src_tier_idx + 1:\n",
    "                for src in src_tier:\n",
    "                    for dst in other_tier:\n",
    "                        if src in node_name_to_idx and dst in node_name_to_idx:\n",
    "                            constraint_matrix[node_name_to_idx[src], node_name_to_idx[dst]] = 0.0\n",
    "\n",
    "    # Apply specific constraints (e.g., Gender  Service/Billing forbidden)\n",
    "    if specific_constraints:\n",
    "        for src, dst in specific_constraints.get(\"forbidden\", []):\n",
    "            if src in node_name_to_idx and dst in node_name_to_idx:\n",
    "                constraint_matrix[node_name_to_idx[src], node_name_to_idx[dst]] = 0.0\n",
    "        for src, dst in specific_constraints.get(\"allowed\", []):\n",
    "            if src in node_name_to_idx and dst in node_name_to_idx:\n",
    "                constraint_matrix[node_name_to_idx[src], node_name_to_idx[dst]] = np.nan\n",
    "\n",
    "    logger.info(\"Constraint matrix created with shape: %s\", constraint_matrix.shape)\n",
    "    return constraint_matrix, node_name_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_j9kg9zBHHbg"
   },
   "outputs": [],
   "source": [
    "def validate_constraints(dag, node_name_to_idx, tiers, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Validate constraints on a DAG or adjacency matrix.\n",
    "\n",
    "    Args:\n",
    "        dag: NetworkX DiGraph or np.ndarray (adjacency/probability matrix).\n",
    "        node_name_to_idx (dict): Mapping of node names to indices.\n",
    "        tiers (list): List of tier lists.\n",
    "        threshold (float): Probability threshold for matrix-based DAGs.\n",
    "\n",
    "    Returns:\n",
    "        list: List of constraint violation messages (empty if none).\n",
    "    \"\"\"\n",
    "    violations = []\n",
    "    num_nodes = len(node_name_to_idx)\n",
    "    churn_idx = node_name_to_idx.get(\"Churn Label\")\n",
    "\n",
    "    # Convert adjacency matrix to DAG if needed\n",
    "    if isinstance(dag, np.ndarray):\n",
    "        G = nx.DiGraph()\n",
    "        G.add_nodes_from(range(num_nodes))\n",
    "        for i in range(num_nodes):\n",
    "            for j in range(num_nodes):\n",
    "                if dag[i, j] > threshold:\n",
    "                    G.add_edge(i, j)\n",
    "    else:\n",
    "        G = dag\n",
    "\n",
    "    # Check if Churn Label has outgoing edges\n",
    "    if churn_idx is not None and any(G.has_edge(churn_idx, j) for j in range(num_nodes)):\n",
    "        violations.append(\"Churn Label has outgoing edges\")\n",
    "\n",
    "    # Check if Tier 1 variables have incoming edges\n",
    "    for var in tiers[0]:\n",
    "        var_idx = node_name_to_idx.get(var)\n",
    "        if var_idx is not None and any(G.has_edge(j, var_idx) for j in range(num_nodes)):\n",
    "            violations.append(f\"{var} has incoming edges\")\n",
    "\n",
    "    # Check for edges within Tier 1\n",
    "    for src in tiers[0]:\n",
    "        src_idx = node_name_to_idx.get(src)\n",
    "        for dst in tiers[0]:\n",
    "            dst_idx = node_name_to_idx.get(dst)\n",
    "            if src != dst and src_idx is not None and dst_idx is not None and G.has_edge(src_idx, dst_idx):\n",
    "                violations.append(f\"T1T1 edge: {src}{dst}\")\n",
    "\n",
    "    # Check for forbidden Gender  Service/Billing edges\n",
    "    gender_idx = node_name_to_idx.get(\"Gender\")\n",
    "    if gender_idx is not None:\n",
    "        for dst in tiers[2] + tiers[3]:  # Service + Billing\n",
    "            dst_idx = node_name_to_idx.get(dst)\n",
    "            if dst_idx is not None and G.has_edge(gender_idx, dst_idx):\n",
    "                violations.append(f\"Gender{dst} edge exists\")\n",
    "\n",
    "    if not violations:\n",
    "        logger.info(\" All constraints validated successfully\")\n",
    "    else:\n",
    "        logger.warning(\" Constraint violations detected: %s\", violations)\n",
    "\n",
    "    return violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p_Ht7zb3HHbg"
   },
   "outputs": [],
   "source": [
    "def save_relations_to_text(dag, node_names, filename, threshold=0.2):\n",
    "    \"\"\"\n",
    "    Save causal relationships to a text file.\n",
    "\n",
    "    Args:\n",
    "        dag: NetworkX DiGraph or np.ndarray (adjacency/probability matrix).\n",
    "        node_names (list): List of node names.\n",
    "        filename (str): Output text file name.\n",
    "        threshold (float): Probability/weight threshold for matrix-based DAGs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        relations = []\n",
    "        if isinstance(dag, np.ndarray):\n",
    "            for i in range(dag.shape[0]):\n",
    "                for j in range(dag.shape[1]):\n",
    "                    if dag[i, j] > threshold:\n",
    "                        relations.append({\n",
    "                            \"source\": node_names[i],\n",
    "                            \"destination\": node_names[j],\n",
    "                            \"weight\": float(dag[i, j])\n",
    "                        })\n",
    "        else:\n",
    "            for src, dst in dag.edges():\n",
    "                weight = dag[src][dst].get(\"weight\", 1.0)  # Default to 1.0 for unweighted edges\n",
    "                relations.append({\n",
    "                    \"source\": node_names[src],\n",
    "                    \"destination\": node_names[dst],\n",
    "                    \"weight\": float(weight)\n",
    "                })\n",
    "\n",
    "        # Convert to DataFrame and save as text\n",
    "        relations_df = pd.DataFrame(relations)\n",
    "        if not relations_df.empty:\n",
    "            relations_df.to_csv(filename, sep=\"\\t\", index=False, columns=[\"source\", \"destination\", \"weight\"])\n",
    "            logger.info(\"Causal relationships saved to %s (%d relations)\", filename, len(relations_df))\n",
    "        else:\n",
    "            with open(filename, \"w\") as f:\n",
    "                f.write(\"No causal relationships found.\")\n",
    "            logger.warning(\"No causal relationships to save for %s\", filename)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"Failed to save relations to %s: %s\", filename, str(e))\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H49mDITAHHbg"
   },
   "outputs": [],
   "source": [
    "def visualize_causal_graph(dag, node_names, filename=\"causal_graph.png\"):\n",
    "    \"\"\"\n",
    "    Visualize a causal graph and save it to a file.\n",
    "\n",
    "    Args:\n",
    "        dag: NetworkX DiGraph or np.ndarray (adjacency matrix).\n",
    "        node_names (list): List of node names.\n",
    "        filename (str): Output file name.\n",
    "\n",
    "    Returns:\n",
    "        nx.DiGraph: Visualized graph.\n",
    "    \"\"\"\n",
    "    if isinstance(dag, np.ndarray):\n",
    "        G = nx.DiGraph(dag)\n",
    "    else:\n",
    "        G = dag\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    nx.draw(G, pos, with_labels=True, labels={i: node_names[i] for i in G.nodes()},\n",
    "            node_color='lightblue', node_size=2000, font_size=10, font_weight='bold', arrowsize=20)\n",
    "\n",
    "    if isinstance(dag, np.ndarray):\n",
    "        edge_labels = {(i, j): f\"{dag[i, j]:.3f}\" for i, j in G.edges()}\n",
    "        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
    "\n",
    "    plt.title(f\"Causal Graph ({filename.split('.')[0]})\")\n",
    "    plt.savefig(filename, format=\"png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    logger.info(\"Causal graph saved as %s\", filename)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ynmA2TPHHbh"
   },
   "outputs": [],
   "source": [
    "def analyze_structure_learning(dag, node_names, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Analyze the structure of a learned DAG.\n",
    "\n",
    "    Args:\n",
    "        dag: NetworkX DiGraph or np.ndarray (adjacency/probability matrix).\n",
    "        node_names (list): List of node names.\n",
    "        threshold (float): Probability threshold for matrix-based DAGs.\n",
    "\n",
    "    Returns:\n",
    "        dict: Structure learning metrics.\n",
    "    \"\"\"\n",
    "    if isinstance(dag, np.ndarray):\n",
    "        adj_matrix = (dag > threshold).astype(int)\n",
    "        G = nx.DiGraph(adj_matrix)\n",
    "    else:\n",
    "        G = dag\n",
    "        adj_matrix = nx.to_numpy_array(G, nodelist=range(len(node_names)))\n",
    "\n",
    "    num_edges = np.sum(adj_matrix)\n",
    "    num_nodes = len(node_names)\n",
    "    graph_density = num_edges / (num_nodes * (num_nodes - 1)) if num_nodes > 1 else 0\n",
    "    in_degree = np.sum(adj_matrix, axis=0)\n",
    "    out_degree = np.sum(adj_matrix, axis=1)\n",
    "    avg_in_degree = np.mean(in_degree)\n",
    "    avg_out_degree = np.mean(out_degree)\n",
    "\n",
    "    most_influential = [(node_names[i], out_degree[i]) for i in np.argsort(-out_degree)[:5]]\n",
    "    most_affected = [(node_names[i], in_degree[i]) for i in np.argsort(-in_degree)[:5]]\n",
    "\n",
    "    metrics = {\n",
    "        \"num_edges\": num_edges,\n",
    "        \"graph_density\": graph_density,\n",
    "        \"avg_in_degree\": avg_in_degree,\n",
    "        \"avg_out_degree\": avg_out_degree,\n",
    "        \"most_influential\": most_influential,\n",
    "        \"most_affected\": most_affected\n",
    "    }\n",
    "\n",
    "    logger.info(\"Structure Learning Metrics: %s\", metrics)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57F6to0tHHbh"
   },
   "outputs": [],
   "source": [
    "def evaluate_causal_discovery(results, test_data, tiers, node_name_to_idx, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate causal discovery algorithms on test data, focusing on constraint violations.\n",
    "\n",
    "    Args:\n",
    "        results (dict): Results from run_causal_discovery_pipeline.\n",
    "        test_data (pd.DataFrame): Test data.\n",
    "        tiers (list): List of tier lists.\n",
    "        node_name_to_idx (dict): Mapping of node names to indices.\n",
    "        threshold (float): Probability threshold for matrix-based DAGs.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Evaluation metrics for each algorithm.\n",
    "    \"\"\"\n",
    "    logger.info(\"Evaluating constraint violations on test data...\")\n",
    "    evaluation_metrics = {}\n",
    "\n",
    "    # Preprocess test data to handle issues\n",
    "    constant_cols = [col for col in test_data.columns if test_data[col].std() == 0]\n",
    "    if constant_cols:\n",
    "        logger.warning(\"Constant columns in test data: %s\", constant_cols)\n",
    "        test_data = test_data.drop(columns=constant_cols)\n",
    "    if test_data.isna().any().any():\n",
    "        logger.warning(\"NaNs in test data, filling with mean\")\n",
    "        test_data = test_data.fillna(test_data.mean())\n",
    "\n",
    "    for algo_name, result in results.items():\n",
    "        if \"error\" in result:\n",
    "            evaluation_metrics[algo_name] = {\n",
    "                \"constraint_violations\": \"N/A\",\n",
    "                \"violation_details\": \"N/A\"\n",
    "            }\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            dag = result[\"dag\"]\n",
    "            adj_matrix = result[\"adj_matrix\"]\n",
    "\n",
    "            # Adjust node_name_to_idx for LiNGAM (uses subset of features)\n",
    "            algo_node_name_to_idx = node_name_to_idx\n",
    "            if algo_name == \"LiNGAM\":\n",
    "                continuous_features = [f for f in test_data.columns if f in [\"Age\", \"Number of Dependents\", \"Number of Referrals\", \"Tenure in Months\", \"Total Revenue\"]]\n",
    "                algo_node_name_to_idx = {name: i for i, name in enumerate(continuous_features)}\n",
    "                # Update tiers for LiNGAM\n",
    "                lingam_tiers = [\n",
    "                    [f for f in tiers[0] if f in continuous_features],\n",
    "                    [f for f in tiers[1] if f in continuous_features],\n",
    "                    [f for f in tiers[3] if f in continuous_features]\n",
    "                ]\n",
    "            else:\n",
    "                lingam_tiers = tiers\n",
    "\n",
    "            # Validate constraints\n",
    "            violations = validate_constraints(dag if algo_name != \"LiNGAM\" else adj_matrix,\n",
    "                                           algo_node_name_to_idx, lingam_tiers, threshold)\n",
    "\n",
    "            evaluation_metrics[algo_name] = {\n",
    "                \"constraint_violations\": len(violations),\n",
    "                \"violation_details\": violations if violations else \"None\"\n",
    "            }\n",
    "\n",
    "            logger.info(\"%s: %d constraint violations on test data: %s\",\n",
    "                        algo_name, len(violations), violations if violations else \"None\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"Evaluation failed for %s: %s\", algo_name, str(e))\n",
    "            evaluation_metrics[algo_name] = {\n",
    "                \"constraint_violations\": \"N/A\",\n",
    "                \"violation_details\": \"N/A\"\n",
    "            }\n",
    "\n",
    "    # Create summary DataFrame\n",
    "    summary = pd.DataFrame(evaluation_metrics).T\n",
    "    logger.info(\"Constraint violation summary:\\n%s\", summary)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7v9Z5zxHHbh"
   },
   "source": [
    "# 2. Define the Causal Discovery and Inference Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0Ghg6xRHHbh"
   },
   "source": [
    "# 2.1. DECI Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TCXr3cjeHHbh"
   },
   "outputs": [],
   "source": [
    "def run_deci_algorithm(data, constraint_matrix, node_names, node_name_to_idx, tiers):\n",
    "    logger.info(\"Running DECI algorithm...\")\n",
    "    try:\n",
    "        # Load variables.json\n",
    "        with fsspec.open(\"data/variables.json\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            variables = json.load(f)[\"variables\"]\n",
    "\n",
    "        # Validate columns\n",
    "        expected_columns = [var[\"name\"] for var in variables]\n",
    "        if set(expected_columns) != set(data.columns):\n",
    "            raise ValueError(f\"Columns mismatch: {set(data.columns)} vs {set(expected_columns)}\")\n",
    "\n",
    "        # Prepare data module\n",
    "        data_module = BasicDECIDataModule(\n",
    "            data,\n",
    "            variables=[Variable.from_dict(d) for d in variables],\n",
    "            batch_size=128,\n",
    "            normalize=True\n",
    "        )\n",
    "\n",
    "        # Initialize DECI module\n",
    "        lightning_module = DECIModule(\n",
    "            noise_dist=ContinuousNoiseDist.GAUSSIAN,\n",
    "            prior_sparsity_lambda=200.0,\n",
    "            init_rho=30.0,\n",
    "            init_alpha=0.20,\n",
    "            auglag_config=AugLagLRConfig(\n",
    "                max_inner_steps=1500,\n",
    "                max_outer_steps=8,\n",
    "                lr_init_dict={\n",
    "                    \"icgnn\": 0.00076,\n",
    "                    \"vardist\": 0.0098,\n",
    "                    \"functional_relationships\": 3e-4,\n",
    "                    \"noise_dist\": 0.0070,\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        lightning_module.constraint_matrix = torch.tensor(constraint_matrix)\n",
    "\n",
    "        # Train\n",
    "        trainer = pl.Trainer(\n",
    "            accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "            devices=1,\n",
    "            max_epochs=10,\n",
    "            callbacks=[TQDMProgressBar(refresh_rate=19)],\n",
    "            enable_checkpointing=False\n",
    "        )\n",
    "        trainer.fit(lightning_module, datamodule=data_module)\n",
    "\n",
    "        # Save model\n",
    "        torch.save(lightning_module.sem_module, \"deci.pt\")\n",
    "\n",
    "        # Compute probability matrix\n",
    "        logits_exist = lightning_module.sem_module.adjacency_module.adjacency_distribution.logits_exist\n",
    "        logits_orient = lightning_module.sem_module.adjacency_module.adjacency_distribution.logits_orient\n",
    "\n",
    "        def fill_triangular(vec, upper=False):\n",
    "            n = int(np.sqrt(2 * len(vec))) + 1\n",
    "            if upper:\n",
    "                return vec.new_zeros(n, n).triu(1).masked_scatter_(\n",
    "                    torch.triu(torch.ones(n, n, device=vec.device), 1).bool(), vec\n",
    "                )\n",
    "            return vec.new_zeros(n, n).tril(-1).masked_scatter_(\n",
    "                    torch.tril(torch.ones(n, n, device=vec.device), -1).bool(), vec\n",
    "                )\n",
    "\n",
    "        neg_theta = fill_triangular(logits_orient, upper=True) - fill_triangular(logits_orient, upper=False)\n",
    "        logits_matrix = -torch.logsumexp(torch.stack([-logits_exist, neg_theta, neg_theta - logits_exist], dim=-1), dim=-1)\n",
    "        prob_matrix = 1 / (1 + np.exp(-logits_matrix.cpu().detach().numpy()))\n",
    "        prob_matrix = prob_matrix * np.isnan(constraint_matrix)\n",
    "\n",
    "        # Validate constraints\n",
    "        violations = validate_constraints(prob_matrix, node_name_to_idx, tiers)\n",
    "\n",
    "        # Visualize\n",
    "        G = visualize_causal_graph(prob_matrix, node_names, \"deci_graph.png\")\n",
    "\n",
    "        # Save relations to text\n",
    "        save_relations_to_text(prob_matrix, node_names, \"deci_relations.txt\")\n",
    "\n",
    "        # Analyze structure\n",
    "        metrics = analyze_structure_learning(prob_matrix, node_names)\n",
    "\n",
    "        return {\n",
    "            \"dag\": G,\n",
    "            \"adj_matrix\": prob_matrix,\n",
    "            \"metrics\": metrics,\n",
    "            \"violations\": violations\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"DECI failed: %s\", str(e))\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bX5Ddfg2HHbh"
   },
   "source": [
    "# 2.2. LiNGAM Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qHaCqPEFHHbh"
   },
   "outputs": [],
   "source": [
    "def run_lingam_algorithm(data, constraint_matrix, node_names, node_name_to_idx, tiers):\n",
    "    logger.info(\"Running LiNGAM algorithm...\")\n",
    "    try:\n",
    "        # Filter continuous features\n",
    "        continuous_features = [f for f in node_names if f in [\"Age\", \"Number of Dependents\", \"Number of Referrals\", \"Tenure in Months\", \"Total Revenue\"]]\n",
    "        lingam_data = data[continuous_features].copy()\n",
    "\n",
    "        # Remove constant columns\n",
    "        constant_cols = [col for col in lingam_data.columns if lingam_data[col].std() == 0]\n",
    "        if constant_cols:\n",
    "            logger.warning(\"Removing constant columns: %s\", constant_cols)\n",
    "            lingam_data = lingam_data.drop(columns=constant_cols)\n",
    "            continuous_features = [f for f in continuous_features if f not in constant_cols]\n",
    "\n",
    "        # Validate data\n",
    "        if lingam_data.isna().any().any():\n",
    "            raise ValueError(\"LiNGAM data contains NaNs\")\n",
    "\n",
    "        # Create LiNGAM-specific constraint matrix\n",
    "        lingam_node_to_idx = {name: i for i, name in enumerate(continuous_features)}\n",
    "        lingam_constraint_matrix = np.full((len(continuous_features), len(continuous_features)), -1, dtype=np.int32)\n",
    "\n",
    "        lingam_tiers = [\n",
    "            [f for f in tiers[0] if f in continuous_features],  # Demographic\n",
    "            [f for f in tiers[1] if f in continuous_features],  # Customer\n",
    "            [f for f in tiers[3] if f in continuous_features]   # Billing\n",
    "        ]\n",
    "\n",
    "        if \"Total Revenue\" in lingam_node_to_idx:\n",
    "            lingam_constraint_matrix[lingam_node_to_idx[\"Total Revenue\"], :] = 0\n",
    "        for feature in lingam_tiers[0]:\n",
    "            lingam_constraint_matrix[:, lingam_node_to_idx[feature]] = 0\n",
    "        for src_tier_idx, src_tier in enumerate(lingam_tiers[:-1]):\n",
    "            dst_tier = lingam_tiers[src_tier_idx + 1]\n",
    "            for src in src_tier:\n",
    "                for dst in dst_tier:\n",
    "                    lingam_constraint_matrix[lingam_node_to_idx[src], lingam_node_to_idx[dst]] = -1\n",
    "            for other_tier_idx, other_tier in enumerate(lingam_tiers):\n",
    "                if other_tier_idx != src_tier_idx + 1:\n",
    "                    for src in src_tier:\n",
    "                        for dst in other_tier:\n",
    "                            lingam_constraint_matrix[lingam_node_to_idx[src], lingam_node_to_idx[dst]] = 0\n",
    "        for src in lingam_tiers[0]:\n",
    "            for dst in lingam_tiers[0]:\n",
    "                if src != dst:\n",
    "                    lingam_constraint_matrix[lingam_node_to_idx[src], lingam_node_to_idx[dst]] = 0\n",
    "\n",
    "        # Fit LiNGAM\n",
    "        model = lingam.DirectLiNGAM(prior_knowledge=lingam_constraint_matrix)\n",
    "        model.fit(lingam_data)\n",
    "\n",
    "        adj_matrix = model.adjacency_matrix_\n",
    "\n",
    "        # Validate constraints\n",
    "        violations = validate_constraints(adj_matrix, lingam_node_to_idx, lingam_tiers)\n",
    "\n",
    "        # Visualize\n",
    "        G = visualize_causal_graph(adj_matrix, continuous_features, \"lingam_graph.png\")\n",
    "\n",
    "        # Save relations to text\n",
    "        save_relations_to_text(adj_matrix, continuous_features, \"lingam_relations.txt\")\n",
    "\n",
    "        # Analyze structure\n",
    "        metrics = analyze_structure_learning(adj_matrix, continuous_features)\n",
    "\n",
    "        return {\n",
    "            \"dag\": G,\n",
    "            \"adj_matrix\": adj_matrix,\n",
    "            \"metrics\": metrics,\n",
    "            \"violations\": violations\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"LiNGAM failed: %s\", str(e))\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_H3WEKFCHHbh"
   },
   "source": [
    "# 2.3. PC-GIN Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FN0P9JnsHHbh"
   },
   "outputs": [],
   "source": [
    "def run_pcgins_algorithm(data, constraint_matrix, node_names, node_name_to_idx, tiers):\n",
    "    logger.info(\"Running PC-GIN algorithm...\")\n",
    "    try:\n",
    "        # Encode categorical columns\n",
    "        categorical_cols = [col for col in data.columns if col in [\"Gender\", \"Internet Type\", \"Offer\", \"Payment Method\"]]\n",
    "        encoded_data = data.copy()\n",
    "        for col in categorical_cols:\n",
    "            le = LabelEncoder()\n",
    "            encoded_data[col] = le.fit_transform(encoded_data[col].astype(str))\n",
    "\n",
    "        def gin_test(X, Y, Z=None, alpha=0.01):\n",
    "            n = len(X)\n",
    "            if Z is None or Z.shape[1] == 0:\n",
    "                corr, p_value = stats.pearsonr(X, Y)\n",
    "                return p_value\n",
    "            model_x = LinearRegression().fit(Z, X)\n",
    "            residuals_x = X - model_x.predict(Z)\n",
    "            model_y = LinearRegression().fit(Z, Y)\n",
    "            residuals_y = Y - model_y.predict(Z)\n",
    "            corr, p_value = stats.pearsonr(residuals_x, residuals_y)\n",
    "            return p_value\n",
    "\n",
    "        def pc_gin(data, constraint_matrix, alpha=0.01):\n",
    "            n = data.shape[1]\n",
    "            skeleton = nx.Graph()\n",
    "            skeleton.add_nodes_from(range(n))\n",
    "            separating_sets = {}\n",
    "\n",
    "            for i in range(n):\n",
    "                for j in range(i + 1, n):\n",
    "                    if np.isnan(constraint_matrix[i, j]) or np.isnan(constraint_matrix[j, i]):\n",
    "                        skeleton.add_edge(i, j)\n",
    "\n",
    "            for d in range(n):\n",
    "                edges = list(skeleton.edges())\n",
    "                for i, j in edges:\n",
    "                    if not skeleton.has_edge(i, j):\n",
    "                        continue\n",
    "                    adj_i = set(skeleton.neighbors(i)) - {j}\n",
    "                    if len(adj_i) >= d:\n",
    "                        for subset in itertools.combinations(adj_i, d):\n",
    "                            subset_list = list(subset)\n",
    "                            conditioning_set = data[:, subset_list] if subset_list else None\n",
    "                            p_val = gin_test(\n",
    "                                data[:, i], data[:, j],\n",
    "                                conditioning_set.reshape(data.shape[0], -1) if conditioning_set is not None else None,\n",
    "                                alpha=alpha\n",
    "                            )\n",
    "                            if p_val > alpha:\n",
    "                                skeleton.remove_edge(i, j)\n",
    "                                separating_sets[(i, j)] = subset\n",
    "                                separating_sets[(j, i)] = subset\n",
    "                                break\n",
    "\n",
    "            dag = nx.DiGraph()\n",
    "            dag.add_nodes_from(range(n))\n",
    "            for i, j in skeleton.edges():\n",
    "                if np.isnan(constraint_matrix[i, j]) and not np.isnan(constraint_matrix[j, i]):\n",
    "                    dag.add_edge(i, j)\n",
    "                elif np.isnan(constraint_matrix[j, i]) and not np.isnan(constraint_matrix[i, j]):\n",
    "                    dag.add_edge(j, i)\n",
    "                else:\n",
    "                    dag.add_edge(i, j)\n",
    "                    dag.add_edge(j, i)\n",
    "\n",
    "            for i in range(n):\n",
    "                for j in range(n):\n",
    "                    if i == j or not dag.has_edge(i, j):\n",
    "                        continue\n",
    "                    for k in range(n):\n",
    "                        if k == i or k == j:\n",
    "                            continue\n",
    "                        if dag.has_edge(k, j) and not skeleton.has_edge(i, k):\n",
    "                            if ((i, k) in separating_sets and j not in separating_sets[(i, k)]) or \\\n",
    "                               ((k, i) in separating_sets and j not in separating_sets[(k, i)]):\n",
    "                                if dag.has_edge(j, i):\n",
    "                                    dag.remove_edge(j, i)\n",
    "                                if dag.has_edge(j, k):\n",
    "                                    dag.remove_edge(j, k)\n",
    "\n",
    "            for i, j in list(dag.edges()):\n",
    "                if dag.has_edge(j, i):\n",
    "                    if np.isnan(constraint_matrix[i, j]) and not np.isnan(constraint_matrix[j, i]):\n",
    "                        dag.remove_edge(j, i)\n",
    "                    elif np.isnan(constraint_matrix[j, i]) and not np.isnan(constraint_matrix[i, j]):\n",
    "                        dag.remove_edge(i, j)\n",
    "                    else:\n",
    "                        dag.remove_edge(j, i)\n",
    "\n",
    "            return dag\n",
    "\n",
    "        dag = pc_gin(encoded_data.values, constraint_matrix)\n",
    "\n",
    "        # Validate constraints\n",
    "        violations = validate_constraints(dag, node_name_to_idx, tiers)\n",
    "\n",
    "        # Visualize\n",
    "        G = visualize_causal_graph(dag, node_names, \"pcgin_graph.png\")\n",
    "\n",
    "        # Save relations to text\n",
    "        save_relations_to_text(dag, node_names, \"pcgin_relations.txt\")\n",
    "\n",
    "        # Analyze structure\n",
    "        metrics = analyze_structure_learning(dag, node_names)\n",
    "\n",
    "        return {\n",
    "            \"dag\": G,\n",
    "            \"adj_matrix\": nx.to_numpy_array(dag, nodelist=range(len(node_names))),\n",
    "            \"metrics\": metrics,\n",
    "            \"violations\": violations\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"PC-GIN failed: %s\", str(e))\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSJ4bvi9HHbi"
   },
   "source": [
    "# 2.4. NOTEARS Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hGVXvjyOHHbi"
   },
   "outputs": [],
   "source": [
    "def run_notears_algorithm(data, constraint_matrix, node_names, node_name_to_idx, tiers):\n",
    "    logger.info(\"Running NOTEARS algorithm...\")\n",
    "    try:\n",
    "        X = data.values\n",
    "        stds = np.std(X, axis=0)\n",
    "        means = np.mean(X, axis=0)\n",
    "        X_standardized = np.where(stds != 0, (X - means) / stds, 0)\n",
    "\n",
    "        def notears_with_constraints(X, constraint_matrix, lambda1=0.1, max_iter=200, h_tol=1e-8, rho_max=1e+16, w_threshold=0.1):\n",
    "            n, d = X.shape\n",
    "            mask = 1.0 - np.isnan(constraint_matrix).astype(float)\n",
    "\n",
    "            def _h(w):\n",
    "                W = w.reshape((d, d))\n",
    "                M = np.eye(d) + W * W / d\n",
    "                return np.trace(slin.expm(M)) - d\n",
    "\n",
    "            def _func(w):\n",
    "                W = w.reshape((d, d))\n",
    "                W = W * (1.0 - mask)\n",
    "                R = X - X @ W\n",
    "                loss = 0.5 / n * np.sum(R * R)\n",
    "                l1_penalty = lambda1 * np.sum(np.abs(W))\n",
    "                return loss + l1_penalty\n",
    "\n",
    "            def _grad(w):\n",
    "                W = w.reshape((d, d))\n",
    "                W = W * (1.0 - mask)\n",
    "                R = X - X @ W\n",
    "                G = -1.0 / n * X.T @ R\n",
    "                G_l1 = lambda1 * np.sign(W)\n",
    "                G = (G + G_l1) * (1.0 - mask)\n",
    "                return G.flatten()\n",
    "\n",
    "            def _h_grad(w):\n",
    "                W = w.reshape((d, d))\n",
    "                M = np.eye(d) + W * W / d\n",
    "                E = slin.expm(M)\n",
    "                G = E.T * (2 * W / d)\n",
    "                G = G * (1.0 - mask)\n",
    "                return G.flatten()\n",
    "\n",
    "            w_est = np.zeros(d * d)\n",
    "            rho, alpha, h = 1.0, 0.0, np.inf\n",
    "            for _ in range(max_iter):\n",
    "                w_new = sopt.minimize(\n",
    "                    lambda w: _func(w) + 0.5 * rho * _h(w) ** 2 + alpha * _h(w),\n",
    "                    w_est,\n",
    "                    method='L-BFGS-B',\n",
    "                    jac=lambda w: _grad(w) + rho * _h(w) * _h_grad(w) + alpha * _h_grad(w),\n",
    "                    options={'ftol': 1e-6, 'gtol': 1e-6}\n",
    "                ).x\n",
    "                h_new = _h(w_new)\n",
    "                if abs(h_new) <= h_tol or rho >= rho_max:\n",
    "                    break\n",
    "                if abs(h_new) > 0.25 * abs(h):\n",
    "                    rho *= 10\n",
    "                alpha += rho * h_new\n",
    "                w_est, h = w_new, h_new\n",
    "\n",
    "            W_est = w_est.reshape((d, d))\n",
    "            W_est = W_est * (1.0 - mask)\n",
    "            W_est[np.abs(W_est) < w_threshold] = 0\n",
    "\n",
    "            G = nx.DiGraph(W_est)\n",
    "            while not nx.is_directed_acyclic_graph(G):\n",
    "                try:\n",
    "                    cycle = nx.find_cycle(G)\n",
    "                    min_weight = float('inf')\n",
    "                    min_edge = None\n",
    "                    for u, v in cycle:\n",
    "                        if abs(W_est[u, v]) < min_weight:\n",
    "                            min_weight = abs(W_est[u, v])\n",
    "                            min_edge = (u, v)\n",
    "                    if min_edge:\n",
    "                        G.remove_edge(*min_edge)\n",
    "                        W_est[min_edge[0], min_edge[1]] = 0\n",
    "                except nx.NetworkXNoCycle:\n",
    "                    break\n",
    "\n",
    "            return W_est\n",
    "\n",
    "        adj_matrix = notears_with_constraints(X_standardized, constraint_matrix)\n",
    "\n",
    "        # Validate constraints\n",
    "        violations = validate_constraints(adj_matrix, node_name_to_idx, tiers)\n",
    "\n",
    "        # Visualize\n",
    "        G = visualize_causal_graph(adj_matrix, node_names, \"notears_graph.png\")\n",
    "\n",
    "        # Save relations to text\n",
    "        save_relations_to_text(adj_matrix, node_names, \"notears_relations.txt\")\n",
    "\n",
    "        # Analyze structure\n",
    "        metrics = analyze_structure_learning(adj_matrix, node_names)\n",
    "\n",
    "        return {\n",
    "            \"dag\": G,\n",
    "            \"adj_matrix\": adj_matrix,\n",
    "            \"metrics\": metrics,\n",
    "            \"violations\": violations\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"NOTEARS failed: %s\", str(e))\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFgcACXZHHbi"
   },
   "source": [
    "# 2.5. GRaSP Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MsQzIkVYHHbi"
   },
   "outputs": [],
   "source": [
    "def run_grasp_algorithm(data, constraint_matrix, node_names, node_name_to_idx, tiers):\n",
    "    logger.info(\"Running GRaSP algorithm...\")\n",
    "    try:\n",
    "        X = data.values\n",
    "        stds = np.std(X, axis=0)\n",
    "        means = np.mean(X, axis=0)\n",
    "        X_standardized = np.where(stds != 0, (X - means) / stds, 0)\n",
    "\n",
    "        def grasp_with_constraints(X, constraint_matrix, lambda1=0.1, max_iter=200, h_tol=1e-8, rho_max=1e+16, w_threshold=0.1):\n",
    "            n, d = X.shape\n",
    "            mask = 1.0 - np.isnan(constraint_matrix).astype(float)\n",
    "\n",
    "            def _h(w):\n",
    "                W = w.reshape((d, d))\n",
    "                M = np.eye(d) + W * W / d\n",
    "                return np.trace(slin.expm(M)) - d\n",
    "\n",
    "            def _func(w, rho, alpha):\n",
    "                W = w.reshape((d, d))\n",
    "                W = W * (1.0 - mask)\n",
    "                R = X - X @ W\n",
    "                loss = 0.5 / n * np.sum(R * R)\n",
    "                l1_penalty = lambda1 * np.sum(np.abs(W))\n",
    "                h_val = _h(w)\n",
    "                return loss + l1_penalty + 0.5 * rho * h_val ** 2 + alpha * h_val\n",
    "\n",
    "            def _grad(w, rho, alpha):\n",
    "                W = w.reshape((d, d))\n",
    "                W = W * (1.0 - mask)\n",
    "                R = X - X @ W\n",
    "                G_loss = -1.0 / n * X.T @ R\n",
    "                G_l1 = lambda1 * np.sign(W)\n",
    "                h_val = _h(w)\n",
    "                h_gradient = _h_grad(w).reshape((d, d))\n",
    "                G_acyclicity = (rho * h_val + alpha) * h_gradient\n",
    "                G = (G_loss + G_l1 + G_acyclicity) * (1.0 - mask)\n",
    "                return G.flatten()\n",
    "\n",
    "            def _h_grad(w):\n",
    "                W = w.reshape((d, d))\n",
    "                M = np.eye(d) + W * W / d\n",
    "                E = slin.expm(M)\n",
    "                G = E.T * (2 * W / d)\n",
    "                G = G * (1.0 - mask)\n",
    "                return G.flatten()\n",
    "\n",
    "            w_est = np.zeros(d * d)\n",
    "            rho, alpha, h = 1.0, 0.0, np.inf\n",
    "            for _ in range(max_iter):\n",
    "                w_new = sopt.minimize(\n",
    "                    lambda w: _func(w, rho, alpha),\n",
    "                    w_est,\n",
    "                    method='L-BFGS-B',\n",
    "                    jac=lambda w: _grad(w, rho, alpha),\n",
    "                    options={'ftol': 1e-6, 'gtol': 1e-6}\n",
    "                ).x\n",
    "                h_new = _h(w_new)\n",
    "                if abs(h_new) <= h_tol or rho >= rho_max:\n",
    "                    break\n",
    "                if abs(h_new) > 0.25 * abs(h):\n",
    "                    rho *= 10\n",
    "                alpha += rho * h_new\n",
    "                w_est, h = w_new, h_new\n",
    "\n",
    "            W_est = w_est.reshape((d, d))\n",
    "            W_est = W_est * (1.0 - mask)\n",
    "            W_est[np.abs(W_est) < w_threshold] = 0\n",
    "\n",
    "            G = nx.DiGraph(W_est)\n",
    "            while not nx.is_directed_acyclic_graph(G):\n",
    "                try:\n",
    "                    cycle = nx.find_cycle(G)\n",
    "                    min_weight = float('inf')\n",
    "                    min_edge = None\n",
    "                    for u, v in cycle:\n",
    "                        if abs(W_est[u, v]) < min_weight:\n",
    "                            min_weight = abs(W_est[u, v])\n",
    "                            min_edge = (u, v)\n",
    "                    if min_edge:\n",
    "                        G.remove_edge(*min_edge)\n",
    "                        W_est[min_edge[0], min_edge[1]] = 0\n",
    "                except nx.NetworkXNoCycle:\n",
    "                    break\n",
    "\n",
    "            return W_est\n",
    "\n",
    "        adj_matrix = grasp_with_constraints(X_standardized, constraint_matrix)\n",
    "\n",
    "        # Validate constraints\n",
    "        violations = validate_constraints(adj_matrix, node_name_to_idx, tiers)\n",
    "\n",
    "        # Visualize\n",
    "        G = visualize_causal_graph(adj_matrix, node_names, \"grasp_graph.png\")\n",
    "\n",
    "        # Save relations to text\n",
    "        save_relations_to_text(adj_matrix, node_names, \"grasp_relations.txt\")\n",
    "\n",
    "        # Analyze structure\n",
    "        metrics = analyze_structure_learning(adj_matrix, node_names)\n",
    "\n",
    "        return {\n",
    "            \"dag\": G,\n",
    "            \"adj_matrix\": adj_matrix,\n",
    "            \"metrics\": metrics,\n",
    "            \"violations\": violations\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"GRaSP failed: %s\", str(e))\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytR2PRKTHHbi"
   },
   "source": [
    "# 3. Define the Causal Discovery Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNYptQh2HHbi"
   },
   "outputs": [],
   "source": [
    "def run_causal_discovery_pipeline(train_data, tiers, specific_constraints=None):\n",
    "    \"\"\"\n",
    "    Run the causal discovery pipeline for all algorithms.\n",
    "\n",
    "    Args:\n",
    "        train_data (pd.DataFrame): Training data.\n",
    "        tiers (list): List of tier lists (e.g., [demographic, customer, billing]).\n",
    "        specific_constraints (dict): Additional constraints (e.g., {\"forbidden\": [(src, dst), ...]}).\n",
    "\n",
    "    Returns:\n",
    "        dict: Results for each algorithm.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting causal discovery pipeline...\")\n",
    "\n",
    "    # Validate input data\n",
    "    if train_data.empty or train_data.isna().all().all():\n",
    "        logger.error(\"Training data is empty or contains only NaNs\")\n",
    "        raise ValueError(\"Invalid training data\")\n",
    "\n",
    "    # Create constraint matrix\n",
    "    node_names = list(train_data.columns)\n",
    "    constraint_matrix, node_name_to_idx = create_constraint_matrix(node_names, tiers, specific_constraints)\n",
    "\n",
    "    # Define algorithm functions\n",
    "    algorithms = {\n",
    "        \"DECI\": run_deci_algorithm,\n",
    "        \"LiNGAM\": run_lingam_algorithm,\n",
    "        \"PC-GIN\": run_pcgins_algorithm,\n",
    "        \"NOTEARS\": run_notears_algorithm,\n",
    "        \"GRaSP\": run_grasp_algorithm\n",
    "    }\n",
    "\n",
    "    # Run algorithms\n",
    "    results = {}\n",
    "    for algo_name, algo_func in algorithms.items():\n",
    "        try:\n",
    "            logger.info(\"Executing %s...\", algo_name)\n",
    "            # Call the algorithm function with the required arguments\n",
    "            result = algo_func(train_data, constraint_matrix, node_names, node_name_to_idx, tiers)\n",
    "            results[algo_name] = result\n",
    "        except Exception as e:\n",
    "            logger.error(\"%s failed: %s\", algo_name, str(e))\n",
    "            results[algo_name] = {\"error\": str(e)}\n",
    "\n",
    "    # Summarize results\n",
    "    summary = pd.DataFrame({\n",
    "        algo_name: {\n",
    "            \"num_edges\": result[\"metrics\"][\"num_edges\"] if \"metrics\" in result else \"N/A\",\n",
    "            \"graph_density\": result[\"metrics\"][\"graph_density\"] if \"metrics\" in result else \"N/A\",\n",
    "            \"violations\": len(result[\"violations\"]) if \"violations\" in result else \"N/A\"\n",
    "        } for algo_name, result in results.items()\n",
    "    }).T\n",
    "    logger.info(\"Summary of results:\\n%s\", summary)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ndl2vFFfHHbi"
   },
   "source": [
    "# 4. Split Data into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5hQMSxBHHbi"
   },
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pspBm_tHHHbi"
   },
   "source": [
    "# 5. Discover the Causality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IjLXcRbqHHbj"
   },
   "outputs": [],
   "source": [
    "# Define tiers and specific constraints\n",
    "tiers = [\n",
    "    [\"Gender\", \"Age\", \"Senior Citizen\", \"Married\", \"Number of Dependents\"],  # Tier 1: Demographic\n",
    "    [\"Number of Referrals\", \"Tenure in Months\", \"Offer\"],                    # Tier 2: Customer\n",
    "    [\"Phone Service\", \"Multiple Lines\", \"Internet Type\", \"Unlimited Data\",\n",
    "     \"Online Security\", \"Online Backup\", \"Device Protection Plan\",\n",
    "     \"Premium Tech Support\", \"Streaming TV\", \"Streaming Movies\", \"Streaming Music\"],  # Tier 3: Service\n",
    "    [\"Total Revenue\", \"Paperless Billing\", \"Payment Method\"],                # Tier 4: Billing\n",
    "    [\"Churn Label\"]                                                         # Tier 5: Outcome\n",
    "]\n",
    "\n",
    "specific_constraints = {\n",
    "    \"forbidden\": [\n",
    "        (\"Gender\", dst) for dst in tiers[2] + tiers[3]\n",
    "    ] + [\n",
    "        (\"Internet Type\", dst) for dst in [\"Unlimited Data\", \"Online Security\", \"Online Backup\",\n",
    "                                          \"Device Protection Plan\", \"Premium Tech Support\",\n",
    "                                          \"Streaming TV\", \"Streaming Movies\", \"Streaming Music\"]\n",
    "    ],\n",
    "    \"allowed\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "teQbf1VgHHbj"
   },
   "outputs": [],
   "source": [
    "# Run the pipeline (assuming train_data is available from preprocessing)\n",
    "results = run_causal_discovery_pipeline(train_data, tiers, specific_constraints)\n",
    "\n",
    "\n",
    "# Evaluate constraint violations on test data\n",
    "node_names = list(train_data.columns)\n",
    "_, node_name_to_idx = create_constraint_matrix(node_names, tiers, specific_constraints)\n",
    "evaluation_summary = evaluate_causal_discovery(results, test_data, tiers, node_name_to_idx)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "project-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
