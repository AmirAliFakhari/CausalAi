{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3fd2f50",
   "metadata": {},
   "source": [
    "<b><p style=\"font-size: 40px;\">Causal Discovery and Inference in Customer Churn</p></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c0ac73",
   "metadata": {},
   "source": [
    "<b><p style=\"font-size: 35px;\">I. First Phase: Prepare the Data</p></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fd7956",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Import the Relevant Packages and Configuire Them if Needed\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797fd004",
   "metadata": {},
   "source": [
    "## 1.1. Import the Libraries and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "24356143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from abc import ABC, abstractmethod\n",
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Third-party imports\n",
    "import fsspec\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "import pytorch_lightning as pl\n",
    "from scipy import stats\n",
    "import scipy.linalg as slin\n",
    "import scipy.optimize as sopt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from causica.datasets.causica_dataset_format import Variable\n",
    "from causica.distributions import ContinuousNoiseDist\n",
    "from causica.lightning.data_modules.basic_data_module import BasicDECIDataModule\n",
    "from causica.lightning.modules.deci_module import DECIModule\n",
    "from causica.training.auglag import AugLagLRConfig\n",
    "import lingam\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5033648",
   "metadata": {},
   "source": [
    "## 1.2. Configure the Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5fa4ca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3, suppress=True)\n",
    "np.random.seed(100)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.precision\", 2)\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91dfe3d",
   "metadata": {},
   "source": [
    "## 1.3. Setup the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "36114dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run = bool(os.environ.get(\"TEST_RUN\", False))\n",
    "DATA_PATH = \"data/dataset.csv\"\n",
    "VARIABLES_PATH = \"data/variables.json\"\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9d36c7",
   "metadata": {},
   "source": [
    "## 1.4 Data Loading and Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0793c03d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_55ff9\">\n",
       "  <caption><b>IBM Telco Customer Churn Dataset (First 5 Rows)</b></caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_55ff9_level0_col0\" class=\"col_heading level0 col0\" >Customer ID</th>\n",
       "      <th id=\"T_55ff9_level0_col1\" class=\"col_heading level0 col1\" >Gender</th>\n",
       "      <th id=\"T_55ff9_level0_col2\" class=\"col_heading level0 col2\" >Age</th>\n",
       "      <th id=\"T_55ff9_level0_col3\" class=\"col_heading level0 col3\" >Under 30</th>\n",
       "      <th id=\"T_55ff9_level0_col4\" class=\"col_heading level0 col4\" >Senior Citizen</th>\n",
       "      <th id=\"T_55ff9_level0_col5\" class=\"col_heading level0 col5\" >Married</th>\n",
       "      <th id=\"T_55ff9_level0_col6\" class=\"col_heading level0 col6\" >Dependents</th>\n",
       "      <th id=\"T_55ff9_level0_col7\" class=\"col_heading level0 col7\" >Number of Dependents</th>\n",
       "      <th id=\"T_55ff9_level0_col8\" class=\"col_heading level0 col8\" >Country</th>\n",
       "      <th id=\"T_55ff9_level0_col9\" class=\"col_heading level0 col9\" >State</th>\n",
       "      <th id=\"T_55ff9_level0_col10\" class=\"col_heading level0 col10\" >City</th>\n",
       "      <th id=\"T_55ff9_level0_col11\" class=\"col_heading level0 col11\" >Zip Code</th>\n",
       "      <th id=\"T_55ff9_level0_col12\" class=\"col_heading level0 col12\" >Latitude</th>\n",
       "      <th id=\"T_55ff9_level0_col13\" class=\"col_heading level0 col13\" >Longitude</th>\n",
       "      <th id=\"T_55ff9_level0_col14\" class=\"col_heading level0 col14\" >Population</th>\n",
       "      <th id=\"T_55ff9_level0_col15\" class=\"col_heading level0 col15\" >Quarter</th>\n",
       "      <th id=\"T_55ff9_level0_col16\" class=\"col_heading level0 col16\" >Referred a Friend</th>\n",
       "      <th id=\"T_55ff9_level0_col17\" class=\"col_heading level0 col17\" >Number of Referrals</th>\n",
       "      <th id=\"T_55ff9_level0_col18\" class=\"col_heading level0 col18\" >Tenure in Months</th>\n",
       "      <th id=\"T_55ff9_level0_col19\" class=\"col_heading level0 col19\" >Offer</th>\n",
       "      <th id=\"T_55ff9_level0_col20\" class=\"col_heading level0 col20\" >Phone Service</th>\n",
       "      <th id=\"T_55ff9_level0_col21\" class=\"col_heading level0 col21\" >Avg Monthly Long Distance Charges</th>\n",
       "      <th id=\"T_55ff9_level0_col22\" class=\"col_heading level0 col22\" >Multiple Lines</th>\n",
       "      <th id=\"T_55ff9_level0_col23\" class=\"col_heading level0 col23\" >Internet Service</th>\n",
       "      <th id=\"T_55ff9_level0_col24\" class=\"col_heading level0 col24\" >Internet Type</th>\n",
       "      <th id=\"T_55ff9_level0_col25\" class=\"col_heading level0 col25\" >Avg Monthly GB Download</th>\n",
       "      <th id=\"T_55ff9_level0_col26\" class=\"col_heading level0 col26\" >Online Security</th>\n",
       "      <th id=\"T_55ff9_level0_col27\" class=\"col_heading level0 col27\" >Online Backup</th>\n",
       "      <th id=\"T_55ff9_level0_col28\" class=\"col_heading level0 col28\" >Device Protection Plan</th>\n",
       "      <th id=\"T_55ff9_level0_col29\" class=\"col_heading level0 col29\" >Premium Tech Support</th>\n",
       "      <th id=\"T_55ff9_level0_col30\" class=\"col_heading level0 col30\" >Streaming TV</th>\n",
       "      <th id=\"T_55ff9_level0_col31\" class=\"col_heading level0 col31\" >Streaming Movies</th>\n",
       "      <th id=\"T_55ff9_level0_col32\" class=\"col_heading level0 col32\" >Streaming Music</th>\n",
       "      <th id=\"T_55ff9_level0_col33\" class=\"col_heading level0 col33\" >Unlimited Data</th>\n",
       "      <th id=\"T_55ff9_level0_col34\" class=\"col_heading level0 col34\" >Contract</th>\n",
       "      <th id=\"T_55ff9_level0_col35\" class=\"col_heading level0 col35\" >Paperless Billing</th>\n",
       "      <th id=\"T_55ff9_level0_col36\" class=\"col_heading level0 col36\" >Payment Method</th>\n",
       "      <th id=\"T_55ff9_level0_col37\" class=\"col_heading level0 col37\" >Monthly Charge</th>\n",
       "      <th id=\"T_55ff9_level0_col38\" class=\"col_heading level0 col38\" >Total Charges</th>\n",
       "      <th id=\"T_55ff9_level0_col39\" class=\"col_heading level0 col39\" >Total Refunds</th>\n",
       "      <th id=\"T_55ff9_level0_col40\" class=\"col_heading level0 col40\" >Total Extra Data Charges</th>\n",
       "      <th id=\"T_55ff9_level0_col41\" class=\"col_heading level0 col41\" >Total Long Distance Charges</th>\n",
       "      <th id=\"T_55ff9_level0_col42\" class=\"col_heading level0 col42\" >Total Revenue</th>\n",
       "      <th id=\"T_55ff9_level0_col43\" class=\"col_heading level0 col43\" >Satisfaction Score</th>\n",
       "      <th id=\"T_55ff9_level0_col44\" class=\"col_heading level0 col44\" >Customer Status</th>\n",
       "      <th id=\"T_55ff9_level0_col45\" class=\"col_heading level0 col45\" >Churn Label</th>\n",
       "      <th id=\"T_55ff9_level0_col46\" class=\"col_heading level0 col46\" >Churn Score</th>\n",
       "      <th id=\"T_55ff9_level0_col47\" class=\"col_heading level0 col47\" >CLTV</th>\n",
       "      <th id=\"T_55ff9_level0_col48\" class=\"col_heading level0 col48\" >Churn Category</th>\n",
       "      <th id=\"T_55ff9_level0_col49\" class=\"col_heading level0 col49\" >Churn Reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_55ff9_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_55ff9_row0_col0\" class=\"data row0 col0\" >8779-QRDMV</td>\n",
       "      <td id=\"T_55ff9_row0_col1\" class=\"data row0 col1\" >Male</td>\n",
       "      <td id=\"T_55ff9_row0_col2\" class=\"data row0 col2\" >78</td>\n",
       "      <td id=\"T_55ff9_row0_col3\" class=\"data row0 col3\" >No</td>\n",
       "      <td id=\"T_55ff9_row0_col4\" class=\"data row0 col4\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row0_col5\" class=\"data row0 col5\" >No</td>\n",
       "      <td id=\"T_55ff9_row0_col6\" class=\"data row0 col6\" >No</td>\n",
       "      <td id=\"T_55ff9_row0_col7\" class=\"data row0 col7\" >0</td>\n",
       "      <td id=\"T_55ff9_row0_col8\" class=\"data row0 col8\" >United States</td>\n",
       "      <td id=\"T_55ff9_row0_col9\" class=\"data row0 col9\" >California</td>\n",
       "      <td id=\"T_55ff9_row0_col10\" class=\"data row0 col10\" >Los Angeles</td>\n",
       "      <td id=\"T_55ff9_row0_col11\" class=\"data row0 col11\" >90022</td>\n",
       "      <td id=\"T_55ff9_row0_col12\" class=\"data row0 col12\" >34.023810</td>\n",
       "      <td id=\"T_55ff9_row0_col13\" class=\"data row0 col13\" >-118.156582</td>\n",
       "      <td id=\"T_55ff9_row0_col14\" class=\"data row0 col14\" >68701</td>\n",
       "      <td id=\"T_55ff9_row0_col15\" class=\"data row0 col15\" >Q3</td>\n",
       "      <td id=\"T_55ff9_row0_col16\" class=\"data row0 col16\" >No</td>\n",
       "      <td id=\"T_55ff9_row0_col17\" class=\"data row0 col17\" >0</td>\n",
       "      <td id=\"T_55ff9_row0_col18\" class=\"data row0 col18\" >1</td>\n",
       "      <td id=\"T_55ff9_row0_col19\" class=\"data row0 col19\" >None</td>\n",
       "      <td id=\"T_55ff9_row0_col20\" class=\"data row0 col20\" >No</td>\n",
       "      <td id=\"T_55ff9_row0_col21\" class=\"data row0 col21\" >0.000000</td>\n",
       "      <td id=\"T_55ff9_row0_col22\" class=\"data row0 col22\" >No</td>\n",
       "      <td id=\"T_55ff9_row0_col23\" class=\"data row0 col23\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row0_col24\" class=\"data row0 col24\" >DSL</td>\n",
       "      <td id=\"T_55ff9_row0_col25\" class=\"data row0 col25\" >8</td>\n",
       "      <td id=\"T_55ff9_row0_col26\" class=\"data row0 col26\" >No</td>\n",
       "      <td id=\"T_55ff9_row0_col27\" class=\"data row0 col27\" >No</td>\n",
       "      <td id=\"T_55ff9_row0_col28\" class=\"data row0 col28\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row0_col29\" class=\"data row0 col29\" >No</td>\n",
       "      <td id=\"T_55ff9_row0_col30\" class=\"data row0 col30\" >No</td>\n",
       "      <td id=\"T_55ff9_row0_col31\" class=\"data row0 col31\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row0_col32\" class=\"data row0 col32\" >No</td>\n",
       "      <td id=\"T_55ff9_row0_col33\" class=\"data row0 col33\" >No</td>\n",
       "      <td id=\"T_55ff9_row0_col34\" class=\"data row0 col34\" >Month-to-Month</td>\n",
       "      <td id=\"T_55ff9_row0_col35\" class=\"data row0 col35\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row0_col36\" class=\"data row0 col36\" >Bank Withdrawal</td>\n",
       "      <td id=\"T_55ff9_row0_col37\" class=\"data row0 col37\" >39.650000</td>\n",
       "      <td id=\"T_55ff9_row0_col38\" class=\"data row0 col38\" >39.650000</td>\n",
       "      <td id=\"T_55ff9_row0_col39\" class=\"data row0 col39\" >0.000000</td>\n",
       "      <td id=\"T_55ff9_row0_col40\" class=\"data row0 col40\" >20</td>\n",
       "      <td id=\"T_55ff9_row0_col41\" class=\"data row0 col41\" >0.000000</td>\n",
       "      <td id=\"T_55ff9_row0_col42\" class=\"data row0 col42\" >59.650000</td>\n",
       "      <td id=\"T_55ff9_row0_col43\" class=\"data row0 col43\" >3</td>\n",
       "      <td id=\"T_55ff9_row0_col44\" class=\"data row0 col44\" >Churned</td>\n",
       "      <td id=\"T_55ff9_row0_col45\" class=\"data row0 col45\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row0_col46\" class=\"data row0 col46\" >91</td>\n",
       "      <td id=\"T_55ff9_row0_col47\" class=\"data row0 col47\" >5433</td>\n",
       "      <td id=\"T_55ff9_row0_col48\" class=\"data row0 col48\" >Competitor</td>\n",
       "      <td id=\"T_55ff9_row0_col49\" class=\"data row0 col49\" >Competitor offered more data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_55ff9_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_55ff9_row1_col0\" class=\"data row1 col0\" >7495-OOKFY</td>\n",
       "      <td id=\"T_55ff9_row1_col1\" class=\"data row1 col1\" >Female</td>\n",
       "      <td id=\"T_55ff9_row1_col2\" class=\"data row1 col2\" >74</td>\n",
       "      <td id=\"T_55ff9_row1_col3\" class=\"data row1 col3\" >No</td>\n",
       "      <td id=\"T_55ff9_row1_col4\" class=\"data row1 col4\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row1_col5\" class=\"data row1 col5\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row1_col6\" class=\"data row1 col6\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row1_col7\" class=\"data row1 col7\" >1</td>\n",
       "      <td id=\"T_55ff9_row1_col8\" class=\"data row1 col8\" >United States</td>\n",
       "      <td id=\"T_55ff9_row1_col9\" class=\"data row1 col9\" >California</td>\n",
       "      <td id=\"T_55ff9_row1_col10\" class=\"data row1 col10\" >Los Angeles</td>\n",
       "      <td id=\"T_55ff9_row1_col11\" class=\"data row1 col11\" >90063</td>\n",
       "      <td id=\"T_55ff9_row1_col12\" class=\"data row1 col12\" >34.044271</td>\n",
       "      <td id=\"T_55ff9_row1_col13\" class=\"data row1 col13\" >-118.185237</td>\n",
       "      <td id=\"T_55ff9_row1_col14\" class=\"data row1 col14\" >55668</td>\n",
       "      <td id=\"T_55ff9_row1_col15\" class=\"data row1 col15\" >Q3</td>\n",
       "      <td id=\"T_55ff9_row1_col16\" class=\"data row1 col16\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row1_col17\" class=\"data row1 col17\" >1</td>\n",
       "      <td id=\"T_55ff9_row1_col18\" class=\"data row1 col18\" >8</td>\n",
       "      <td id=\"T_55ff9_row1_col19\" class=\"data row1 col19\" >Offer E</td>\n",
       "      <td id=\"T_55ff9_row1_col20\" class=\"data row1 col20\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row1_col21\" class=\"data row1 col21\" >48.850000</td>\n",
       "      <td id=\"T_55ff9_row1_col22\" class=\"data row1 col22\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row1_col23\" class=\"data row1 col23\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row1_col24\" class=\"data row1 col24\" >Fiber Optic</td>\n",
       "      <td id=\"T_55ff9_row1_col25\" class=\"data row1 col25\" >17</td>\n",
       "      <td id=\"T_55ff9_row1_col26\" class=\"data row1 col26\" >No</td>\n",
       "      <td id=\"T_55ff9_row1_col27\" class=\"data row1 col27\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row1_col28\" class=\"data row1 col28\" >No</td>\n",
       "      <td id=\"T_55ff9_row1_col29\" class=\"data row1 col29\" >No</td>\n",
       "      <td id=\"T_55ff9_row1_col30\" class=\"data row1 col30\" >No</td>\n",
       "      <td id=\"T_55ff9_row1_col31\" class=\"data row1 col31\" >No</td>\n",
       "      <td id=\"T_55ff9_row1_col32\" class=\"data row1 col32\" >No</td>\n",
       "      <td id=\"T_55ff9_row1_col33\" class=\"data row1 col33\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row1_col34\" class=\"data row1 col34\" >Month-to-Month</td>\n",
       "      <td id=\"T_55ff9_row1_col35\" class=\"data row1 col35\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row1_col36\" class=\"data row1 col36\" >Credit Card</td>\n",
       "      <td id=\"T_55ff9_row1_col37\" class=\"data row1 col37\" >80.650000</td>\n",
       "      <td id=\"T_55ff9_row1_col38\" class=\"data row1 col38\" >633.300000</td>\n",
       "      <td id=\"T_55ff9_row1_col39\" class=\"data row1 col39\" >0.000000</td>\n",
       "      <td id=\"T_55ff9_row1_col40\" class=\"data row1 col40\" >0</td>\n",
       "      <td id=\"T_55ff9_row1_col41\" class=\"data row1 col41\" >390.800000</td>\n",
       "      <td id=\"T_55ff9_row1_col42\" class=\"data row1 col42\" >1024.100000</td>\n",
       "      <td id=\"T_55ff9_row1_col43\" class=\"data row1 col43\" >3</td>\n",
       "      <td id=\"T_55ff9_row1_col44\" class=\"data row1 col44\" >Churned</td>\n",
       "      <td id=\"T_55ff9_row1_col45\" class=\"data row1 col45\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row1_col46\" class=\"data row1 col46\" >69</td>\n",
       "      <td id=\"T_55ff9_row1_col47\" class=\"data row1 col47\" >5302</td>\n",
       "      <td id=\"T_55ff9_row1_col48\" class=\"data row1 col48\" >Competitor</td>\n",
       "      <td id=\"T_55ff9_row1_col49\" class=\"data row1 col49\" >Competitor made better offer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_55ff9_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_55ff9_row2_col0\" class=\"data row2 col0\" >1658-BYGOY</td>\n",
       "      <td id=\"T_55ff9_row2_col1\" class=\"data row2 col1\" >Male</td>\n",
       "      <td id=\"T_55ff9_row2_col2\" class=\"data row2 col2\" >71</td>\n",
       "      <td id=\"T_55ff9_row2_col3\" class=\"data row2 col3\" >No</td>\n",
       "      <td id=\"T_55ff9_row2_col4\" class=\"data row2 col4\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row2_col5\" class=\"data row2 col5\" >No</td>\n",
       "      <td id=\"T_55ff9_row2_col6\" class=\"data row2 col6\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row2_col7\" class=\"data row2 col7\" >3</td>\n",
       "      <td id=\"T_55ff9_row2_col8\" class=\"data row2 col8\" >United States</td>\n",
       "      <td id=\"T_55ff9_row2_col9\" class=\"data row2 col9\" >California</td>\n",
       "      <td id=\"T_55ff9_row2_col10\" class=\"data row2 col10\" >Los Angeles</td>\n",
       "      <td id=\"T_55ff9_row2_col11\" class=\"data row2 col11\" >90065</td>\n",
       "      <td id=\"T_55ff9_row2_col12\" class=\"data row2 col12\" >34.108833</td>\n",
       "      <td id=\"T_55ff9_row2_col13\" class=\"data row2 col13\" >-118.229715</td>\n",
       "      <td id=\"T_55ff9_row2_col14\" class=\"data row2 col14\" >47534</td>\n",
       "      <td id=\"T_55ff9_row2_col15\" class=\"data row2 col15\" >Q3</td>\n",
       "      <td id=\"T_55ff9_row2_col16\" class=\"data row2 col16\" >No</td>\n",
       "      <td id=\"T_55ff9_row2_col17\" class=\"data row2 col17\" >0</td>\n",
       "      <td id=\"T_55ff9_row2_col18\" class=\"data row2 col18\" >18</td>\n",
       "      <td id=\"T_55ff9_row2_col19\" class=\"data row2 col19\" >Offer D</td>\n",
       "      <td id=\"T_55ff9_row2_col20\" class=\"data row2 col20\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row2_col21\" class=\"data row2 col21\" >11.330000</td>\n",
       "      <td id=\"T_55ff9_row2_col22\" class=\"data row2 col22\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row2_col23\" class=\"data row2 col23\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row2_col24\" class=\"data row2 col24\" >Fiber Optic</td>\n",
       "      <td id=\"T_55ff9_row2_col25\" class=\"data row2 col25\" >52</td>\n",
       "      <td id=\"T_55ff9_row2_col26\" class=\"data row2 col26\" >No</td>\n",
       "      <td id=\"T_55ff9_row2_col27\" class=\"data row2 col27\" >No</td>\n",
       "      <td id=\"T_55ff9_row2_col28\" class=\"data row2 col28\" >No</td>\n",
       "      <td id=\"T_55ff9_row2_col29\" class=\"data row2 col29\" >No</td>\n",
       "      <td id=\"T_55ff9_row2_col30\" class=\"data row2 col30\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row2_col31\" class=\"data row2 col31\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row2_col32\" class=\"data row2 col32\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row2_col33\" class=\"data row2 col33\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row2_col34\" class=\"data row2 col34\" >Month-to-Month</td>\n",
       "      <td id=\"T_55ff9_row2_col35\" class=\"data row2 col35\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row2_col36\" class=\"data row2 col36\" >Bank Withdrawal</td>\n",
       "      <td id=\"T_55ff9_row2_col37\" class=\"data row2 col37\" >95.450000</td>\n",
       "      <td id=\"T_55ff9_row2_col38\" class=\"data row2 col38\" >1752.550000</td>\n",
       "      <td id=\"T_55ff9_row2_col39\" class=\"data row2 col39\" >45.610000</td>\n",
       "      <td id=\"T_55ff9_row2_col40\" class=\"data row2 col40\" >0</td>\n",
       "      <td id=\"T_55ff9_row2_col41\" class=\"data row2 col41\" >203.940000</td>\n",
       "      <td id=\"T_55ff9_row2_col42\" class=\"data row2 col42\" >1910.880000</td>\n",
       "      <td id=\"T_55ff9_row2_col43\" class=\"data row2 col43\" >2</td>\n",
       "      <td id=\"T_55ff9_row2_col44\" class=\"data row2 col44\" >Churned</td>\n",
       "      <td id=\"T_55ff9_row2_col45\" class=\"data row2 col45\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row2_col46\" class=\"data row2 col46\" >81</td>\n",
       "      <td id=\"T_55ff9_row2_col47\" class=\"data row2 col47\" >3179</td>\n",
       "      <td id=\"T_55ff9_row2_col48\" class=\"data row2 col48\" >Competitor</td>\n",
       "      <td id=\"T_55ff9_row2_col49\" class=\"data row2 col49\" >Competitor made better offer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_55ff9_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_55ff9_row3_col0\" class=\"data row3 col0\" >4598-XLKNJ</td>\n",
       "      <td id=\"T_55ff9_row3_col1\" class=\"data row3 col1\" >Female</td>\n",
       "      <td id=\"T_55ff9_row3_col2\" class=\"data row3 col2\" >78</td>\n",
       "      <td id=\"T_55ff9_row3_col3\" class=\"data row3 col3\" >No</td>\n",
       "      <td id=\"T_55ff9_row3_col4\" class=\"data row3 col4\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row3_col5\" class=\"data row3 col5\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row3_col6\" class=\"data row3 col6\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row3_col7\" class=\"data row3 col7\" >1</td>\n",
       "      <td id=\"T_55ff9_row3_col8\" class=\"data row3 col8\" >United States</td>\n",
       "      <td id=\"T_55ff9_row3_col9\" class=\"data row3 col9\" >California</td>\n",
       "      <td id=\"T_55ff9_row3_col10\" class=\"data row3 col10\" >Inglewood</td>\n",
       "      <td id=\"T_55ff9_row3_col11\" class=\"data row3 col11\" >90303</td>\n",
       "      <td id=\"T_55ff9_row3_col12\" class=\"data row3 col12\" >33.936291</td>\n",
       "      <td id=\"T_55ff9_row3_col13\" class=\"data row3 col13\" >-118.332639</td>\n",
       "      <td id=\"T_55ff9_row3_col14\" class=\"data row3 col14\" >27778</td>\n",
       "      <td id=\"T_55ff9_row3_col15\" class=\"data row3 col15\" >Q3</td>\n",
       "      <td id=\"T_55ff9_row3_col16\" class=\"data row3 col16\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row3_col17\" class=\"data row3 col17\" >1</td>\n",
       "      <td id=\"T_55ff9_row3_col18\" class=\"data row3 col18\" >25</td>\n",
       "      <td id=\"T_55ff9_row3_col19\" class=\"data row3 col19\" >Offer C</td>\n",
       "      <td id=\"T_55ff9_row3_col20\" class=\"data row3 col20\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row3_col21\" class=\"data row3 col21\" >19.760000</td>\n",
       "      <td id=\"T_55ff9_row3_col22\" class=\"data row3 col22\" >No</td>\n",
       "      <td id=\"T_55ff9_row3_col23\" class=\"data row3 col23\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row3_col24\" class=\"data row3 col24\" >Fiber Optic</td>\n",
       "      <td id=\"T_55ff9_row3_col25\" class=\"data row3 col25\" >12</td>\n",
       "      <td id=\"T_55ff9_row3_col26\" class=\"data row3 col26\" >No</td>\n",
       "      <td id=\"T_55ff9_row3_col27\" class=\"data row3 col27\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row3_col28\" class=\"data row3 col28\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row3_col29\" class=\"data row3 col29\" >No</td>\n",
       "      <td id=\"T_55ff9_row3_col30\" class=\"data row3 col30\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row3_col31\" class=\"data row3 col31\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row3_col32\" class=\"data row3 col32\" >No</td>\n",
       "      <td id=\"T_55ff9_row3_col33\" class=\"data row3 col33\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row3_col34\" class=\"data row3 col34\" >Month-to-Month</td>\n",
       "      <td id=\"T_55ff9_row3_col35\" class=\"data row3 col35\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row3_col36\" class=\"data row3 col36\" >Bank Withdrawal</td>\n",
       "      <td id=\"T_55ff9_row3_col37\" class=\"data row3 col37\" >98.500000</td>\n",
       "      <td id=\"T_55ff9_row3_col38\" class=\"data row3 col38\" >2514.500000</td>\n",
       "      <td id=\"T_55ff9_row3_col39\" class=\"data row3 col39\" >13.430000</td>\n",
       "      <td id=\"T_55ff9_row3_col40\" class=\"data row3 col40\" >0</td>\n",
       "      <td id=\"T_55ff9_row3_col41\" class=\"data row3 col41\" >494.000000</td>\n",
       "      <td id=\"T_55ff9_row3_col42\" class=\"data row3 col42\" >2995.070000</td>\n",
       "      <td id=\"T_55ff9_row3_col43\" class=\"data row3 col43\" >2</td>\n",
       "      <td id=\"T_55ff9_row3_col44\" class=\"data row3 col44\" >Churned</td>\n",
       "      <td id=\"T_55ff9_row3_col45\" class=\"data row3 col45\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row3_col46\" class=\"data row3 col46\" >88</td>\n",
       "      <td id=\"T_55ff9_row3_col47\" class=\"data row3 col47\" >5337</td>\n",
       "      <td id=\"T_55ff9_row3_col48\" class=\"data row3 col48\" >Dissatisfaction</td>\n",
       "      <td id=\"T_55ff9_row3_col49\" class=\"data row3 col49\" >Limited range of services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_55ff9_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_55ff9_row4_col0\" class=\"data row4 col0\" >4846-WHAFZ</td>\n",
       "      <td id=\"T_55ff9_row4_col1\" class=\"data row4 col1\" >Female</td>\n",
       "      <td id=\"T_55ff9_row4_col2\" class=\"data row4 col2\" >80</td>\n",
       "      <td id=\"T_55ff9_row4_col3\" class=\"data row4 col3\" >No</td>\n",
       "      <td id=\"T_55ff9_row4_col4\" class=\"data row4 col4\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row4_col5\" class=\"data row4 col5\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row4_col6\" class=\"data row4 col6\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row4_col7\" class=\"data row4 col7\" >1</td>\n",
       "      <td id=\"T_55ff9_row4_col8\" class=\"data row4 col8\" >United States</td>\n",
       "      <td id=\"T_55ff9_row4_col9\" class=\"data row4 col9\" >California</td>\n",
       "      <td id=\"T_55ff9_row4_col10\" class=\"data row4 col10\" >Whittier</td>\n",
       "      <td id=\"T_55ff9_row4_col11\" class=\"data row4 col11\" >90602</td>\n",
       "      <td id=\"T_55ff9_row4_col12\" class=\"data row4 col12\" >33.972119</td>\n",
       "      <td id=\"T_55ff9_row4_col13\" class=\"data row4 col13\" >-118.020188</td>\n",
       "      <td id=\"T_55ff9_row4_col14\" class=\"data row4 col14\" >26265</td>\n",
       "      <td id=\"T_55ff9_row4_col15\" class=\"data row4 col15\" >Q3</td>\n",
       "      <td id=\"T_55ff9_row4_col16\" class=\"data row4 col16\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row4_col17\" class=\"data row4 col17\" >1</td>\n",
       "      <td id=\"T_55ff9_row4_col18\" class=\"data row4 col18\" >37</td>\n",
       "      <td id=\"T_55ff9_row4_col19\" class=\"data row4 col19\" >Offer C</td>\n",
       "      <td id=\"T_55ff9_row4_col20\" class=\"data row4 col20\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row4_col21\" class=\"data row4 col21\" >6.330000</td>\n",
       "      <td id=\"T_55ff9_row4_col22\" class=\"data row4 col22\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row4_col23\" class=\"data row4 col23\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row4_col24\" class=\"data row4 col24\" >Fiber Optic</td>\n",
       "      <td id=\"T_55ff9_row4_col25\" class=\"data row4 col25\" >14</td>\n",
       "      <td id=\"T_55ff9_row4_col26\" class=\"data row4 col26\" >No</td>\n",
       "      <td id=\"T_55ff9_row4_col27\" class=\"data row4 col27\" >No</td>\n",
       "      <td id=\"T_55ff9_row4_col28\" class=\"data row4 col28\" >No</td>\n",
       "      <td id=\"T_55ff9_row4_col29\" class=\"data row4 col29\" >No</td>\n",
       "      <td id=\"T_55ff9_row4_col30\" class=\"data row4 col30\" >No</td>\n",
       "      <td id=\"T_55ff9_row4_col31\" class=\"data row4 col31\" >No</td>\n",
       "      <td id=\"T_55ff9_row4_col32\" class=\"data row4 col32\" >No</td>\n",
       "      <td id=\"T_55ff9_row4_col33\" class=\"data row4 col33\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row4_col34\" class=\"data row4 col34\" >Month-to-Month</td>\n",
       "      <td id=\"T_55ff9_row4_col35\" class=\"data row4 col35\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row4_col36\" class=\"data row4 col36\" >Bank Withdrawal</td>\n",
       "      <td id=\"T_55ff9_row4_col37\" class=\"data row4 col37\" >76.500000</td>\n",
       "      <td id=\"T_55ff9_row4_col38\" class=\"data row4 col38\" >2868.150000</td>\n",
       "      <td id=\"T_55ff9_row4_col39\" class=\"data row4 col39\" >0.000000</td>\n",
       "      <td id=\"T_55ff9_row4_col40\" class=\"data row4 col40\" >0</td>\n",
       "      <td id=\"T_55ff9_row4_col41\" class=\"data row4 col41\" >234.210000</td>\n",
       "      <td id=\"T_55ff9_row4_col42\" class=\"data row4 col42\" >3102.360000</td>\n",
       "      <td id=\"T_55ff9_row4_col43\" class=\"data row4 col43\" >2</td>\n",
       "      <td id=\"T_55ff9_row4_col44\" class=\"data row4 col44\" >Churned</td>\n",
       "      <td id=\"T_55ff9_row4_col45\" class=\"data row4 col45\" >Yes</td>\n",
       "      <td id=\"T_55ff9_row4_col46\" class=\"data row4 col46\" >67</td>\n",
       "      <td id=\"T_55ff9_row4_col47\" class=\"data row4 col47\" >2793</td>\n",
       "      <td id=\"T_55ff9_row4_col48\" class=\"data row4 col48\" >Price</td>\n",
       "      <td id=\"T_55ff9_row4_col49\" class=\"data row4 col49\" >Extra data charges</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1cdb1d538e0>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(DATA_PATH)\n",
    "data.head(5).style.set_caption(\"<b>IBM Telco Customer Churn Dataset (First 5 Rows)</b>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53c2128",
   "metadata": {},
   "source": [
    "## 1.5. Feature Organization and Data Reordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7b84aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_mapping = {\n",
    "    \"Customer Info\": [\n",
    "        \"Customer ID\", \"Gender\", \"Age\", \"Under 30\", \"Senior Citizen\", \"Married\", \"Dependents\", \"Number of Dependents\"\n",
    "    ],\n",
    "    \"Location Info\": [\n",
    "        \"Country\", \"State\", \"City\", \"Zip Code\", \"Latitude\", \"Longitude\", \"Population\"\n",
    "    ],\n",
    "    \"Referral & Tenure\": [\n",
    "        \"Quarter\", \"Referred a Friend\", \"Number of Referrals\", \"Tenure in Months\", \"Offer\"\n",
    "    ],\n",
    "    \"Services Signed Up\": [\n",
    "        \"Phone Service\", \"Multiple Lines\", \"Internet Service\", \"Internet Type\", \"Unlimited Data\"\n",
    "    ],\n",
    "    \"Internet Features\": [\n",
    "        \"Online Security\", \"Online Backup\", \"Device Protection Plan\", \"Premium Tech Support\",\n",
    "        \"Streaming TV\", \"Streaming Movies\", \"Streaming Music\"\n",
    "    ],\n",
    "    \"Billing & Payment\": [\n",
    "        \"Avg Monthly Long Distance Charges\", \"Avg Monthly GB Download\", \"Monthly Charge\",\n",
    "        \"Total Charges\", \"Total Refunds\", \"Total Extra Data Charges\",\n",
    "        \"Total Long Distance Charges\", \"Total Revenue\", \"Paperless Billing\", \"Payment Method\"\n",
    "    ],\n",
    "    \"Customer Scores\": [\n",
    "        \"Satisfaction Score\", \"CLTV\", \"Churn Score\"\n",
    "    ],\n",
    "    \"Churn Info\": [\n",
    "        \"Customer Status\", \"Churn Label\", \"Churn Category\", \"Churn Reason\"\n",
    "    ]\n",
    "}\n",
    "desired_order = [col for group in feature_mapping.values() for col in group]\n",
    "data = data[desired_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2641b601",
   "metadata": {},
   "source": [
    "## 1.6. Feature Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "61390d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_remove = [\n",
    "    \"Customer ID\", \"Under 30\", \"Dependents\", \"Country\", \"State\", \"Zip Code\", \"City\", \"Latitude\", \"Longitude\",\n",
    "    \"Population\", \"Referred a Friend\", \"Internet Service\", \"Customer Status\", \"Churn Score\", \"Churn Category\",\n",
    "    \"Churn Reason\", \"CLTV\", \"Satisfaction Score\", \"Quarter\", \"Avg Monthly Long Distance Charges\",\n",
    "    \"Avg Monthly GB Download\", \"Monthly Charge\", \"Total Charges\", \"Total Refunds\", \"Total Extra Data Charges\",\n",
    "    \"Total Long Distance Charges\"\n",
    "]\n",
    "data = data.drop(columns=features_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0099bbf6",
   "metadata": {},
   "source": [
    "## 1.7. Outlier Handling for Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4f8ba793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics before outlier handling:\n",
      "            Age  Number of Dependents  Number of Referrals  Tenure in Months  \\\n",
      "count  7043.00               7043.00              7043.00           7043.00   \n",
      "mean     46.51                  0.47                 1.95             32.39   \n",
      "std      16.75                  0.96                 3.00             24.54   \n",
      "min      19.00                  0.00                 0.00              1.00   \n",
      "25%      32.00                  0.00                 0.00              9.00   \n",
      "50%      46.00                  0.00                 0.00             29.00   \n",
      "75%      60.00                  0.00                 3.00             55.00   \n",
      "max      80.00                  9.00                11.00             72.00   \n",
      "\n",
      "       Total Revenue  \n",
      "count        7043.00  \n",
      "mean         3034.38  \n",
      "std          2865.20  \n",
      "min            21.36  \n",
      "25%           605.61  \n",
      "50%          2108.64  \n",
      "75%          4801.15  \n",
      "max         11979.34  \n",
      "Summary statistics after outlier handling:\n",
      "            Age  Number of Dependents  Number of Referrals  Tenure in Months  \\\n",
      "count  7043.00                7043.0              7043.00           7043.00   \n",
      "mean     46.51                   0.0                 1.81             32.39   \n",
      "std      16.75                   0.0                 2.66             24.54   \n",
      "min      19.00                   0.0                 0.00              1.00   \n",
      "25%      32.00                   0.0                 0.00              9.00   \n",
      "50%      46.00                   0.0                 0.00             29.00   \n",
      "75%      60.00                   0.0                 3.00             55.00   \n",
      "max      80.00                   0.0                 7.50             72.00   \n",
      "\n",
      "       Total Revenue  \n",
      "count        7043.00  \n",
      "mean         3033.27  \n",
      "std          2861.98  \n",
      "min            21.36  \n",
      "25%           605.61  \n",
      "50%          2108.64  \n",
      "75%          4801.15  \n",
      "max         11094.45  \n"
     ]
    }
   ],
   "source": [
    "# Handle outliers in numeric features\n",
    "numeric_features = [\"Age\", \"Number of Dependents\", \"Number of Referrals\", \"Tenure in Months\", \"Total Revenue\"]\n",
    "print(\"Summary statistics before outlier handling:\\n\", data[numeric_features].describe())\n",
    "for feature in numeric_features:\n",
    "    q1, q3 = data[feature].quantile([0.25, 0.75])\n",
    "    iqr = q3 - q1\n",
    "    lower_bound, upper_bound = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "    data[feature] = data[feature].clip(lower=lower_bound, upper=upper_bound)\n",
    "print(\"Summary statistics after outlier handling:\\n\", data[numeric_features].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ac7c15",
   "metadata": {},
   "source": [
    "## 1.8. Missing Value Imputation and Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7d2dded7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values after preprocessing.\n"
     ]
    }
   ],
   "source": [
    "# Fill missing values and verify none remain\n",
    "data[\"Offer\"] = data[\"Offer\"].fillna(\"None\")\n",
    "data[\"Internet Type\"] = data[\"Internet Type\"].fillna(\"No Internet\")\n",
    "missing_values = data.isnull().sum()\n",
    "if missing_values.sum() > 0:\n",
    "    print(\"Warning: Missing values found after preprocessing:\\n\", missing_values[missing_values > 0])\n",
    "else:\n",
    "    print(\"No missing values after preprocessing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b4e15d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_aee4b\">\n",
       "  <caption><b>Non-Missing Percentage of Features</b></caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_aee4b_level0_col0\" class=\"col_heading level0 col0\" >Feature</th>\n",
       "      <th id=\"T_aee4b_level0_col1\" class=\"col_heading level0 col1\" >Non-Missing Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_aee4b_level0_row0\" class=\"row_heading level0 row0\" >1</th>\n",
       "      <td id=\"T_aee4b_row0_col0\" class=\"data row0 col0\" >Gender</td>\n",
       "      <td id=\"T_aee4b_row0_col1\" class=\"data row0 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_aee4b_level0_row1\" class=\"row_heading level0 row1\" >2</th>\n",
       "      <td id=\"T_aee4b_row1_col0\" class=\"data row1 col0\" >Age</td>\n",
       "      <td id=\"T_aee4b_row1_col1\" class=\"data row1 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_aee4b_level0_row2\" class=\"row_heading level0 row2\" >3</th>\n",
       "      <td id=\"T_aee4b_row2_col0\" class=\"data row2 col0\" >Senior Citizen</td>\n",
       "      <td id=\"T_aee4b_row2_col1\" class=\"data row2 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_aee4b_level0_row3\" class=\"row_heading level0 row3\" >4</th>\n",
       "      <td id=\"T_aee4b_row3_col0\" class=\"data row3 col0\" >Married</td>\n",
       "      <td id=\"T_aee4b_row3_col1\" class=\"data row3 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_aee4b_level0_row4\" class=\"row_heading level0 row4\" >5</th>\n",
       "      <td id=\"T_aee4b_row4_col0\" class=\"data row4 col0\" >Number of Dependents</td>\n",
       "      <td id=\"T_aee4b_row4_col1\" class=\"data row4 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_aee4b_level0_row5\" class=\"row_heading level0 row5\" >6</th>\n",
       "      <td id=\"T_aee4b_row5_col0\" class=\"data row5 col0\" >Number of Referrals</td>\n",
       "      <td id=\"T_aee4b_row5_col1\" class=\"data row5 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_aee4b_level0_row6\" class=\"row_heading level0 row6\" >7</th>\n",
       "      <td id=\"T_aee4b_row6_col0\" class=\"data row6 col0\" >Tenure in Months</td>\n",
       "      <td id=\"T_aee4b_row6_col1\" class=\"data row6 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_aee4b_level0_row7\" class=\"row_heading level0 row7\" >8</th>\n",
       "      <td id=\"T_aee4b_row7_col0\" class=\"data row7 col0\" >Offer</td>\n",
       "      <td id=\"T_aee4b_row7_col1\" class=\"data row7 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_aee4b_level0_row8\" class=\"row_heading level0 row8\" >9</th>\n",
       "      <td id=\"T_aee4b_row8_col0\" class=\"data row8 col0\" >Phone Service</td>\n",
       "      <td id=\"T_aee4b_row8_col1\" class=\"data row8 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_aee4b_level0_row9\" class=\"row_heading level0 row9\" >10</th>\n",
       "      <td id=\"T_aee4b_row9_col0\" class=\"data row9 col0\" >Multiple Lines</td>\n",
       "      <td id=\"T_aee4b_row9_col1\" class=\"data row9 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_aee4b_level0_row10\" class=\"row_heading level0 row10\" >11</th>\n",
       "      <td id=\"T_aee4b_row10_col0\" class=\"data row10 col0\" >Internet Type</td>\n",
       "      <td id=\"T_aee4b_row10_col1\" class=\"data row10 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_aee4b_level0_row11\" class=\"row_heading level0 row11\" >12</th>\n",
       "      <td id=\"T_aee4b_row11_col0\" class=\"data row11 col0\" >Unlimited Data</td>\n",
       "      <td id=\"T_aee4b_row11_col1\" class=\"data row11 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_aee4b_level0_row12\" class=\"row_heading level0 row12\" >13</th>\n",
       "      <td id=\"T_aee4b_row12_col0\" class=\"data row12 col0\" >Online Security</td>\n",
       "      <td id=\"T_aee4b_row12_col1\" class=\"data row12 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_aee4b_level0_row13\" class=\"row_heading level0 row13\" >14</th>\n",
       "      <td id=\"T_aee4b_row13_col0\" class=\"data row13 col0\" >Online Backup</td>\n",
       "      <td id=\"T_aee4b_row13_col1\" class=\"data row13 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_aee4b_level0_row14\" class=\"row_heading level0 row14\" >15</th>\n",
       "      <td id=\"T_aee4b_row14_col0\" class=\"data row14 col0\" >Device Protection Plan</td>\n",
       "      <td id=\"T_aee4b_row14_col1\" class=\"data row14 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_aee4b_level0_row15\" class=\"row_heading level0 row15\" >16</th>\n",
       "      <td id=\"T_aee4b_row15_col0\" class=\"data row15 col0\" >Premium Tech Support</td>\n",
       "      <td id=\"T_aee4b_row15_col1\" class=\"data row15 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_aee4b_level0_row16\" class=\"row_heading level0 row16\" >17</th>\n",
       "      <td id=\"T_aee4b_row16_col0\" class=\"data row16 col0\" >Streaming TV</td>\n",
       "      <td id=\"T_aee4b_row16_col1\" class=\"data row16 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_aee4b_level0_row17\" class=\"row_heading level0 row17\" >18</th>\n",
       "      <td id=\"T_aee4b_row17_col0\" class=\"data row17 col0\" >Streaming Movies</td>\n",
       "      <td id=\"T_aee4b_row17_col1\" class=\"data row17 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_aee4b_level0_row18\" class=\"row_heading level0 row18\" >19</th>\n",
       "      <td id=\"T_aee4b_row18_col0\" class=\"data row18 col0\" >Streaming Music</td>\n",
       "      <td id=\"T_aee4b_row18_col1\" class=\"data row18 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_aee4b_level0_row19\" class=\"row_heading level0 row19\" >20</th>\n",
       "      <td id=\"T_aee4b_row19_col0\" class=\"data row19 col0\" >Total Revenue</td>\n",
       "      <td id=\"T_aee4b_row19_col1\" class=\"data row19 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_aee4b_level0_row20\" class=\"row_heading level0 row20\" >21</th>\n",
       "      <td id=\"T_aee4b_row20_col0\" class=\"data row20 col0\" >Paperless Billing</td>\n",
       "      <td id=\"T_aee4b_row20_col1\" class=\"data row20 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_aee4b_level0_row21\" class=\"row_heading level0 row21\" >22</th>\n",
       "      <td id=\"T_aee4b_row21_col0\" class=\"data row21 col0\" >Payment Method</td>\n",
       "      <td id=\"T_aee4b_row21_col1\" class=\"data row21 col1\" >100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_aee4b_level0_row22\" class=\"row_heading level0 row22\" >23</th>\n",
       "      <td id=\"T_aee4b_row22_col0\" class=\"data row22 col0\" >Churn Label</td>\n",
       "      <td id=\"T_aee4b_row22_col1\" class=\"data row22 col1\" >100.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1cd814d0a30>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_missing_percentage = data.notnull().mean() * 100\n",
    "non_missing_df = pd.DataFrame(non_missing_percentage, columns=[\"Non-Missing Percentage\"])\n",
    "non_missing_df = non_missing_df.reset_index().rename(columns={\"index\": \"Feature\"})\n",
    "non_missing_df.index = non_missing_df.index + 1\n",
    "non_missing_df.style.set_caption(\"<b>Non-Missing Percentage of Features</b>\").format({\"Non-Missing Percentage\": \"{:.2f}\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f20734",
   "metadata": {},
   "source": [
    "## 1.9. Feature Type Categorization and Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c71fee5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_1ad53\">\n",
       "  <caption><b>Categorization of Features by Type</b></caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_1ad53_level0_col0\" class=\"col_heading level0 col0\" >Categorical Features</th>\n",
       "      <th id=\"T_1ad53_level0_col1\" class=\"col_heading level0 col1\" >Numeric Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad53_level0_row0\" class=\"row_heading level0 row0\" >1</th>\n",
       "      <td id=\"T_1ad53_row0_col0\" class=\"data row0 col0\" >Gender</td>\n",
       "      <td id=\"T_1ad53_row0_col1\" class=\"data row0 col1\" >Age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad53_level0_row1\" class=\"row_heading level0 row1\" >2</th>\n",
       "      <td id=\"T_1ad53_row1_col0\" class=\"data row1 col0\" >Senior Citizen</td>\n",
       "      <td id=\"T_1ad53_row1_col1\" class=\"data row1 col1\" >Number of Dependents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad53_level0_row2\" class=\"row_heading level0 row2\" >3</th>\n",
       "      <td id=\"T_1ad53_row2_col0\" class=\"data row2 col0\" >Married</td>\n",
       "      <td id=\"T_1ad53_row2_col1\" class=\"data row2 col1\" >Number of Referrals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad53_level0_row3\" class=\"row_heading level0 row3\" >4</th>\n",
       "      <td id=\"T_1ad53_row3_col0\" class=\"data row3 col0\" >Offer</td>\n",
       "      <td id=\"T_1ad53_row3_col1\" class=\"data row3 col1\" >Tenure in Months</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad53_level0_row4\" class=\"row_heading level0 row4\" >5</th>\n",
       "      <td id=\"T_1ad53_row4_col0\" class=\"data row4 col0\" >Phone Service</td>\n",
       "      <td id=\"T_1ad53_row4_col1\" class=\"data row4 col1\" >Total Revenue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad53_level0_row5\" class=\"row_heading level0 row5\" >6</th>\n",
       "      <td id=\"T_1ad53_row5_col0\" class=\"data row5 col0\" >Multiple Lines</td>\n",
       "      <td id=\"T_1ad53_row5_col1\" class=\"data row5 col1\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad53_level0_row6\" class=\"row_heading level0 row6\" >7</th>\n",
       "      <td id=\"T_1ad53_row6_col0\" class=\"data row6 col0\" >Internet Type</td>\n",
       "      <td id=\"T_1ad53_row6_col1\" class=\"data row6 col1\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad53_level0_row7\" class=\"row_heading level0 row7\" >8</th>\n",
       "      <td id=\"T_1ad53_row7_col0\" class=\"data row7 col0\" >Unlimited Data</td>\n",
       "      <td id=\"T_1ad53_row7_col1\" class=\"data row7 col1\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad53_level0_row8\" class=\"row_heading level0 row8\" >9</th>\n",
       "      <td id=\"T_1ad53_row8_col0\" class=\"data row8 col0\" >Online Security</td>\n",
       "      <td id=\"T_1ad53_row8_col1\" class=\"data row8 col1\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad53_level0_row9\" class=\"row_heading level0 row9\" >10</th>\n",
       "      <td id=\"T_1ad53_row9_col0\" class=\"data row9 col0\" >Online Backup</td>\n",
       "      <td id=\"T_1ad53_row9_col1\" class=\"data row9 col1\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad53_level0_row10\" class=\"row_heading level0 row10\" >11</th>\n",
       "      <td id=\"T_1ad53_row10_col0\" class=\"data row10 col0\" >Device Protection Plan</td>\n",
       "      <td id=\"T_1ad53_row10_col1\" class=\"data row10 col1\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad53_level0_row11\" class=\"row_heading level0 row11\" >12</th>\n",
       "      <td id=\"T_1ad53_row11_col0\" class=\"data row11 col0\" >Premium Tech Support</td>\n",
       "      <td id=\"T_1ad53_row11_col1\" class=\"data row11 col1\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad53_level0_row12\" class=\"row_heading level0 row12\" >13</th>\n",
       "      <td id=\"T_1ad53_row12_col0\" class=\"data row12 col0\" >Streaming TV</td>\n",
       "      <td id=\"T_1ad53_row12_col1\" class=\"data row12 col1\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad53_level0_row13\" class=\"row_heading level0 row13\" >14</th>\n",
       "      <td id=\"T_1ad53_row13_col0\" class=\"data row13 col0\" >Streaming Movies</td>\n",
       "      <td id=\"T_1ad53_row13_col1\" class=\"data row13 col1\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad53_level0_row14\" class=\"row_heading level0 row14\" >15</th>\n",
       "      <td id=\"T_1ad53_row14_col0\" class=\"data row14 col0\" >Streaming Music</td>\n",
       "      <td id=\"T_1ad53_row14_col1\" class=\"data row14 col1\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad53_level0_row15\" class=\"row_heading level0 row15\" >16</th>\n",
       "      <td id=\"T_1ad53_row15_col0\" class=\"data row15 col0\" >Paperless Billing</td>\n",
       "      <td id=\"T_1ad53_row15_col1\" class=\"data row15 col1\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad53_level0_row16\" class=\"row_heading level0 row16\" >17</th>\n",
       "      <td id=\"T_1ad53_row16_col0\" class=\"data row16 col0\" >Payment Method</td>\n",
       "      <td id=\"T_1ad53_row16_col1\" class=\"data row16 col1\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ad53_level0_row17\" class=\"row_heading level0 row17\" >18</th>\n",
       "      <td id=\"T_1ad53_row17_col0\" class=\"data row17 col0\" >Churn Label</td>\n",
       "      <td id=\"T_1ad53_row17_col1\" class=\"data row17 col1\" ></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1cd816d9690>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features = [\n",
    "    \"Gender\", \"Senior Citizen\", \"Married\", \"Offer\", \"Phone Service\", \"Multiple Lines\", \"Internet Type\",\n",
    "    \"Unlimited Data\", \"Online Security\", \"Online Backup\", \"Device Protection Plan\", \"Premium Tech Support\",\n",
    "    \"Streaming TV\", \"Streaming Movies\", \"Streaming Music\", \"Paperless Billing\", \"Payment Method\", \"Churn Label\"\n",
    "]\n",
    "numeric_features = [\"Age\", \"Number of Dependents\", \"Number of Referrals\", \"Tenure in Months\", \"Total Revenue\"]\n",
    "max_length = max(len(categorical_features), len(numeric_features))\n",
    "categorical_features_feature = categorical_features.copy()\n",
    "numeric_features_feature = numeric_features.copy()\n",
    "categorical_features_feature += [\"\"] * (max_length - len(categorical_features))\n",
    "numeric_features_feature += [\"\"] * (max_length - len(numeric_features))\n",
    "feature_types_df = pd.DataFrame({\n",
    "    \"Categorical Features\": categorical_features_feature,\n",
    "    \"Numeric Features\": numeric_features_feature\n",
    "})\n",
    "feature_types_df.index = feature_types_df.index + 1\n",
    "feature_types_df.style.set_caption(\"<b>Categorization of Features by Type</b>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c51cccce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_dfa38\">\n",
       "  <caption>Numeric Features' Data Types</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_dfa38_level0_col0\" class=\"col_heading level0 col0\" >Numeric Features' Data Types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_dfa38_level0_row0\" class=\"row_heading level0 row0\" >Age</th>\n",
       "      <td id=\"T_dfa38_row0_col0\" class=\"data row0 col0\" >int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dfa38_level0_row1\" class=\"row_heading level0 row1\" >Number of Dependents</th>\n",
       "      <td id=\"T_dfa38_row1_col0\" class=\"data row1 col0\" >int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dfa38_level0_row2\" class=\"row_heading level0 row2\" >Number of Referrals</th>\n",
       "      <td id=\"T_dfa38_row2_col0\" class=\"data row2 col0\" >float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dfa38_level0_row3\" class=\"row_heading level0 row3\" >Tenure in Months</th>\n",
       "      <td id=\"T_dfa38_row3_col0\" class=\"data row3 col0\" >int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dfa38_level0_row4\" class=\"row_heading level0 row4\" >Total Revenue</th>\n",
       "      <td id=\"T_dfa38_row4_col0\" class=\"data row4 col0\" >float64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1cd814647f0>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features_data_types = pd.DataFrame(data[categorical_features].dtypes, columns=[\"Categorical Features' Data Types\"])\n",
    "categorical_features_data_types.style.set_caption(\"<b>Categorical Features' Data Types</b>\")\n",
    "numeric_features_data_types = pd.DataFrame(data[numeric_features].dtypes, columns=[\"Numeric Features' Data Types\"])\n",
    "numeric_features_data_types.style.set_caption(\"Numeric Features' Data Types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f495843d",
   "metadata": {},
   "source": [
    "## 1.10. Analysis of Categorical Feature Unique Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "729dc758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_366da\">\n",
       "  <caption><b>Unique Values in Categorical Features</b></caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_366da_level0_col0\" class=\"col_heading level0 col0\" >Feature</th>\n",
       "      <th id=\"T_366da_level0_col1\" class=\"col_heading level0 col1\" >Unique Value</th>\n",
       "      <th id=\"T_366da_level0_col2\" class=\"col_heading level0 col2\" >Frequency</th>\n",
       "      <th id=\"T_366da_level0_col3\" class=\"col_heading level0 col3\" >Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row0_col0\" class=\"data row0 col0\" >Gender</td>\n",
       "      <td id=\"T_366da_row0_col1\" class=\"data row0 col1\" >Male</td>\n",
       "      <td id=\"T_366da_row0_col2\" class=\"data row0 col2\" >3555</td>\n",
       "      <td id=\"T_366da_row0_col3\" class=\"data row0 col3\" >50.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row1_col0\" class=\"data row1 col0\" ></td>\n",
       "      <td id=\"T_366da_row1_col1\" class=\"data row1 col1\" >Female</td>\n",
       "      <td id=\"T_366da_row1_col2\" class=\"data row1 col2\" >3488</td>\n",
       "      <td id=\"T_366da_row1_col3\" class=\"data row1 col3\" >49.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row2_col0\" class=\"data row2 col0\" >Senior Citizen</td>\n",
       "      <td id=\"T_366da_row2_col1\" class=\"data row2 col1\" >No</td>\n",
       "      <td id=\"T_366da_row2_col2\" class=\"data row2 col2\" >5901</td>\n",
       "      <td id=\"T_366da_row2_col3\" class=\"data row2 col3\" >83.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row3_col0\" class=\"data row3 col0\" ></td>\n",
       "      <td id=\"T_366da_row3_col1\" class=\"data row3 col1\" >Yes</td>\n",
       "      <td id=\"T_366da_row3_col2\" class=\"data row3 col2\" >1142</td>\n",
       "      <td id=\"T_366da_row3_col3\" class=\"data row3 col3\" >16.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row4_col0\" class=\"data row4 col0\" >Married</td>\n",
       "      <td id=\"T_366da_row4_col1\" class=\"data row4 col1\" >No</td>\n",
       "      <td id=\"T_366da_row4_col2\" class=\"data row4 col2\" >3641</td>\n",
       "      <td id=\"T_366da_row4_col3\" class=\"data row4 col3\" >51.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row5_col0\" class=\"data row5 col0\" ></td>\n",
       "      <td id=\"T_366da_row5_col1\" class=\"data row5 col1\" >Yes</td>\n",
       "      <td id=\"T_366da_row5_col2\" class=\"data row5 col2\" >3402</td>\n",
       "      <td id=\"T_366da_row5_col3\" class=\"data row5 col3\" >48.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row6_col0\" class=\"data row6 col0\" >Offer</td>\n",
       "      <td id=\"T_366da_row6_col1\" class=\"data row6 col1\" >None</td>\n",
       "      <td id=\"T_366da_row6_col2\" class=\"data row6 col2\" >3877</td>\n",
       "      <td id=\"T_366da_row6_col3\" class=\"data row6 col3\" >55.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row7_col0\" class=\"data row7 col0\" ></td>\n",
       "      <td id=\"T_366da_row7_col1\" class=\"data row7 col1\" >Offer B</td>\n",
       "      <td id=\"T_366da_row7_col2\" class=\"data row7 col2\" >824</td>\n",
       "      <td id=\"T_366da_row7_col3\" class=\"data row7 col3\" >11.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row8_col0\" class=\"data row8 col0\" ></td>\n",
       "      <td id=\"T_366da_row8_col1\" class=\"data row8 col1\" >Offer E</td>\n",
       "      <td id=\"T_366da_row8_col2\" class=\"data row8 col2\" >805</td>\n",
       "      <td id=\"T_366da_row8_col3\" class=\"data row8 col3\" >11.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row9_col0\" class=\"data row9 col0\" ></td>\n",
       "      <td id=\"T_366da_row9_col1\" class=\"data row9 col1\" >Offer D</td>\n",
       "      <td id=\"T_366da_row9_col2\" class=\"data row9 col2\" >602</td>\n",
       "      <td id=\"T_366da_row9_col3\" class=\"data row9 col3\" >8.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row10_col0\" class=\"data row10 col0\" ></td>\n",
       "      <td id=\"T_366da_row10_col1\" class=\"data row10 col1\" >Offer A</td>\n",
       "      <td id=\"T_366da_row10_col2\" class=\"data row10 col2\" >520</td>\n",
       "      <td id=\"T_366da_row10_col3\" class=\"data row10 col3\" >7.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row11_col0\" class=\"data row11 col0\" ></td>\n",
       "      <td id=\"T_366da_row11_col1\" class=\"data row11 col1\" >Offer C</td>\n",
       "      <td id=\"T_366da_row11_col2\" class=\"data row11 col2\" >415</td>\n",
       "      <td id=\"T_366da_row11_col3\" class=\"data row11 col3\" >5.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row12_col0\" class=\"data row12 col0\" >Phone Service</td>\n",
       "      <td id=\"T_366da_row12_col1\" class=\"data row12 col1\" >Yes</td>\n",
       "      <td id=\"T_366da_row12_col2\" class=\"data row12 col2\" >6361</td>\n",
       "      <td id=\"T_366da_row12_col3\" class=\"data row12 col3\" >90.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row13_col0\" class=\"data row13 col0\" ></td>\n",
       "      <td id=\"T_366da_row13_col1\" class=\"data row13 col1\" >No</td>\n",
       "      <td id=\"T_366da_row13_col2\" class=\"data row13 col2\" >682</td>\n",
       "      <td id=\"T_366da_row13_col3\" class=\"data row13 col3\" >9.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row14_col0\" class=\"data row14 col0\" >Multiple Lines</td>\n",
       "      <td id=\"T_366da_row14_col1\" class=\"data row14 col1\" >No</td>\n",
       "      <td id=\"T_366da_row14_col2\" class=\"data row14 col2\" >4072</td>\n",
       "      <td id=\"T_366da_row14_col3\" class=\"data row14 col3\" >57.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row15_col0\" class=\"data row15 col0\" ></td>\n",
       "      <td id=\"T_366da_row15_col1\" class=\"data row15 col1\" >Yes</td>\n",
       "      <td id=\"T_366da_row15_col2\" class=\"data row15 col2\" >2971</td>\n",
       "      <td id=\"T_366da_row15_col3\" class=\"data row15 col3\" >42.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row16_col0\" class=\"data row16 col0\" >Internet Type</td>\n",
       "      <td id=\"T_366da_row16_col1\" class=\"data row16 col1\" >Fiber Optic</td>\n",
       "      <td id=\"T_366da_row16_col2\" class=\"data row16 col2\" >3035</td>\n",
       "      <td id=\"T_366da_row16_col3\" class=\"data row16 col3\" >43.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row17_col0\" class=\"data row17 col0\" ></td>\n",
       "      <td id=\"T_366da_row17_col1\" class=\"data row17 col1\" >DSL</td>\n",
       "      <td id=\"T_366da_row17_col2\" class=\"data row17 col2\" >1652</td>\n",
       "      <td id=\"T_366da_row17_col3\" class=\"data row17 col3\" >23.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row18_col0\" class=\"data row18 col0\" ></td>\n",
       "      <td id=\"T_366da_row18_col1\" class=\"data row18 col1\" >None</td>\n",
       "      <td id=\"T_366da_row18_col2\" class=\"data row18 col2\" >1526</td>\n",
       "      <td id=\"T_366da_row18_col3\" class=\"data row18 col3\" >21.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row19_col0\" class=\"data row19 col0\" ></td>\n",
       "      <td id=\"T_366da_row19_col1\" class=\"data row19 col1\" >Cable</td>\n",
       "      <td id=\"T_366da_row19_col2\" class=\"data row19 col2\" >830</td>\n",
       "      <td id=\"T_366da_row19_col3\" class=\"data row19 col3\" >11.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row20_col0\" class=\"data row20 col0\" >Unlimited Data</td>\n",
       "      <td id=\"T_366da_row20_col1\" class=\"data row20 col1\" >Yes</td>\n",
       "      <td id=\"T_366da_row20_col2\" class=\"data row20 col2\" >4745</td>\n",
       "      <td id=\"T_366da_row20_col3\" class=\"data row20 col3\" >67.370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row21_col0\" class=\"data row21 col0\" ></td>\n",
       "      <td id=\"T_366da_row21_col1\" class=\"data row21 col1\" >No</td>\n",
       "      <td id=\"T_366da_row21_col2\" class=\"data row21 col2\" >2298</td>\n",
       "      <td id=\"T_366da_row21_col3\" class=\"data row21 col3\" >32.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row22_col0\" class=\"data row22 col0\" >Online Security</td>\n",
       "      <td id=\"T_366da_row22_col1\" class=\"data row22 col1\" >No</td>\n",
       "      <td id=\"T_366da_row22_col2\" class=\"data row22 col2\" >5024</td>\n",
       "      <td id=\"T_366da_row22_col3\" class=\"data row22 col3\" >71.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row23_col0\" class=\"data row23 col0\" ></td>\n",
       "      <td id=\"T_366da_row23_col1\" class=\"data row23 col1\" >Yes</td>\n",
       "      <td id=\"T_366da_row23_col2\" class=\"data row23 col2\" >2019</td>\n",
       "      <td id=\"T_366da_row23_col3\" class=\"data row23 col3\" >28.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row24_col0\" class=\"data row24 col0\" >Online Backup</td>\n",
       "      <td id=\"T_366da_row24_col1\" class=\"data row24 col1\" >No</td>\n",
       "      <td id=\"T_366da_row24_col2\" class=\"data row24 col2\" >4614</td>\n",
       "      <td id=\"T_366da_row24_col3\" class=\"data row24 col3\" >65.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row25_col0\" class=\"data row25 col0\" ></td>\n",
       "      <td id=\"T_366da_row25_col1\" class=\"data row25 col1\" >Yes</td>\n",
       "      <td id=\"T_366da_row25_col2\" class=\"data row25 col2\" >2429</td>\n",
       "      <td id=\"T_366da_row25_col3\" class=\"data row25 col3\" >34.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row26_col0\" class=\"data row26 col0\" >Device Protection Plan</td>\n",
       "      <td id=\"T_366da_row26_col1\" class=\"data row26 col1\" >No</td>\n",
       "      <td id=\"T_366da_row26_col2\" class=\"data row26 col2\" >4621</td>\n",
       "      <td id=\"T_366da_row26_col3\" class=\"data row26 col3\" >65.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row27_col0\" class=\"data row27 col0\" ></td>\n",
       "      <td id=\"T_366da_row27_col1\" class=\"data row27 col1\" >Yes</td>\n",
       "      <td id=\"T_366da_row27_col2\" class=\"data row27 col2\" >2422</td>\n",
       "      <td id=\"T_366da_row27_col3\" class=\"data row27 col3\" >34.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row28_col0\" class=\"data row28 col0\" >Premium Tech Support</td>\n",
       "      <td id=\"T_366da_row28_col1\" class=\"data row28 col1\" >No</td>\n",
       "      <td id=\"T_366da_row28_col2\" class=\"data row28 col2\" >4999</td>\n",
       "      <td id=\"T_366da_row28_col3\" class=\"data row28 col3\" >70.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row29_col0\" class=\"data row29 col0\" ></td>\n",
       "      <td id=\"T_366da_row29_col1\" class=\"data row29 col1\" >Yes</td>\n",
       "      <td id=\"T_366da_row29_col2\" class=\"data row29 col2\" >2044</td>\n",
       "      <td id=\"T_366da_row29_col3\" class=\"data row29 col3\" >29.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row30_col0\" class=\"data row30 col0\" >Streaming TV</td>\n",
       "      <td id=\"T_366da_row30_col1\" class=\"data row30 col1\" >No</td>\n",
       "      <td id=\"T_366da_row30_col2\" class=\"data row30 col2\" >4336</td>\n",
       "      <td id=\"T_366da_row30_col3\" class=\"data row30 col3\" >61.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row31_col0\" class=\"data row31 col0\" ></td>\n",
       "      <td id=\"T_366da_row31_col1\" class=\"data row31 col1\" >Yes</td>\n",
       "      <td id=\"T_366da_row31_col2\" class=\"data row31 col2\" >2707</td>\n",
       "      <td id=\"T_366da_row31_col3\" class=\"data row31 col3\" >38.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row32_col0\" class=\"data row32 col0\" >Streaming Movies</td>\n",
       "      <td id=\"T_366da_row32_col1\" class=\"data row32 col1\" >No</td>\n",
       "      <td id=\"T_366da_row32_col2\" class=\"data row32 col2\" >4311</td>\n",
       "      <td id=\"T_366da_row32_col3\" class=\"data row32 col3\" >61.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row33_col0\" class=\"data row33 col0\" ></td>\n",
       "      <td id=\"T_366da_row33_col1\" class=\"data row33 col1\" >Yes</td>\n",
       "      <td id=\"T_366da_row33_col2\" class=\"data row33 col2\" >2732</td>\n",
       "      <td id=\"T_366da_row33_col3\" class=\"data row33 col3\" >38.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row34_col0\" class=\"data row34 col0\" >Streaming Music</td>\n",
       "      <td id=\"T_366da_row34_col1\" class=\"data row34 col1\" >No</td>\n",
       "      <td id=\"T_366da_row34_col2\" class=\"data row34 col2\" >4555</td>\n",
       "      <td id=\"T_366da_row34_col3\" class=\"data row34 col3\" >64.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row35_col0\" class=\"data row35 col0\" ></td>\n",
       "      <td id=\"T_366da_row35_col1\" class=\"data row35 col1\" >Yes</td>\n",
       "      <td id=\"T_366da_row35_col2\" class=\"data row35 col2\" >2488</td>\n",
       "      <td id=\"T_366da_row35_col3\" class=\"data row35 col3\" >35.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row36_col0\" class=\"data row36 col0\" >Paperless Billing</td>\n",
       "      <td id=\"T_366da_row36_col1\" class=\"data row36 col1\" >Yes</td>\n",
       "      <td id=\"T_366da_row36_col2\" class=\"data row36 col2\" >4171</td>\n",
       "      <td id=\"T_366da_row36_col3\" class=\"data row36 col3\" >59.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row37_col0\" class=\"data row37 col0\" ></td>\n",
       "      <td id=\"T_366da_row37_col1\" class=\"data row37 col1\" >No</td>\n",
       "      <td id=\"T_366da_row37_col2\" class=\"data row37 col2\" >2872</td>\n",
       "      <td id=\"T_366da_row37_col3\" class=\"data row37 col3\" >40.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row38_col0\" class=\"data row38 col0\" >Payment Method</td>\n",
       "      <td id=\"T_366da_row38_col1\" class=\"data row38 col1\" >Bank Withdrawal</td>\n",
       "      <td id=\"T_366da_row38_col2\" class=\"data row38 col2\" >3909</td>\n",
       "      <td id=\"T_366da_row38_col3\" class=\"data row38 col3\" >55.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row39_col0\" class=\"data row39 col0\" ></td>\n",
       "      <td id=\"T_366da_row39_col1\" class=\"data row39 col1\" >Credit Card</td>\n",
       "      <td id=\"T_366da_row39_col2\" class=\"data row39 col2\" >2749</td>\n",
       "      <td id=\"T_366da_row39_col3\" class=\"data row39 col3\" >39.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row40_col0\" class=\"data row40 col0\" ></td>\n",
       "      <td id=\"T_366da_row40_col1\" class=\"data row40 col1\" >Mailed Check</td>\n",
       "      <td id=\"T_366da_row40_col2\" class=\"data row40 col2\" >385</td>\n",
       "      <td id=\"T_366da_row40_col3\" class=\"data row40 col3\" >5.470000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row41_col0\" class=\"data row41 col0\" >Churn Label</td>\n",
       "      <td id=\"T_366da_row41_col1\" class=\"data row41 col1\" >No</td>\n",
       "      <td id=\"T_366da_row41_col2\" class=\"data row41 col2\" >5174</td>\n",
       "      <td id=\"T_366da_row41_col3\" class=\"data row41 col3\" >73.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_366da_row42_col0\" class=\"data row42 col0\" ></td>\n",
       "      <td id=\"T_366da_row42_col1\" class=\"data row42 col1\" >Yes</td>\n",
       "      <td id=\"T_366da_row42_col2\" class=\"data row42 col2\" >1869</td>\n",
       "      <td id=\"T_366da_row42_col3\" class=\"data row42 col3\" >26.540000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1cdb1c45b70>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "for feature in categorical_features:\n",
    "    value_counts = data[feature].value_counts()\n",
    "    first_row = True\n",
    "    for value, count in value_counts.items():\n",
    "        percentage = round((count / len(data)) * 100, 2)\n",
    "        feature_name = feature if first_row else \"\"\n",
    "        rows.append([feature_name, value, count, percentage])\n",
    "        first_row = False\n",
    "unique_values_df = pd.DataFrame(rows, columns=[\"Feature\", \"Unique Value\", \"Frequency\", \"Percentage\"])\n",
    "unique_values_df[\"Percentage\"] = unique_values_df[\"Percentage\"].round(2)\n",
    "unique_values_df = (\n",
    "    unique_values_df.style\n",
    "    .set_caption(\"<b>Unique Values in Categorical Features</b>\")\n",
    "    .hide(axis=\"index\")\n",
    ")\n",
    "unique_values_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9337b30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (5634, 23), Test set shape: (1409, 23)\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=100)\n",
    "print(f\"Train set shape: {train_data.shape}, Test set shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943c15e0",
   "metadata": {},
   "source": [
    "## 1.11. Binary Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9e9af2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in Gender before encoding: ['Female' 'Male']\n",
      "Unique values in Senior Citizen before encoding: ['No' 'Yes']\n",
      "Unique values in Married before encoding: ['Yes' 'No']\n",
      "Unique values in Phone Service before encoding: ['Yes' 'No']\n",
      "Unique values in Multiple Lines before encoding: ['Yes' 'No']\n",
      "Unique values in Unlimited Data before encoding: ['Yes' 'No']\n",
      "Unique values in Online Security before encoding: ['Yes' 'No']\n",
      "Unique values in Online Backup before encoding: ['No' 'Yes']\n",
      "Unique values in Device Protection Plan before encoding: ['Yes' 'No']\n",
      "Unique values in Premium Tech Support before encoding: ['Yes' 'No']\n",
      "Unique values in Streaming TV before encoding: ['Yes' 'No']\n",
      "Unique values in Streaming Movies before encoding: ['Yes' 'No']\n",
      "Unique values in Streaming Music before encoding: ['Yes' 'No']\n",
      "Unique values in Paperless Billing before encoding: ['Yes' 'No']\n",
      "Unique values in Churn Label before encoding: ['No' 'Yes']\n",
      "Unique values in Gender before encoding: ['Female' 'Male']\n",
      "Unique values in Senior Citizen before encoding: ['No' 'Yes']\n",
      "Unique values in Married before encoding: ['Yes' 'No']\n",
      "Unique values in Phone Service before encoding: ['Yes' 'No']\n",
      "Unique values in Multiple Lines before encoding: ['No' 'Yes']\n",
      "Unique values in Unlimited Data before encoding: ['No' 'Yes']\n",
      "Unique values in Online Security before encoding: ['No' 'Yes']\n",
      "Unique values in Online Backup before encoding: ['No' 'Yes']\n",
      "Unique values in Device Protection Plan before encoding: ['No' 'Yes']\n",
      "Unique values in Premium Tech Support before encoding: ['No' 'Yes']\n",
      "Unique values in Streaming TV before encoding: ['No' 'Yes']\n",
      "Unique values in Streaming Movies before encoding: ['No' 'Yes']\n",
      "Unique values in Streaming Music before encoding: ['No' 'Yes']\n",
      "Unique values in Paperless Billing before encoding: ['No' 'Yes']\n",
      "Unique values in Churn Label before encoding: ['No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "# Encode binary features for train and test\n",
    "def encode_binary_features(datasets, features, mapping):\n",
    "    for df in datasets:\n",
    "        for feature in features:\n",
    "            print(f\"Unique values in {feature} before encoding:\", df[feature].unique())\n",
    "            df[feature] = df[feature].astype(str).map(mapping)\n",
    "            df[feature] = df[feature].fillna(0)\n",
    "binary_features = [\n",
    "    \"Gender\", \"Senior Citizen\", \"Married\", \"Phone Service\", \"Multiple Lines\", \"Unlimited Data\",\n",
    "    \"Online Security\", \"Online Backup\", \"Device Protection Plan\", \"Premium Tech Support\",\n",
    "    \"Streaming TV\", \"Streaming Movies\", \"Streaming Music\", \"Paperless Billing\", \"Churn Label\"\n",
    "]\n",
    "binary_mapping = {\"Yes\": 1, \"No\": 0, \"Male\": 1, \"Female\": 0}\n",
    "encode_binary_features(datasets=[train_data, test_data], features=binary_features, mapping=binary_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db9e052",
   "metadata": {},
   "source": [
    "## 1.12. Ordinal Feature Encoding and Type Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9f23d60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in Offer before encoding: ['None' 'Offer D' 'Offer A' 'Offer B' 'Offer C' 'Offer E']\n",
      "Unique values in Internet Type before encoding: ['Fiber Optic' 'None' 'DSL' 'Cable']\n",
      "Unique values in Payment Method before encoding: ['Bank Withdrawal' 'Credit Card' 'Mailed Check']\n",
      "Unique values in Offer before encoding: ['Offer B' 'None' 'Offer C' 'Offer D' 'Offer E' 'Offer A']\n",
      "Unique values in Internet Type before encoding: ['None' 'Cable' 'DSL' 'Fiber Optic']\n",
      "Unique values in Payment Method before encoding: ['Credit Card' 'Bank Withdrawal' 'Mailed Check']\n",
      "Train data types:\n",
      " Gender                    float64\n",
      "Age                       float64\n",
      "Senior Citizen            float64\n",
      "Married                   float64\n",
      "Number of Dependents      float64\n",
      "Number of Referrals       float64\n",
      "Tenure in Months          float64\n",
      "Offer                     float64\n",
      "Phone Service             float64\n",
      "Multiple Lines            float64\n",
      "Internet Type             float64\n",
      "Unlimited Data            float64\n",
      "Online Security           float64\n",
      "Online Backup             float64\n",
      "Device Protection Plan    float64\n",
      "Premium Tech Support      float64\n",
      "Streaming TV              float64\n",
      "Streaming Movies          float64\n",
      "Streaming Music           float64\n",
      "Total Revenue             float64\n",
      "Paperless Billing         float64\n",
      "Payment Method            float64\n",
      "Churn Label               float64\n",
      "dtype: object\n",
      "Test data types:\n",
      " Gender                    float64\n",
      "Age                       float64\n",
      "Senior Citizen            float64\n",
      "Married                   float64\n",
      "Number of Dependents      float64\n",
      "Number of Referrals       float64\n",
      "Tenure in Months          float64\n",
      "Offer                     float64\n",
      "Phone Service             float64\n",
      "Multiple Lines            float64\n",
      "Internet Type             float64\n",
      "Unlimited Data            float64\n",
      "Online Security           float64\n",
      "Online Backup             float64\n",
      "Device Protection Plan    float64\n",
      "Premium Tech Support      float64\n",
      "Streaming TV              float64\n",
      "Streaming Movies          float64\n",
      "Streaming Music           float64\n",
      "Total Revenue             float64\n",
      "Paperless Billing         float64\n",
      "Payment Method            float64\n",
      "Churn Label               float64\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aafz1\\AppData\\Local\\Temp\\ipykernel_13468\\521558324.py:8: UserWarning: Unmapped values in Internet Type: {'None'}\n",
      "  warnings.warn(f\"Unmapped values in {feature}: {unmapped}\")\n"
     ]
    }
   ],
   "source": [
    "# Encode ordinal features and ensure float types\n",
    "def encode_ordinal_features(datasets, mappings):\n",
    "    for df in datasets:\n",
    "        for feature, mapping in mappings.items():\n",
    "            print(f\"Unique values in {feature} before encoding:\", df[feature].unique())\n",
    "            unmapped = set(df[feature].astype(str)) - set(mapping.keys())\n",
    "            if unmapped:\n",
    "                warnings.warn(f\"Unmapped values in {feature}: {unmapped}\")\n",
    "            df[feature] = df[feature].astype(str).map(lambda x: mapping.get(x, 0))\n",
    "            df[feature] = df[feature].fillna(0)\n",
    "offer_mapping = {\n",
    "    \"None\": 0, \"Offer A\": 1, \"Offer B\": 2, \"Offer C\": 3, \"Offer D\": 4, \"Offer E\": 5\n",
    "}\n",
    "internet_type_mapping = {\n",
    "    \"No Internet\": 0, \"DSL\": 1, \"Cable\": 2, \"Fiber Optic\": 3\n",
    "}\n",
    "payment_method_mapping = {\n",
    "    \"Mailed Check\": 1, \"Bank Withdrawal\": 2, \"Credit Card\": 3\n",
    "}\n",
    "ordinal_mappings = {\n",
    "    \"Offer\": offer_mapping,\n",
    "    \"Internet Type\": internet_type_mapping,\n",
    "    \"Payment Method\": payment_method_mapping\n",
    "}\n",
    "encode_ordinal_features(datasets=[train_data, test_data], mappings=ordinal_mappings)\n",
    "train_data = train_data.astype(float)\n",
    "test_data = test_data.astype(float)\n",
    "print(\"Train data types:\\n\", train_data.dtypes)\n",
    "print(\"Test data types:\\n\", test_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7e22ea",
   "metadata": {},
   "source": [
    "## 1.13. Numeric Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ba6d5dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train numeric features scaled:\n",
      "             Age  Number of Dependents  Number of Referrals  Tenure in Months  \\\n",
      "count  5.63e+03                5634.0             5.63e+03          5.63e+03   \n",
      "mean   1.08e-16                   0.0            -3.56e-17         -1.14e-16   \n",
      "std    1.00e+00                   0.0             1.00e+00          1.00e+00   \n",
      "min   -1.64e+00                   0.0            -6.88e-01         -1.29e+00   \n",
      "25%   -8.64e-01                   0.0            -6.88e-01         -9.61e-01   \n",
      "50%   -3.11e-02                   0.0            -6.88e-01         -1.45e-01   \n",
      "75%    8.02e-01                   0.0             4.33e-01          9.15e-01   \n",
      "max    1.99e+00                   0.0             2.12e+00          1.61e+00   \n",
      "\n",
      "       Total Revenue  \n",
      "count       5.63e+03  \n",
      "mean        1.46e-16  \n",
      "std         1.00e+00  \n",
      "min        -1.06e+00  \n",
      "25%        -8.48e-01  \n",
      "50%        -3.18e-01  \n",
      "75%         6.14e-01  \n",
      "max         2.82e+00  \n",
      "Test numeric features scaled:\n",
      "             Age  Number of Dependents  Number of Referrals  Tenure in Months  \\\n",
      "count  1.41e+03                1409.0              1409.00           1409.00   \n",
      "mean  -4.02e-03                   0.0                -0.07             -0.04   \n",
      "std    9.84e-01                   0.0                 0.97              1.01   \n",
      "min   -1.64e+00                   0.0                -0.69             -1.29   \n",
      "25%   -8.05e-01                   0.0                -0.69             -1.00   \n",
      "50%   -3.11e-02                   0.0                -0.69             -0.23   \n",
      "75%    7.43e-01                   0.0                 0.06              0.92   \n",
      "max    1.99e+00                   0.0                 2.12              1.61   \n",
      "\n",
      "       Total Revenue  \n",
      "count        1409.00  \n",
      "mean           -0.02  \n",
      "std             1.01  \n",
      "min            -1.06  \n",
      "25%            -0.88  \n",
      "50%            -0.35  \n",
      "75%             0.62  \n",
      "max             2.82  \n"
     ]
    }
   ],
   "source": [
    "# Scale numeric features for train and test\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "numeric_features = [\"Age\", \"Number of Dependents\", \"Number of Referrals\", \"Tenure in Months\", \"Total Revenue\"]\n",
    "train_data[numeric_features] = scaler.fit_transform(train_data[numeric_features])\n",
    "test_data[numeric_features] = scaler.transform(test_data[numeric_features])\n",
    "print(\"Train numeric features scaled:\\n\", train_data[numeric_features].describe())\n",
    "print(\"Test numeric features scaled:\\n\", test_data[numeric_features].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93ada45",
   "metadata": {},
   "source": [
    "<b><p style=\"font-size: 35px;\">II. Second Phase: Causal Discovery and Inference</p></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328df278",
   "metadata": {},
   "source": [
    "## 2.1. Logging and Random Seed Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "89829cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.FileHandler(\"causal_discovery.log\"), logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(100)\n",
    "torch.manual_seed(100)\n",
    "pl.seed_everything(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc06a67c",
   "metadata": {},
   "source": [
    "## 2.2.1 constraint_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "355cd656",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def create_constraint_matrix(node_names, tiers, specific_constraints=None):\n",
    "    \"\"\"\n",
    "    Create a constraint matrix for causal discovery algorithms.\n",
    "    \n",
    "    Args:\n",
    "        node_names (list): List of variable names.\n",
    "        tiers (list): List of tier lists (e.g., [demographic, customer, ...]).\n",
    "        specific_constraints (dict): Additional constraints (e.g., {\"forbidden\": [(src, dst), ...]}).\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Constraint matrix (np.nan for allowed edges, 0.0 for forbidden).\n",
    "    \"\"\"\n",
    "    num_nodes = len(node_names)\n",
    "    node_name_to_idx = {name: i for i, name in enumerate(node_names)}\n",
    "    constraint_matrix = np.full((num_nodes, num_nodes), np.nan, dtype=np.float32)\n",
    "    \n",
    "    # Set Churn Label as sink node (no outgoing edges)\n",
    "    churn_idx = node_name_to_idx.get(\"Churn Label\")\n",
    "    if churn_idx is not None:\n",
    "        constraint_matrix[churn_idx, :] = 0.0\n",
    "    \n",
    "    # Set demographic variables as root nodes (no incoming edges)\n",
    "    for feature in tiers[0]:  # Tier 1: Demographic\n",
    "        if feature in node_name_to_idx:\n",
    "            feature_idx = node_name_to_idx[feature]\n",
    "            constraint_matrix[:, feature_idx] = 0.0\n",
    "    \n",
    "    # Prevent edges within Tier 1\n",
    "    for src in tiers[0]:\n",
    "        for dst in tiers[0]:\n",
    "            if src != dst and src in node_name_to_idx and dst in node_name_to_idx:\n",
    "                constraint_matrix[node_name_to_idx[src], node_name_to_idx[dst]] = 0.0\n",
    "    \n",
    "    # Allow edges only from Tier N to Tier N+1\n",
    "    for src_tier_idx, src_tier in enumerate(tiers[:-1]):\n",
    "        dst_tier = tiers[src_tier_idx + 1]\n",
    "        for src in src_tier:\n",
    "            for dst in dst_tier:\n",
    "                if src in node_name_to_idx and dst in node_name_to_idx:\n",
    "                    constraint_matrix[node_name_to_idx[src], node_name_to_idx[dst]] = np.nan\n",
    "        # Block edges to other tiers\n",
    "        for other_tier_idx, other_tier in enumerate(tiers):\n",
    "            if other_tier_idx != src_tier_idx + 1:\n",
    "                for src in src_tier:\n",
    "                    for dst in other_tier:\n",
    "                        if src in node_name_to_idx and dst in node_name_to_idx:\n",
    "                            constraint_matrix[node_name_to_idx[src], node_name_to_idx[dst]] = 0.0\n",
    "    \n",
    "    # Apply specific constraints (e.g., Gender → Service/Billing forbidden)\n",
    "    if specific_constraints:\n",
    "        for src, dst in specific_constraints.get(\"forbidden\", []):\n",
    "            if src in node_name_to_idx and dst in node_name_to_idx:\n",
    "                constraint_matrix[node_name_to_idx[src], node_name_to_idx[dst]] = 0.0\n",
    "        for src, dst in specific_constraints.get(\"allowed\", []):\n",
    "            if src in node_name_to_idx and dst in node_name_to_idx:\n",
    "                constraint_matrix[node_name_to_idx[src], node_name_to_idx[dst]] = np.nan\n",
    "    \n",
    "    logger.info(\"Constraint matrix created with shape: %s\", constraint_matrix.shape)\n",
    "    return constraint_matrix, node_name_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa9cf1f",
   "metadata": {},
   "source": [
    "## 2.2.2 validatation constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "528b87f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def validate_constraints(dag, node_name_to_idx, tiers, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Validate constraints on a DAG or adjacency matrix.\n",
    "    \n",
    "    Args:\n",
    "        dag: NetworkX DiGraph or np.ndarray (adjacency/probability matrix).\n",
    "        node_name_to_idx (dict): Mapping of node names to indices.\n",
    "        tiers (list): List of tier lists.\n",
    "        threshold (float): Probability threshold for matrix-based DAGs.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of constraint violation messages (empty if none).\n",
    "    \"\"\"\n",
    "    violations = []\n",
    "    num_nodes = len(node_name_to_idx)\n",
    "    churn_idx = node_name_to_idx.get(\"Churn Label\")\n",
    "    \n",
    "    # Convert adjacency matrix to DAG if needed\n",
    "    if isinstance(dag, np.ndarray):\n",
    "        G = nx.DiGraph()\n",
    "        G.add_nodes_from(range(num_nodes))\n",
    "        for i in range(num_nodes):\n",
    "            for j in range(num_nodes):\n",
    "                if dag[i, j] > threshold:\n",
    "                    G.add_edge(i, j)\n",
    "    else:\n",
    "        G = dag\n",
    "    \n",
    "    # Check if Churn Label has outgoing edges\n",
    "    if churn_idx is not None and any(G.has_edge(churn_idx, j) for j in range(num_nodes)):\n",
    "        violations.append(\"Churn Label has outgoing edges\")\n",
    "    \n",
    "    # Check if Tier 1 variables have incoming edges\n",
    "    for var in tiers[0]:\n",
    "        var_idx = node_name_to_idx.get(var)\n",
    "        if var_idx is not None and any(G.has_edge(j, var_idx) for j in range(num_nodes)):\n",
    "            violations.append(f\"{var} has incoming edges\")\n",
    "    \n",
    "    # Check for edges within Tier 1\n",
    "    for src in tiers[0]:\n",
    "        src_idx = node_name_to_idx.get(src)\n",
    "        for dst in tiers[0]:\n",
    "            dst_idx = node_name_to_idx.get(dst)\n",
    "            if src != dst and src_idx is not None and dst_idx is not None and G.has_edge(src_idx, dst_idx):\n",
    "                violations.append(f\"T1↛T1 edge: {src}→{dst}\")\n",
    "    \n",
    "    # Check for forbidden Gender → Service/Billing edges\n",
    "    gender_idx = node_name_to_idx.get(\"Gender\")\n",
    "    if gender_idx is not None:\n",
    "        for dst in tiers[2] + tiers[3]:  # Service + Billing\n",
    "            dst_idx = node_name_to_idx.get(dst)\n",
    "            if dst_idx is not None and G.has_edge(gender_idx, dst_idx):\n",
    "                violations.append(f\"Gender→{dst} edge exists\")\n",
    "    \n",
    "    if not violations:\n",
    "        logger.info(\"✅ All constraints validated successfully\")\n",
    "    else:\n",
    "        logger.warning(\"❌ Constraint violations detected: %s\", violations)\n",
    "    \n",
    "    return violations\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9406209a",
   "metadata": {},
   "source": [
    "## 2.2.3 save relations to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a34e9486",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_relations_to_text(dag, node_names, filename, threshold=0.2):\n",
    "    \"\"\"\n",
    "    Save causal relationships to a text file and print to console with improved formatting.\n",
    "    \n",
    "    Args:\n",
    "        dag: NetworkX DiGraph or np.ndarray (adjacency/probability matrix).\n",
    "        node_names (list): List of node names.\n",
    "        filename (str): Output text file name.\n",
    "        threshold (float): Probability/weight threshold for matrix-based DAGs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        relations = []\n",
    "        print(\"\\n=== Causal Relationships ===\")  # Header for console output\n",
    "        if isinstance(dag, np.ndarray):\n",
    "            for i in range(dag.shape[0]):\n",
    "                for j in range(dag.shape[1]):\n",
    "                    if dag[i, j] > threshold:\n",
    "                        relations.append({\n",
    "                            \"source\": node_names[i],\n",
    "                            \"destination\": node_names[j],\n",
    "                            \"weight\": float(dag[i, j])\n",
    "                        })\n",
    "                        # Improved console output with weight and separator\n",
    "                        print(f\"{node_names[i]} -> {node_names[j]} (weight: {dag[i, j]:.3f})\")\n",
    "                        print(\"---\")\n",
    "        else:\n",
    "            for src, dst in dag.edges():\n",
    "                weight = dag[src][dst].get(\"weight\", 1.0)\n",
    "                relations.append({\n",
    "                    \"source\": node_names[src],\n",
    "                    \"destination\": node_names[dst],\n",
    "                    \"weight\": float(weight)\n",
    "                })\n",
    "                # Improved console output with weight and separator\n",
    "                print(f\"{node_names[src]} -> {node_names[dst]} (weight: {weight:.3f})\")\n",
    "                print(\"---\")\n",
    "        \n",
    "        # Convert to DataFrame and save with better formatting\n",
    "        relations_df = pd.DataFrame(relations)\n",
    "        if not relations_df.empty:\n",
    "            # Write to file with a header and formatted entries\n",
    "            with open(filename, \"w\") as f:\n",
    "                f.write(\"Source -> Destination (Weight)\\n\")\n",
    "                f.write(\"=\" * 30 + \"\\n\")\n",
    "                for _, row in relations_df.iterrows():\n",
    "                    f.write(f\"{row['source']:<25} -> {row['destination']:<25} ({row['weight']:.3f})\\n\")\n",
    "                    f.write(\"-\" * 30 + \"\\n\")\n",
    "            logger.info(\"Causal relationships saved to %s (%d relations)\", filename, len(relations_df))\n",
    "        else:\n",
    "            with open(filename, \"w\") as f:\n",
    "                f.write(\"No causal relationships found.\\n\")\n",
    "            logger.warning(\"No causal relationships to save for %s\", filename)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(\"Failed to save relations to %s: %s\", filename, str(e))\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cf9ee2",
   "metadata": {},
   "source": [
    "## 2.2.4 visualize causal graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1b1a87a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_causal_graph(dag, node_names, filename=\"causal_graph.png\"):\n",
    "    \"\"\"\n",
    "    Visualize a causal graph with uniform edge lengths, focusing on nodes and relationships.\n",
    "    \n",
    "    Args:\n",
    "        dag: NetworkX DiGraph or np.ndarray (adjacency matrix).\n",
    "        node_names (list): List of node names.\n",
    "        filename (str): Output file name.\n",
    "    \n",
    "    Returns:\n",
    "        nx.DiGraph: Visualized graph.\n",
    "    \"\"\"\n",
    "    if isinstance(dag, np.ndarray):\n",
    "        G = nx.DiGraph(dag)\n",
    "    else:\n",
    "        G = dag\n",
    "\n",
    "    # Relabel nodes with names\n",
    "    G = nx.relabel_nodes(G, dict(enumerate(node_names)))\n",
    "\n",
    "    # Use circular layout for uniform edge lengths and clear node placement\n",
    "    plt.figure(figsize=(12, 10), dpi=300)\n",
    "    pos = nx.circular_layout(G)  # Changed to circular_layout for uniform edge lengths\n",
    "\n",
    "    # Draw the graph with uniform edges (no weight consideration)\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=1000, node_color=\"lightblue\", alpha=0.8)\n",
    "    nx.draw_networkx_edges(G, pos, width=1, alpha=0.6, arrowsize=15)  # Fixed width for all edges\n",
    "    nx.draw_networkx_labels(G, pos, font_size=8, font_weight=\"bold\")\n",
    "\n",
    "    plt.title(f\"Causal Graph ({filename.split('.')[0]})\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(filename, format=\"png\", bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    logger.info(\"Causal graph saved as %s\", filename)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89f49c1",
   "metadata": {},
   "source": [
    "## 2.2.5 evaluating the learned graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "396daf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_structure_learning(dag, node_names, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Analyze the structure of a learned DAG.\n",
    "    \n",
    "    Args:\n",
    "        dag: NetworkX DiGraph or np.ndarray (adjacency/probability matrix).\n",
    "        node_names (list): List of node names.\n",
    "        threshold (float): Probability threshold for matrix-based DAGs.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Structure learning metrics.\n",
    "    \"\"\"\n",
    "    if isinstance(dag, np.ndarray):\n",
    "        adj_matrix = (dag > threshold).astype(int)\n",
    "        G = nx.DiGraph(adj_matrix)\n",
    "    else:\n",
    "        G = dag\n",
    "        adj_matrix = nx.to_numpy_array(G, nodelist=range(len(node_names)))\n",
    "    \n",
    "    num_edges = np.sum(adj_matrix)\n",
    "    num_nodes = len(node_names)\n",
    "    graph_density = num_edges / (num_nodes * (num_nodes - 1)) if num_nodes > 1 else 0\n",
    "    in_degree = np.sum(adj_matrix, axis=0)\n",
    "    out_degree = np.sum(adj_matrix, axis=1)\n",
    "    avg_in_degree = np.mean(in_degree)\n",
    "    avg_out_degree = np.mean(out_degree)\n",
    "    \n",
    "    most_influential = [(node_names[i], out_degree[i]) for i in np.argsort(-out_degree)[:5]]\n",
    "    most_affected = [(node_names[i], in_degree[i]) for i in np.argsort(-in_degree)[:5]]\n",
    "    \n",
    "    metrics = {\n",
    "        \"num_edges\": num_edges,\n",
    "        \"graph_density\": graph_density,\n",
    "        \"avg_in_degree\": avg_in_degree,\n",
    "        \"avg_out_degree\": avg_out_degree,\n",
    "        \"most_influential\": most_influential,\n",
    "        \"most_affected\": most_affected\n",
    "    }\n",
    "    \n",
    "    logger.info(\"Structure Learning Metrics: %s\", metrics)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_causal_discovery(results, test_data, tiers, node_name_to_idx, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate causal discovery algorithms on test data, focusing on constraint violations.\n",
    "    \n",
    "    Args:\n",
    "        results (dict): Results from run_causal_discovery_pipeline.\n",
    "        test_data (pd.DataFrame): Test data.\n",
    "        tiers (list): List of tier lists.\n",
    "        node_name_to_idx (dict): Mapping of node names to indices.\n",
    "        threshold (float): Probability threshold for matrix-based DAGs.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Evaluation metrics for each algorithm.\n",
    "    \"\"\"\n",
    "    logger.info(\"Evaluating constraint violations on test data...\")\n",
    "    evaluation_metrics = {}\n",
    "    \n",
    "    # Preprocess test data to handle issues\n",
    "    constant_cols = [col for col in test_data.columns if test_data[col].std() == 0]\n",
    "    if constant_cols:\n",
    "        logger.warning(\"Constant columns in test data: %s\", constant_cols)\n",
    "        test_data = test_data.drop(columns=constant_cols)\n",
    "    if test_data.isna().any().any():\n",
    "        logger.warning(\"NaNs in test data, filling with mean\")\n",
    "        test_data = test_data.fillna(test_data.mean())\n",
    "    \n",
    "    for algo_name, result in results.items():\n",
    "        if \"error\" in result:\n",
    "            evaluation_metrics[algo_name] = {\n",
    "                \"constraint_violations\": \"N/A\",\n",
    "                \"violation_details\": \"N/A\"\n",
    "            }\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            dag = result[\"dag\"]\n",
    "            adj_matrix = result[\"adj_matrix\"]\n",
    "            \n",
    "            # Adjust node_name_to_idx for LiNGAM (uses subset of features)\n",
    "            algo_node_name_to_idx = node_name_to_idx\n",
    "            if algo_name == \"LiNGAM\":\n",
    "                continuous_features = [f for f in test_data.columns if f in [\"Age\", \"Number of Dependents\", \"Number of Referrals\", \"Tenure in Months\", \"Total Revenue\"]]\n",
    "                algo_node_name_to_idx = {name: i for i, name in enumerate(continuous_features)}\n",
    "                # Update tiers for LiNGAM\n",
    "                lingam_tiers = [\n",
    "                    [f for f in tiers[0] if f in continuous_features],\n",
    "                    [f for f in tiers[1] if f in continuous_features],\n",
    "                    [f for f in tiers[3] if f in continuous_features]\n",
    "                ]\n",
    "            else:\n",
    "                lingam_tiers = tiers\n",
    "            \n",
    "            # Validate constraints\n",
    "            violations = validate_constraints(dag if algo_name != \"LiNGAM\" else adj_matrix, \n",
    "                                           algo_node_name_to_idx, lingam_tiers, threshold)\n",
    "            \n",
    "            evaluation_metrics[algo_name] = {\n",
    "                \"constraint_violations\": len(violations),\n",
    "                \"violation_details\": violations if violations else \"None\"\n",
    "            }\n",
    "            \n",
    "            logger.info(\"%s: %d constraint violations on test data: %s\",\n",
    "                        algo_name, len(violations), violations if violations else \"None\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(\"Evaluation failed for %s: %s\", algo_name, str(e))\n",
    "            evaluation_metrics[algo_name] = {\n",
    "                \"constraint_violations\": \"N/A\",\n",
    "                \"violation_details\": \"N/A\"\n",
    "            }\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary = pd.DataFrame(evaluation_metrics).T\n",
    "    logger.info(\"Constraint violation summary:\\n%s\", summary)\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319eaaf1",
   "metadata": {},
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cc1c77",
   "metadata": {},
   "source": [
    "## 2.3.1. CausalDiscoveryAlgorithm Base Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1fa20b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalDiscoveryAlgorithm(ABC):\n",
    "    \"\"\"Base class for causal discovery algorithms.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def fit(self, data, constraint_matrix, node_names, node_name_to_idx, tiers, output_dir):\n",
    "        \"\"\"\n",
    "        Fit the causal discovery algorithm.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): Input data.\n",
    "            constraint_matrix (np.ndarray): Constraint matrix.\n",
    "            node_names (list): List of node names.\n",
    "            node_name_to_idx (dict): Mapping of node names to indices.\n",
    "            tiers (list): List of tier lists.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Results including DAG, adjacency matrix, metrics, and violations.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0cab8f",
   "metadata": {},
   "source": [
    "## 2.3.2. DECI Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1f272408",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DECIAlgorithm(CausalDiscoveryAlgorithm):\n",
    "    def fit(self, data, constraint_matrix, node_names, node_name_to_idx, tiers, output_dir):\n",
    "        logger.info(\"Running DECI algorithm...\")\n",
    "        try:\n",
    "            # Load variables.json\n",
    "            with fsspec.open(\"data/variables.json\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "                variables = json.load(f)[\"variables\"]\n",
    "            \n",
    "            # Validate columns\n",
    "            expected_columns = [var[\"name\"] for var in variables]\n",
    "            if set(expected_columns) != set(data.columns):\n",
    "                raise ValueError(f\"Columns mismatch: {set(data.columns)} vs {set(expected_columns)}\")\n",
    "            \n",
    "            # Prepare data module\n",
    "            data_module = BasicDECIDataModule(\n",
    "                data,\n",
    "                variables=[Variable.from_dict(d) for d in variables],\n",
    "                batch_size=128,\n",
    "                normalize=True\n",
    "            )\n",
    "            \n",
    "            # Initialize DECI module\n",
    "            lightning_module = DECIModule(\n",
    "                noise_dist=ContinuousNoiseDist.GAUSSIAN,\n",
    "                prior_sparsity_lambda=100.0,\n",
    "                init_rho=30.0,\n",
    "                init_alpha=0.20,\n",
    "                auglag_config=AugLagLRConfig(\n",
    "                    max_inner_steps=1500,\n",
    "                    max_outer_steps=8,\n",
    "                    lr_init_dict={\n",
    "                        \"icgnn\": 0.00076,\n",
    "                        \"vardist\": 0.0098,\n",
    "                        \"functional_relationships\": 3e-4,\n",
    "                        \"noise_dist\": 0.0070,\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "            lightning_module.constraint_matrix = torch.tensor(constraint_matrix)\n",
    "            \n",
    "            # Train\n",
    "            trainer = pl.Trainer(\n",
    "                accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "                devices=1,\n",
    "                max_epochs=10,\n",
    "                callbacks=[TQDMProgressBar(refresh_rate=19)],\n",
    "                enable_checkpointing=False\n",
    "            )\n",
    "            trainer.fit(lightning_module, datamodule=data_module)\n",
    "            \n",
    "            # Save model\n",
    "            torch.save(lightning_module.sem_module, \"deci.pt\")\n",
    "            \n",
    "            # Compute probability matrix\n",
    "            logits_exist = lightning_module.sem_module.adjacency_module.adjacency_distribution.logits_exist\n",
    "            logits_orient = lightning_module.sem_module.adjacency_module.adjacency_distribution.logits_orient\n",
    "            \n",
    "            def fill_triangular(vec, upper=False):\n",
    "                n = int(np.sqrt(2 * len(vec))) + 1\n",
    "                if upper:\n",
    "                    return vec.new_zeros(n, n).triu(1).masked_scatter_(\n",
    "                        torch.triu(torch.ones(n, n, device=vec.device), 1).bool(), vec\n",
    "                    )\n",
    "                return vec.new_zeros(n, n).tril(-1).masked_scatter_(\n",
    "                        torch.tril(torch.ones(n, n, device=vec.device), -1).bool(), vec\n",
    "                    )\n",
    "            \n",
    "            neg_theta = fill_triangular(logits_orient, upper=True) - fill_triangular(logits_orient, upper=False)\n",
    "            logits_matrix = -torch.logsumexp(torch.stack([-logits_exist, neg_theta, neg_theta - logits_exist], dim=-1), dim=-1)\n",
    "            prob_matrix = 1 / (1 + np.exp(-logits_matrix.cpu().detach().numpy()))\n",
    "            prob_matrix = prob_matrix * np.isnan(constraint_matrix)\n",
    "            \n",
    "            # Validate constraints\n",
    "            violations = validate_constraints(prob_matrix, node_name_to_idx, tiers)\n",
    "            \n",
    "            # Create a directed graph for visualization\n",
    "            G = nx.DiGraph()\n",
    "            for i in range(len(node_names)):\n",
    "                G.add_node(i)\n",
    "            for i in range(len(node_names)):\n",
    "                for j in range(len(node_names)):\n",
    "                    if prob_matrix[i, j] > 0.5:  # Threshold for edge existence\n",
    "                        G.add_edge(i, j)\n",
    "            \n",
    "            # Visualize and save results\n",
    "            G_viz = visualize_causal_graph(G, node_names, os.path.join(output_dir, \"deci_graph.png\"))\n",
    "            save_relations_to_text(prob_matrix, node_names, os.path.join(output_dir, \"deci_relations.txt\"))\n",
    "            \n",
    "            # Analyze structure\n",
    "            metrics = analyze_structure_learning(prob_matrix, node_names)\n",
    "            \n",
    "            return {\n",
    "                \"dag\": G,\n",
    "                \"adj_matrix\": prob_matrix,\n",
    "                \"metrics\": metrics,\n",
    "                \"violations\": violations\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(\"DECI failed: %s\", str(e))\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd960ec",
   "metadata": {},
   "source": [
    "## 2.3.3. LiNGAM Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5af9b564",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LiNGAMAlgorithm(CausalDiscoveryAlgorithm):\n",
    "    def fit(self, data, constraint_matrix, node_names, node_name_to_idx, tiers, output_dir):\n",
    "        logger.info(\"Running LiNGAM algorithm...\")\n",
    "        try:\n",
    "            # Filter continuous features\n",
    "            continuous_features = [f for f in node_names if f in [\"Age\", \"Number of Dependents\", \"Number of Referrals\", \"Tenure in Months\", \"Total Revenue\"]]\n",
    "            lingam_data = data[continuous_features].copy()\n",
    "            \n",
    "            # Remove constant columns\n",
    "            constant_cols = [col for col in lingam_data.columns if lingam_data[col].std() == 0]\n",
    "            if constant_cols:\n",
    "                logger.warning(\"Removing constant columns: %s\", constant_cols)\n",
    "                lingam_data = lingam_data.drop(columns=constant_cols)\n",
    "                continuous_features = [f for f in continuous_features if f not in constant_cols]\n",
    "            \n",
    "            # Validate data\n",
    "            if lingam_data.isna().any().any():\n",
    "                raise ValueError(\"LiNGAM data contains NaNs\")\n",
    "            \n",
    "            # Create LiNGAM-specific constraint matrix\n",
    "            lingam_node_to_idx = {name: i for i, name in enumerate(continuous_features)}\n",
    "            lingam_constraint_matrix = np.full((len(continuous_features), len(continuous_features)), -1, dtype=np.int32)\n",
    "            \n",
    "            lingam_tiers = [\n",
    "                [f for f in tiers[0] if f in continuous_features],  # Demographic\n",
    "                [f for f in tiers[1] if f in continuous_features],  # Customer\n",
    "                [f for f in tiers[3] if f in continuous_features]   # Billing\n",
    "            ]\n",
    "            \n",
    "            if \"Total Revenue\" in lingam_node_to_idx:\n",
    "                lingam_constraint_matrix[lingam_node_to_idx[\"Total Revenue\"], :] = 0\n",
    "            for feature in lingam_tiers[0]:\n",
    "                lingam_constraint_matrix[:, lingam_node_to_idx[feature]] = 0\n",
    "            for src_tier_idx, src_tier in enumerate(lingam_tiers[:-1]):\n",
    "                dst_tier = lingam_tiers[src_tier_idx + 1]\n",
    "                for src in src_tier:\n",
    "                    for dst in dst_tier:\n",
    "                        lingam_constraint_matrix[lingam_node_to_idx[src], lingam_node_to_idx[dst]] = -1\n",
    "                for other_tier_idx, other_tier in enumerate(lingam_tiers):\n",
    "                    if other_tier_idx != src_tier_idx + 1:\n",
    "                        for src in src_tier:\n",
    "                            for dst in other_tier:\n",
    "                                lingam_constraint_matrix[lingam_node_to_idx[src], lingam_node_to_idx[dst]] = 0\n",
    "            for src in lingam_tiers[0]:\n",
    "                for dst in lingam_tiers[0]:\n",
    "                    if src != dst:\n",
    "                        lingam_constraint_matrix[lingam_node_to_idx[src], lingam_node_to_idx[dst]] = 0\n",
    "            \n",
    "            # Fit LiNGAM\n",
    "            model = lingam.DirectLiNGAM(prior_knowledge=lingam_constraint_matrix)\n",
    "            model.fit(lingam_data)\n",
    "            \n",
    "            adj_matrix = model.adjacency_matrix_\n",
    "            \n",
    "            # Validate constraints\n",
    "            violations = validate_constraints(adj_matrix, lingam_node_to_idx, lingam_tiers)\n",
    "            \n",
    "            # Create a directed graph for visualization\n",
    "            G = nx.DiGraph()\n",
    "            for i in range(len(continuous_features)):\n",
    "                G.add_node(i)\n",
    "            for i in range(len(continuous_features)):\n",
    "                for j in range(len(continuous_features)):\n",
    "                    if adj_matrix[i, j] != 0:\n",
    "                        G.add_edge(i, j)\n",
    "                        \n",
    "            # Visualize and save results\n",
    "            G_viz = visualize_causal_graph(G, continuous_features, os.path.join(output_dir, \"lingam_graph.png\"))\n",
    "            save_relations_to_text(adj_matrix, continuous_features, os.path.join(output_dir, \"lingam_relations.txt\"))\n",
    "            \n",
    "            # Analyze structure\n",
    "            metrics = analyze_structure_learning(adj_matrix, continuous_features)\n",
    "            \n",
    "            return {\n",
    "                \"dag\": G,\n",
    "                \"adj_matrix\": adj_matrix,\n",
    "                \"metrics\": metrics,\n",
    "                \"violations\": violations\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(\"LiNGAM failed: %s\", str(e))\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7cbe6d",
   "metadata": {},
   "source": [
    "## 2.3.4 NOTEARS Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2c909ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NOTEARSAlgorithm(CausalDiscoveryAlgorithm):\n",
    "    def fit(self, data, constraint_matrix, node_names, node_name_to_idx, tiers, output_dir):\n",
    "        logger.info(\"Running NOTEARS algorithm...\")\n",
    "        try:\n",
    "            X = data.values\n",
    "            stds = np.std(X, axis=0)\n",
    "            means = np.mean(X, axis=0)\n",
    "            X_standardized = np.where(stds != 0, (X - means) / stds, 0)\n",
    "            \n",
    "            def notears_with_constraints(X, constraint_matrix, lambda1=0.01, max_iter=200, h_tol=1e-8, rho_max=1e+16, w_threshold=0.1):\n",
    "                n, d = X.shape\n",
    "                mask = 1.0 - np.isnan(constraint_matrix).astype(float)\n",
    "                \n",
    "                def _h(w):\n",
    "                    W = w.reshape((d, d))\n",
    "                    M = np.eye(d) + W * W / d\n",
    "                    return np.trace(slin.expm(M)) - d\n",
    "                \n",
    "                def _func(w):\n",
    "                    W = w.reshape((d, d))\n",
    "                    W = W * (1.0 - mask)\n",
    "                    R = X - X @ W\n",
    "                    loss = 0.5 / n * np.sum(R * R)\n",
    "                    l1_penalty = lambda1 * np.sum(np.abs(W))\n",
    "                    return loss + l1_penalty\n",
    "                \n",
    "                def _grad(w):\n",
    "                    W = w.reshape((d, d))\n",
    "                    W = W * (1.0 - mask)\n",
    "                    R = X - X @ W\n",
    "                    G = -1.0 / n * X.T @ R\n",
    "                    G_l1 = lambda1 * np.sign(W)\n",
    "                    G = (G + G_l1) * (1.0 - mask)\n",
    "                    return G.flatten()\n",
    "                \n",
    "                def _h_grad(w):\n",
    "                    W = w.reshape((d, d))\n",
    "                    M = np.eye(d) + W * W / d\n",
    "                    E = slin.expm(M)\n",
    "                    G = E.T * (2 * W / d)\n",
    "                    G = G * (1.0 - mask)\n",
    "                    return G.flatten()\n",
    "                \n",
    "                w_est = np.zeros(d * d)\n",
    "                rho, alpha, h = 1.0, 0.0, np.inf\n",
    "                for _ in range(max_iter):\n",
    "                    w_new = sopt.minimize(\n",
    "                        lambda w: _func(w) + 0.5 * rho * _h(w) ** 2 + alpha * _h(w),\n",
    "                        w_est,\n",
    "                        method='L-BFGS-B',\n",
    "                        jac=lambda w: _grad(w) + rho * _h(w) * _h_grad(w) + alpha * _h_grad(w),\n",
    "                        options={'ftol': 1e-6, 'gtol': 1e-6}\n",
    "                    ).x\n",
    "                    h_new = _h(w_new)\n",
    "                    if abs(h_new) <= h_tol or rho >= rho_max:\n",
    "                        break\n",
    "                    if abs(h_new) > 0.25 * abs(h):\n",
    "                        rho *= 10\n",
    "                    alpha += rho * h_new\n",
    "                    w_est, h = w_new, h_new\n",
    "                \n",
    "                W_est = w_est.reshape((d, d))\n",
    "                W_est = W_est * (1.0 - mask)\n",
    "                W_est[np.abs(W_est) < w_threshold] = 0\n",
    "                \n",
    "                G = nx.DiGraph(W_est)\n",
    "                while not nx.is_directed_acyclic_graph(G):\n",
    "                    try:\n",
    "                        cycle = nx.find_cycle(G)\n",
    "                        min_weight = float('inf')\n",
    "                        min_edge = None\n",
    "                        for u, v in cycle:\n",
    "                            if abs(W_est[u, v]) < min_weight:\n",
    "                                min_weight = abs(W_est[u, v])\n",
    "                                min_edge = (u, v)\n",
    "                        if min_edge:\n",
    "                            G.remove_edge(*min_edge)\n",
    "                            W_est[min_edge[0], min_edge[1]] = 0\n",
    "                    except nx.NetworkXNoCycle:\n",
    "                        break\n",
    "                \n",
    "                return W_est\n",
    "            \n",
    "            adj_matrix = notears_with_constraints(X_standardized, constraint_matrix, lambda1=0.01)\n",
    "            \n",
    "            # Validate constraints\n",
    "            violations = validate_constraints(adj_matrix, node_name_to_idx, tiers)\n",
    "            \n",
    "            # Create a directed graph for visualization\n",
    "            G = nx.DiGraph(adj_matrix)\n",
    "            \n",
    "            # Visualize and save results\n",
    "            G_viz = visualize_causal_graph(G, node_names, os.path.join(output_dir, \"notears_graph.png\"))\n",
    "            save_relations_to_text(adj_matrix, node_names, os.path.join(output_dir, \"notears_relations.txt\"))\n",
    "            \n",
    "            # Analyze structure\n",
    "            metrics = analyze_structure_learning(adj_matrix, node_names)\n",
    "            \n",
    "            return {\n",
    "                \"dag\": G,\n",
    "                \"adj_matrix\": adj_matrix,\n",
    "                \"metrics\": metrics,\n",
    "                \"violations\": violations\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(\"NOTEARS failed: %s\", str(e))\n",
    "            raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0d4f46",
   "metadata": {},
   "source": [
    "## 2.3.5 PC-Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "db01471f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PCGINAlgorithm(CausalDiscoveryAlgorithm):\n",
    "    def fit(self, data, constraint_matrix, node_names, node_name_to_idx, tiers, output_dir):\n",
    "        logger.info(\"Running PC-GIN algorithm...\")\n",
    "        try:\n",
    "            # Encode categorical columns\n",
    "            categorical_cols = [col for col in data.columns if col in [\"Gender\", \"Internet Type\", \"Offer\", \"Payment Method\"]]\n",
    "            encoded_data = data.copy()\n",
    "            for col in categorical_cols:\n",
    "                le = LabelEncoder()\n",
    "                encoded_data[col] = le.fit_transform(encoded_data[col].astype(str))\n",
    "            \n",
    "            def gin_test(X, Y, Z=None, alpha=0.05):\n",
    "                n = len(X)\n",
    "                if Z is None or Z.shape[1] == 0:\n",
    "                    corr, p_value = stats.pearsonr(X, Y)\n",
    "                    return p_value\n",
    "                model_x = LinearRegression().fit(Z, X)\n",
    "                residuals_x = X - model_x.predict(Z)\n",
    "                model_y = LinearRegression().fit(Z, Y)\n",
    "                residuals_y = Y - model_y.predict(Z)\n",
    "                corr, p_value = stats.pearsonr(residuals_x, residuals_y)\n",
    "                return p_value\n",
    "            \n",
    "            def pc_gin(data, constraint_matrix, alpha=0.01):\n",
    "                n = data.shape[1]\n",
    "                skeleton = nx.Graph()\n",
    "                skeleton.add_nodes_from(range(n))\n",
    "                separating_sets = {}\n",
    "                \n",
    "                for i in range(n):\n",
    "                    for j in range(i + 1, n):\n",
    "                        if np.isnan(constraint_matrix[i, j]) or np.isnan(constraint_matrix[j, i]):\n",
    "                            skeleton.add_edge(i, j)\n",
    "                \n",
    "                for d in range(n):\n",
    "                    edges = list(skeleton.edges())\n",
    "                    for i, j in edges:\n",
    "                        if not skeleton.has_edge(i, j):\n",
    "                            continue\n",
    "                        adj_i = set(skeleton.neighbors(i)) - {j}\n",
    "                        if len(adj_i) >= d:\n",
    "                            for subset in itertools.combinations(adj_i, d):\n",
    "                                subset_list = list(subset)\n",
    "                                conditioning_set = data[:, subset_list] if subset_list else None\n",
    "                                p_val = gin_test(\n",
    "                                    data[:, i], data[:, j],\n",
    "                                    conditioning_set.reshape(data.shape[0], -1) if conditioning_set is not None else None,\n",
    "                                    alpha=alpha\n",
    "                                )\n",
    "                                if p_val > alpha:\n",
    "                                    skeleton.remove_edge(i, j)\n",
    "                                    separating_sets[(i, j)] = subset\n",
    "                                    separating_sets[(j, i)] = subset\n",
    "                                    break\n",
    "                \n",
    "                dag = nx.DiGraph()\n",
    "                dag.add_nodes_from(range(n))\n",
    "                for i, j in skeleton.edges():\n",
    "                    if np.isnan(constraint_matrix[i, j]) and not np.isnan(constraint_matrix[j, i]):\n",
    "                        dag.add_edge(i, j)\n",
    "                    elif np.isnan(constraint_matrix[j, i]) and not np.isnan(constraint_matrix[i, j]):\n",
    "                        dag.add_edge(j, i)\n",
    "                    else:\n",
    "                        dag.add_edge(i, j)\n",
    "                        dag.add_edge(j, i)\n",
    "                \n",
    "                for i in range(n):\n",
    "                    for j in range(n):\n",
    "                        if i == j or not dag.has_edge(i, j):\n",
    "                            continue\n",
    "                        for k in range(n):\n",
    "                            if k == i or k == j:\n",
    "                                continue\n",
    "                            if dag.has_edge(k, j) and not skeleton.has_edge(i, k):\n",
    "                                if ((i, k) in separating_sets and j not in separating_sets[(i, k)]) or \\\n",
    "                                   ((k, i) in separating_sets and j not in separating_sets[(k, i)]):\n",
    "                                    if dag.has_edge(j, i):\n",
    "                                        dag.remove_edge(j, i)\n",
    "                                    if dag.has_edge(j, k):\n",
    "                                        dag.remove_edge(j, k)\n",
    "                \n",
    "                for i, j in list(dag.edges()):\n",
    "                    if dag.has_edge(j, i):\n",
    "                        if np.isnan(constraint_matrix[i, j]) and not np.isnan(constraint_matrix[j, i]):\n",
    "                            dag.remove_edge(j, i)\n",
    "                        elif np.isnan(constraint_matrix[j, i]) and not np.isnan(constraint_matrix[i, j]):\n",
    "                            dag.remove_edge(i, j)\n",
    "                        else:\n",
    "                            dag.remove_edge(j, i)\n",
    "                \n",
    "                return dag\n",
    "            \n",
    "            dag = pc_gin(encoded_data.values, constraint_matrix, alpha=0.05)\n",
    "            \n",
    "            # Create adjacency matrix from DAG\n",
    "            adj_matrix = nx.to_numpy_array(dag, nodelist=range(len(node_names)))\n",
    "            \n",
    "            # Validate constraints\n",
    "            violations = validate_constraints(dag, node_name_to_idx, tiers)\n",
    "            \n",
    "            # Visualize and save results\n",
    "            G_viz = visualize_causal_graph(dag, node_names, os.path.join(output_dir, \"pcgin_graph.png\"))\n",
    "            save_relations_to_text(dag, node_names, os.path.join(output_dir, \"pcgin_relations.txt\"))\n",
    "            \n",
    "            # Analyze structure\n",
    "            metrics = analyze_structure_learning(dag, node_names)\n",
    "            \n",
    "            return {\n",
    "                \"dag\": dag,\n",
    "                \"adj_matrix\": adj_matrix,\n",
    "                \"metrics\": metrics,\n",
    "                \"violations\": violations\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(\"PC-GIN failed: %s\", str(e))\n",
    "            raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a41a02",
   "metadata": {},
   "source": [
    "## 2.3.6 GRaSPAlgorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f8829cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRaSPAlgorithm(CausalDiscoveryAlgorithm):\n",
    "    def fit(self, data, constraint_matrix, node_names, node_name_to_idx, tiers, output_dir):\n",
    "        logger.info(\"Running GRaSP algorithm...\")\n",
    "        try:\n",
    "            X = data.values\n",
    "            stds = np.std(X, axis=0)\n",
    "            means = np.mean(X, axis=0)\n",
    "            X_standardized = np.where(stds != 0, (X - means) / stds, 0)\n",
    "            \n",
    "            def grasp_with_constraints(X, constraint_matrix, lambda1=0.01, max_iter=200, h_tol=1e-8, rho_max=1e+16, w_threshold=0.1):\n",
    "                n, d = X.shape\n",
    "                mask = 1.0 - np.isnan(constraint_matrix).astype(float)\n",
    "                \n",
    "                def _h(w):\n",
    "                    W = w.reshape((d, d))\n",
    "                    M = np.eye(d) + W * W / d\n",
    "                    return np.trace(slin.expm(M)) - d\n",
    "                \n",
    "                def _func(w, rho, alpha):\n",
    "                    W = w.reshape((d, d))\n",
    "                    W = W * (1.0 - mask)\n",
    "                    R = X - X @ W\n",
    "                    loss = 0.5 / n * np.sum(R * R)\n",
    "                    l1_penalty = lambda1 * np.sum(np.abs(W))\n",
    "                    h_val = _h(w)\n",
    "                    return loss + l1_penalty + 0.5 * rho * h_val ** 2 + alpha * h_val\n",
    "                \n",
    "                def _grad(w, rho, alpha):\n",
    "                    W = w.reshape((d, d))\n",
    "                    W = W * (1.0 - mask)\n",
    "                    R = X - X @ W\n",
    "                    G_loss = -1.0 / n * X.T @ R\n",
    "                    G_l1 = lambda1 * np.sign(W)\n",
    "                    h_val = _h(w)\n",
    "                    h_gradient = _h_grad(w).reshape((d, d))\n",
    "                    G_acyclicity = (rho * h_val + alpha) * h_gradient\n",
    "                    G = (G_loss + G_l1 + G_acyclicity) * (1.0 - mask)\n",
    "                    return G.flatten()\n",
    "                \n",
    "                def _h_grad(w):\n",
    "                    W = w.reshape((d, d))\n",
    "                    M = np.eye(d) + W * W / d\n",
    "                    E = slin.expm(M)\n",
    "                    G = E.T * (2 * W / d)\n",
    "                    G = G * (1.0 - mask)\n",
    "                    return G.flatten()\n",
    "                \n",
    "                w_est = np.zeros(d * d)\n",
    "                rho, alpha, h = 1.0, 0.0, np.inf\n",
    "                for _ in range(max_iter):\n",
    "                    w_new = sopt.minimize(\n",
    "                        lambda w: _func(w, rho, alpha),\n",
    "                        w_est,\n",
    "                        method='L-BFGS-B',\n",
    "                        jac=lambda w: _grad(w, rho, alpha),\n",
    "                        options={'ftol': 1e-6, 'gtol': 1e-6}\n",
    "                    ).x\n",
    "                    h_new = _h(w_new)\n",
    "                    if abs(h_new) <= h_tol or rho >= rho_max:\n",
    "                        break\n",
    "                    if abs(h_new) > 0.25 * abs(h):\n",
    "                        rho *= 10\n",
    "                    alpha += rho * h_new\n",
    "                    w_est, h = w_new, h_new\n",
    "                \n",
    "                W_est = w_est.reshape((d, d))\n",
    "                W_est = W_est * (1.0 - mask)\n",
    "                W_est[np.abs(W_est) < w_threshold] = 0\n",
    "                \n",
    "                G = nx.DiGraph(W_est)\n",
    "                while not nx.is_directed_acyclic_graph(G):\n",
    "                    try:\n",
    "                        cycle = nx.find_cycle(G)\n",
    "                        min_weight = float('inf')\n",
    "                        min_edge = None\n",
    "                        for u, v in cycle:\n",
    "                            if abs(W_est[u, v]) < min_weight:\n",
    "                                min_weight = abs(W_est[u, v])\n",
    "                                min_edge = (u, v)\n",
    "                        if min_edge:\n",
    "                            G.remove_edge(*min_edge)\n",
    "                            W_est[min_edge[0], min_edge[1]] = 0\n",
    "                    except nx.NetworkXNoCycle:\n",
    "                        break\n",
    "                \n",
    "                return W_est\n",
    "            \n",
    "            adj_matrix = grasp_with_constraints(X_standardized, constraint_matrix)\n",
    "            \n",
    "            # Validate constraints\n",
    "            violations = validate_constraints(adj_matrix, node_name_to_idx, tiers)\n",
    "            \n",
    "            # Create a directed graph for visualization\n",
    "            G = nx.DiGraph(adj_matrix)\n",
    "            \n",
    "            # Visualize and save results\n",
    "            G_viz = visualize_causal_graph(G, node_names, os.path.join(output_dir, \"grasp_graph.png\"))\n",
    "            save_relations_to_text(adj_matrix, node_names, os.path.join(output_dir, \"grasp_relations.txt\"))\n",
    "            \n",
    "            # Analyze structure\n",
    "            metrics = analyze_structure_learning(adj_matrix, node_names)\n",
    "            \n",
    "            return {\n",
    "                \"dag\": G,\n",
    "                \"adj_matrix\": adj_matrix,\n",
    "                \"metrics\": metrics,\n",
    "                \"violations\": violations\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(\"GRaSP failed: %s\", str(e))\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7612161",
   "metadata": {},
   "source": [
    "## 2.4.1. run_causal_discovery_pipeline Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "208f3cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def run_causal_discovery_pipeline(train_data, tiers, specific_constraints=None, output_dir=\"causal_discovery_output\"):\n",
    "    \"\"\"\n",
    "    Run the causal discovery pipeline for all algorithms.\n",
    "    \n",
    "    Args:\n",
    "        train_data (pd.DataFrame): Training data.\n",
    "        tiers (list): List of tier lists.\n",
    "        specific_constraints (dict): Additional constraints.\n",
    "        output_dir (str): Directory to save output files.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results for each algorithm.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting causal discovery pipeline...\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create constraint matrix\n",
    "    node_names = list(train_data.columns)\n",
    "    constraint_matrix, node_name_to_idx = create_constraint_matrix(node_names, tiers, specific_constraints)\n",
    "    \n",
    "    # Initialize algorithms\n",
    "    algorithms = {\n",
    "        \"DECI\": DECIAlgorithm(),\n",
    "        \"LiNGAM\": LiNGAMAlgorithm(),\n",
    "        \"PC-GIN\": PCGINAlgorithm(),\n",
    "        \"NOTEARS\": NOTEARSAlgorithm(),\n",
    "        \"GRaSP\": GRaSPAlgorithm()\n",
    "    }\n",
    "    \n",
    "    # Run algorithms\n",
    "    results = {}\n",
    "    for algo_name, algo in algorithms.items():\n",
    "        try:\n",
    "            logger.info(\"Executing %s...\", algo_name)\n",
    "            print(f\"\\n{algo_name} Causal Relationships:\")\n",
    "            result = algo.fit(train_data, constraint_matrix, node_names, node_name_to_idx, tiers, output_dir)\n",
    "            results[algo_name] = result\n",
    "        except Exception as e:\n",
    "            logger.error(\"%s failed: %s\", algo_name, str(e))\n",
    "            results[algo_name] = {\"error\": str(e)}\n",
    "    \n",
    "    # Summarize results\n",
    "    summary = pd.DataFrame({\n",
    "        algo_name: {\n",
    "            \"num_edges\": result[\"metrics\"][\"num_edges\"] if \"metrics\" in result else \"N/A\",\n",
    "            \"graph_density\": result[\"metrics\"][\"graph_density\"] if \"metrics\" in result else \"N/A\",\n",
    "            \"violations\": len(result[\"violations\"]) if \"violations\" in result else \"N/A\"\n",
    "        } for algo_name, result in results.items()\n",
    "    }).T\n",
    "    logger.info(\"Summary of results:\\n%s\", summary)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25164617",
   "metadata": {},
   "source": [
    "## 2.4.2. Tiers Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5d0c14e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tiers and specific constraints\n",
    "tiers = [\n",
    "    [\"Gender\", \"Age\", \"Senior Citizen\", \"Married\", \"Number of Dependents\"],  # Tier 1: Demographic\n",
    "    [\"Number of Referrals\", \"Tenure in Months\", \"Offer\"],                    # Tier 2: Customer\n",
    "    [\"Phone Service\", \"Multiple Lines\", \"Internet Type\", \"Unlimited Data\",\n",
    "     \"Online Security\", \"Online Backup\", \"Device Protection Plan\",\n",
    "     \"Premium Tech Support\", \"Streaming TV\", \"Streaming Movies\", \"Streaming Music\"],  # Tier 3: Service\n",
    "    [\"Total Revenue\", \"Paperless Billing\", \"Payment Method\"],                # Tier 4: Billing\n",
    "    [\"Churn Label\"]                                                         # Tier 5: Outcome\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a043ba95",
   "metadata": {},
   "source": [
    "## 2.4.3. Specific Constraints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e88a6150",
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_constraints = {\n",
    "    \"forbidden\": [\n",
    "        (\"Gender\", dst) for dst in tiers[2] + tiers[3]\n",
    "    ] + [\n",
    "        (\"Internet Type\", dst) for dst in [\"Unlimited Data\", \"Online Security\", \"Online Backup\",\n",
    "                                          \"Device Protection Plan\", \"Premium Tech Support\",\n",
    "                                          \"Streaming TV\", \"Streaming Movies\", \"Streaming Music\"]\n",
    "    ],\n",
    "    \"allowed\": []\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d8e9b1",
   "metadata": {},
   "source": [
    "## 2.4.4. Pipeline Execution\n",
    "## 2.4.5. Constraint Violations Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "59e0bf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting causal discovery pipeline...\n",
      "INFO:__main__:Constraint matrix created with shape: (23, 23)\n",
      "INFO:__main__:Executing DECI...\n",
      "INFO:__main__:Running DECI algorithm...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type                  | Params\n",
      "------------------------------------------------------\n",
      "0 | auglag_loss | AugLagLossCalculator  | 0     \n",
      "1 | sem_module  | SEMDistributionModule | 36.7 K\n",
      "------------------------------------------------------\n",
      "36.2 K    Trainable params\n",
      "529       Non-trainable params\n",
      "36.7 K    Total params\n",
      "0.147     Total estimated model params size (MB)\n",
      "c:\\Users\\aafz1\\miniconda3\\envs\\project-env\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\aafz1\\miniconda3\\envs\\project-env\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:293: The number of training batches (44) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DECI Causal Relationships:\n",
      "Epoch 9: 100%|██████████| 44/44 [00:02<00:00, 20.48it/s, v_num=11]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 44/44 [00:02<00:00, 20.33it/s, v_num=11]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:✅ All constraints validated successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Causal graph saved as causal_discovery_output\\deci_graph.png\n",
      "INFO:__main__:Causal relationships saved to causal_discovery_output\\deci_relations.txt (25 relations)\n",
      "INFO:__main__:Structure Learning Metrics: {'num_edges': 17, 'graph_density': 0.03359683794466403, 'avg_in_degree': 0.7391304347826086, 'avg_out_degree': 0.7391304347826086, 'most_influential': [('Tenure in Months', 8), ('Married', 2), ('Streaming TV', 1), ('Premium Tech Support', 1), ('Multiple Lines', 1)], 'most_affected': [('Total Revenue', 6), ('Paperless Billing', 1), ('Streaming Music', 1), ('Streaming Movies', 1), ('Number of Referrals', 1)]}\n",
      "INFO:__main__:Executing LiNGAM...\n",
      "INFO:__main__:Running LiNGAM algorithm...\n",
      "WARNING:__main__:Removing constant columns: ['Number of Dependents']\n",
      "INFO:__main__:✅ All constraints validated successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Causal Relationships ===\n",
      "Married -> Number of Referrals (weight: 0.767)\n",
      "---\n",
      "Married -> Tenure in Months (weight: 0.672)\n",
      "---\n",
      "Tenure in Months -> Multiple Lines (weight: 0.661)\n",
      "---\n",
      "Tenure in Months -> Online Security (weight: 0.748)\n",
      "---\n",
      "Tenure in Months -> Online Backup (weight: 0.712)\n",
      "---\n",
      "Tenure in Months -> Device Protection Plan (weight: 0.723)\n",
      "---\n",
      "Tenure in Months -> Premium Tech Support (weight: 0.618)\n",
      "---\n",
      "Tenure in Months -> Streaming TV (weight: 0.683)\n",
      "---\n",
      "Tenure in Months -> Streaming Movies (weight: 0.635)\n",
      "---\n",
      "Tenure in Months -> Streaming Music (weight: 0.607)\n",
      "---\n",
      "Phone Service -> Total Revenue (weight: 0.384)\n",
      "---\n",
      "Multiple Lines -> Total Revenue (weight: 0.702)\n",
      "---\n",
      "Internet Type -> Total Revenue (weight: 0.370)\n",
      "---\n",
      "Internet Type -> Paperless Billing (weight: 0.691)\n",
      "---\n",
      "Unlimited Data -> Paperless Billing (weight: 0.396)\n",
      "---\n",
      "Online Security -> Total Revenue (weight: 0.623)\n",
      "---\n",
      "Online Backup -> Total Revenue (weight: 0.733)\n",
      "---\n",
      "Device Protection Plan -> Total Revenue (weight: 0.663)\n",
      "---\n",
      "Premium Tech Support -> Total Revenue (weight: 0.547)\n",
      "---\n",
      "Streaming TV -> Total Revenue (weight: 0.563)\n",
      "---\n",
      "Streaming Movies -> Total Revenue (weight: 0.452)\n",
      "---\n",
      "Streaming Movies -> Paperless Billing (weight: 0.236)\n",
      "---\n",
      "Streaming Music -> Total Revenue (weight: 0.460)\n",
      "---\n",
      "Total Revenue -> Churn Label (weight: 0.402)\n",
      "---\n",
      "Paperless Billing -> Churn Label (weight: 0.311)\n",
      "---\n",
      "\n",
      "LiNGAM Causal Relationships:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Causal graph saved as causal_discovery_output\\lingam_graph.png\n",
      "INFO:__main__:Causal relationships saved to causal_discovery_output\\lingam_relations.txt (2 relations)\n",
      "INFO:__main__:Structure Learning Metrics: {'num_edges': 1, 'graph_density': 0.08333333333333333, 'avg_in_degree': 0.25, 'avg_out_degree': 0.25, 'most_influential': [('Tenure in Months', 1), ('Age', 0), ('Number of Referrals', 0), ('Total Revenue', 0)], 'most_affected': [('Total Revenue', 1), ('Age', 0), ('Number of Referrals', 0), ('Tenure in Months', 0)]}\n",
      "INFO:__main__:Executing PC-GIN...\n",
      "INFO:__main__:Running PC-GIN algorithm...\n",
      "c:\\Users\\aafz1\\miniconda3\\envs\\project-env\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "c:\\Users\\aafz1\\miniconda3\\envs\\project-env\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "c:\\Users\\aafz1\\miniconda3\\envs\\project-env\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "c:\\Users\\aafz1\\miniconda3\\envs\\project-env\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "c:\\Users\\aafz1\\miniconda3\\envs\\project-env\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "c:\\Users\\aafz1\\miniconda3\\envs\\project-env\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "c:\\Users\\aafz1\\miniconda3\\envs\\project-env\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Causal Relationships ===\n",
      "Number of Referrals -> Total Revenue (weight: 0.269)\n",
      "---\n",
      "Tenure in Months -> Total Revenue (weight: 0.852)\n",
      "---\n",
      "\n",
      "PC-GIN Causal Relationships:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aafz1\\miniconda3\\envs\\project-env\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "c:\\Users\\aafz1\\miniconda3\\envs\\project-env\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "c:\\Users\\aafz1\\miniconda3\\envs\\project-env\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "INFO:__main__:✅ All constraints validated successfully\n",
      "INFO:__main__:Causal graph saved as causal_discovery_output\\pcgin_graph.png\n",
      "INFO:__main__:Causal relationships saved to causal_discovery_output\\pcgin_relations.txt (50 relations)\n",
      "INFO:__main__:Structure Learning Metrics: {'num_edges': 50.0, 'graph_density': 0.09881422924901186, 'avg_in_degree': 2.1739130434782608, 'avg_out_degree': 2.1739130434782608, 'most_influential': [('Tenure in Months', 6.0), ('Offer', 6.0), ('Unlimited Data', 3.0), ('Number of Dependents', 3.0), ('Streaming Music', 3.0)], 'most_affected': [('Total Revenue', 11.0), ('Paperless Billing', 8.0), ('Payment Method', 7.0), ('Churn Label', 3.0), ('Offer', 3.0)]}\n",
      "INFO:__main__:Executing NOTEARS...\n",
      "INFO:__main__:Running NOTEARS algorithm...\n",
      "C:\\Users\\aafz1\\AppData\\Local\\Temp\\ipykernel_13468\\804282254.py:8: RuntimeWarning: invalid value encountered in divide\n",
      "  X_standardized = np.where(stds != 0, (X - means) / stds, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Causal Relationships ===\n",
      "Gender -> Offer (weight: 1.000)\n",
      "---\n",
      "Senior Citizen -> Offer (weight: 1.000)\n",
      "---\n",
      "Married -> Number of Referrals (weight: 1.000)\n",
      "---\n",
      "Married -> Tenure in Months (weight: 1.000)\n",
      "---\n",
      "Number of Dependents -> Number of Referrals (weight: 1.000)\n",
      "---\n",
      "Number of Dependents -> Tenure in Months (weight: 1.000)\n",
      "---\n",
      "Number of Dependents -> Offer (weight: 1.000)\n",
      "---\n",
      "Number of Referrals -> Internet Type (weight: 1.000)\n",
      "---\n",
      "Number of Referrals -> Online Security (weight: 1.000)\n",
      "---\n",
      "Tenure in Months -> Multiple Lines (weight: 1.000)\n",
      "---\n",
      "Tenure in Months -> Online Security (weight: 1.000)\n",
      "---\n",
      "Tenure in Months -> Online Backup (weight: 1.000)\n",
      "---\n",
      "Tenure in Months -> Device Protection Plan (weight: 1.000)\n",
      "---\n",
      "Tenure in Months -> Premium Tech Support (weight: 1.000)\n",
      "---\n",
      "Tenure in Months -> Streaming Movies (weight: 1.000)\n",
      "---\n",
      "Offer -> Multiple Lines (weight: 1.000)\n",
      "---\n",
      "Offer -> Online Security (weight: 1.000)\n",
      "---\n",
      "Offer -> Online Backup (weight: 1.000)\n",
      "---\n",
      "Offer -> Device Protection Plan (weight: 1.000)\n",
      "---\n",
      "Offer -> Premium Tech Support (weight: 1.000)\n",
      "---\n",
      "Offer -> Streaming TV (weight: 1.000)\n",
      "---\n",
      "Phone Service -> Total Revenue (weight: 1.000)\n",
      "---\n",
      "Multiple Lines -> Total Revenue (weight: 1.000)\n",
      "---\n",
      "Multiple Lines -> Paperless Billing (weight: 1.000)\n",
      "---\n",
      "Multiple Lines -> Payment Method (weight: 1.000)\n",
      "---\n",
      "Internet Type -> Total Revenue (weight: 1.000)\n",
      "---\n",
      "Internet Type -> Paperless Billing (weight: 1.000)\n",
      "---\n",
      "Internet Type -> Payment Method (weight: 1.000)\n",
      "---\n",
      "Unlimited Data -> Total Revenue (weight: 1.000)\n",
      "---\n",
      "Unlimited Data -> Paperless Billing (weight: 1.000)\n",
      "---\n",
      "Unlimited Data -> Payment Method (weight: 1.000)\n",
      "---\n",
      "Online Security -> Total Revenue (weight: 1.000)\n",
      "---\n",
      "Online Backup -> Total Revenue (weight: 1.000)\n",
      "---\n",
      "Online Backup -> Paperless Billing (weight: 1.000)\n",
      "---\n",
      "Device Protection Plan -> Total Revenue (weight: 1.000)\n",
      "---\n",
      "Device Protection Plan -> Paperless Billing (weight: 1.000)\n",
      "---\n",
      "Premium Tech Support -> Total Revenue (weight: 1.000)\n",
      "---\n",
      "Premium Tech Support -> Payment Method (weight: 1.000)\n",
      "---\n",
      "Streaming TV -> Total Revenue (weight: 1.000)\n",
      "---\n",
      "Streaming TV -> Paperless Billing (weight: 1.000)\n",
      "---\n",
      "Streaming TV -> Payment Method (weight: 1.000)\n",
      "---\n",
      "Streaming Movies -> Total Revenue (weight: 1.000)\n",
      "---\n",
      "Streaming Movies -> Paperless Billing (weight: 1.000)\n",
      "---\n",
      "Streaming Movies -> Payment Method (weight: 1.000)\n",
      "---\n",
      "Streaming Music -> Total Revenue (weight: 1.000)\n",
      "---\n",
      "Streaming Music -> Paperless Billing (weight: 1.000)\n",
      "---\n",
      "Streaming Music -> Payment Method (weight: 1.000)\n",
      "---\n",
      "Total Revenue -> Churn Label (weight: 1.000)\n",
      "---\n",
      "Paperless Billing -> Churn Label (weight: 1.000)\n",
      "---\n",
      "Payment Method -> Churn Label (weight: 1.000)\n",
      "---\n",
      "\n",
      "NOTEARS Causal Relationships:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:✅ All constraints validated successfully\n",
      "INFO:__main__:Causal graph saved as causal_discovery_output\\notears_graph.png\n",
      "INFO:__main__:Causal relationships saved to causal_discovery_output\\notears_relations.txt (13 relations)\n",
      "INFO:__main__:Structure Learning Metrics: {'num_edges': 1, 'graph_density': 0.001976284584980237, 'avg_in_degree': 0.043478260869565216, 'avg_out_degree': 0.043478260869565216, 'most_influential': [('Married', 1), ('Gender', 0), ('Paperless Billing', 0), ('Total Revenue', 0), ('Streaming Music', 0)], 'most_affected': [('Number of Referrals', 1), ('Gender', 0), ('Paperless Billing', 0), ('Total Revenue', 0), ('Streaming Music', 0)]}\n",
      "INFO:__main__:Executing GRaSP...\n",
      "INFO:__main__:Running GRaSP algorithm...\n",
      "C:\\Users\\aafz1\\AppData\\Local\\Temp\\ipykernel_13468\\1406470996.py:8: RuntimeWarning: invalid value encountered in divide\n",
      "  X_standardized = np.where(stds != 0, (X - means) / stds, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Causal Relationships ===\n",
      "Married -> Number of Referrals (weight: 0.692)\n",
      "---\n",
      "Married -> Tenure in Months (weight: 0.366)\n",
      "---\n",
      "Tenure in Months -> Multiple Lines (weight: 0.326)\n",
      "---\n",
      "Tenure in Months -> Online Security (weight: 0.308)\n",
      "---\n",
      "Tenure in Months -> Online Backup (weight: 0.351)\n",
      "---\n",
      "Tenure in Months -> Device Protection Plan (weight: 0.350)\n",
      "---\n",
      "Tenure in Months -> Premium Tech Support (weight: 0.307)\n",
      "---\n",
      "Tenure in Months -> Streaming TV (weight: 0.272)\n",
      "---\n",
      "Tenure in Months -> Streaming Movies (weight: 0.283)\n",
      "---\n",
      "Tenure in Months -> Streaming Music (weight: 0.234)\n",
      "---\n",
      "Multiple Lines -> Total Revenue (weight: 0.235)\n",
      "---\n",
      "Internet Type -> Paperless Billing (weight: 0.285)\n",
      "---\n",
      "Online Backup -> Total Revenue (weight: 0.222)\n",
      "---\n",
      "\n",
      "GRaSP Causal Relationships:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:✅ All constraints validated successfully\n",
      "INFO:__main__:Causal graph saved as causal_discovery_output\\grasp_graph.png\n",
      "INFO:__main__:Causal relationships saved to causal_discovery_output\\grasp_relations.txt (13 relations)\n",
      "INFO:__main__:Structure Learning Metrics: {'num_edges': 1, 'graph_density': 0.001976284584980237, 'avg_in_degree': 0.043478260869565216, 'avg_out_degree': 0.043478260869565216, 'most_influential': [('Married', 1), ('Gender', 0), ('Paperless Billing', 0), ('Total Revenue', 0), ('Streaming Music', 0)], 'most_affected': [('Number of Referrals', 1), ('Gender', 0), ('Paperless Billing', 0), ('Total Revenue', 0), ('Streaming Music', 0)]}\n",
      "INFO:__main__:Summary of results:\n",
      "         num_edges  graph_density  violations\n",
      "DECI          17.0       3.36e-02         0.0\n",
      "LiNGAM         1.0       8.33e-02         0.0\n",
      "PC-GIN        50.0       9.88e-02         0.0\n",
      "NOTEARS        1.0       1.98e-03         0.0\n",
      "GRaSP          1.0       1.98e-03         0.0\n",
      "INFO:__main__:Constraint matrix created with shape: (23, 23)\n",
      "INFO:__main__:Evaluating constraint violations on test data...\n",
      "WARNING:__main__:Constant columns in test data: ['Number of Dependents']\n",
      "INFO:__main__:✅ All constraints validated successfully\n",
      "INFO:__main__:DECI: 0 constraint violations on test data: None\n",
      "INFO:__main__:✅ All constraints validated successfully\n",
      "INFO:__main__:LiNGAM: 0 constraint violations on test data: None\n",
      "INFO:__main__:✅ All constraints validated successfully\n",
      "INFO:__main__:PC-GIN: 0 constraint violations on test data: None\n",
      "INFO:__main__:✅ All constraints validated successfully\n",
      "INFO:__main__:NOTEARS: 0 constraint violations on test data: None\n",
      "INFO:__main__:✅ All constraints validated successfully\n",
      "INFO:__main__:GRaSP: 0 constraint violations on test data: None\n",
      "INFO:__main__:Constraint violation summary:\n",
      "        constraint_violations violation_details\n",
      "DECI                        0              None\n",
      "LiNGAM                      0              None\n",
      "PC-GIN                      0              None\n",
      "NOTEARS                     0              None\n",
      "GRaSP                       0              None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Causal Relationships ===\n",
      "Married -> Number of Referrals (weight: 0.692)\n",
      "---\n",
      "Married -> Tenure in Months (weight: 0.366)\n",
      "---\n",
      "Tenure in Months -> Multiple Lines (weight: 0.326)\n",
      "---\n",
      "Tenure in Months -> Online Security (weight: 0.308)\n",
      "---\n",
      "Tenure in Months -> Online Backup (weight: 0.351)\n",
      "---\n",
      "Tenure in Months -> Device Protection Plan (weight: 0.350)\n",
      "---\n",
      "Tenure in Months -> Premium Tech Support (weight: 0.307)\n",
      "---\n",
      "Tenure in Months -> Streaming TV (weight: 0.272)\n",
      "---\n",
      "Tenure in Months -> Streaming Movies (weight: 0.283)\n",
      "---\n",
      "Tenure in Months -> Streaming Music (weight: 0.234)\n",
      "---\n",
      "Multiple Lines -> Total Revenue (weight: 0.235)\n",
      "---\n",
      "Internet Type -> Paperless Billing (weight: 0.285)\n",
      "---\n",
      "Online Backup -> Total Revenue (weight: 0.222)\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Run the pipeline (assuming train_data is available from preprocessing)\n",
    "results = run_causal_discovery_pipeline(train_data, tiers, specific_constraints, output_dir=\"causal_discovery_output\")\n",
    "\n",
    "# Evaluate constraint violations on test data\n",
    "node_names = list(train_data.columns)\n",
    "_, node_name_to_idx = create_constraint_matrix(node_names, tiers, specific_constraints)\n",
    "evaluation_summary = evaluate_causal_discovery(results, test_data, tiers, node_name_to_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
